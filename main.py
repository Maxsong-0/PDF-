#!/usr/bin/env python3
# type: ignore
"""
PDFæ‰¹é‡é‡å‘½åå·¥å…· - WebUIç‰ˆæœ¬
æ”¯æŒOCRè¯†åˆ«é”€è´§å‡ºåº“å•å·ï¼ŒåŒ…å«æ—‹è½¬æ£€æµ‹åŠŸèƒ½
"""

import os
import re
import io
import tempfile
import zipfile
from pathlib import Path
from typing import List, Optional, Tuple, Union, Dict, Any
import logging
import glob
import time
import uuid

# è®¾ç½®ç¯å¢ƒå˜é‡è§£å†³å„ç§è­¦å‘Šå’Œå…¼å®¹æ€§é—®é¢˜
os.environ['NUMEXPR_MAX_THREADS'] = '8'
os.environ['OMP_NUM_THREADS'] = '8'
# PaddlePaddleç¯å¢ƒå˜é‡ï¼ˆè§£å†³MKLDNNç¼–è¯‘é—®é¢˜ï¼‰
os.environ['PADDLE_DISABLE_MKLDNN'] = '1'        # ç¦ç”¨MKLDNNï¼ˆè§£å†³macOSç¼–è¯‘é—®é¢˜ï¼‰
os.environ['PADDLE_DISABLE_CUDA'] = '1'         # ç¦ç”¨CUDAï¼ˆä½¿ç”¨CPUï¼‰
os.environ['PADDLE_CPP_LOG_LEVEL'] = '3'        # å‡å°‘æ—¥å¿—è¾“å‡º
os.environ['FLAGS_allocator_strategy'] = 'auto_growth'  # å†…å­˜è‡ªåŠ¨å¢é•¿ç­–ç•¥
os.environ['FLAGS_fraction_of_gpu_memory_to_use'] = '0'  # ä¸ä½¿ç”¨GPUå†…å­˜

from fastapi import FastAPI, File, UploadFile, Request, HTTPException
from fastapi.responses import HTMLResponse, FileResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.middleware.cors import CORSMiddleware
from starlette.background import BackgroundTask
import uvicorn

# ç±»å‹æ³¨è§£
try:
    import fitz  # PyMuPDF
    from PIL import Image, ImageEnhance
    import cv2
    import numpy as np
    import easyocr  # type: ignore
    from paddleocr import PaddleOCR  # type: ignore
    
    # ç±»å‹æç¤º
    FitzType = type(fitz)
    ImageType = type(Image)
    CV2Type = type(cv2)
    NPType = type(np)
    ImageEnhanceType = type(ImageEnhance)
    EasyOCRType = type(easyocr)
    PaddleOCRType = type(PaddleOCR)
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨Anyç±»å‹
    FitzType = Any
    ImageType = Any
    CV2Type = Any
    NPType = Any
    ImageEnhanceType = Any
    EasyOCRType = Any
    PaddleOCRType = Any

def lazy_import_ocr():
    """å»¶è¿Ÿå¯¼å…¥OCRç›¸å…³åº“ä»¥å‡å°‘å¯åŠ¨å†…å­˜"""
    global fitz, Image, cv2, np, ImageEnhance, easyocr, paddleocr
    try:
        import fitz  # PyMuPDF
        from PIL import Image, ImageEnhance
        
        # PILå…¼å®¹æ€§è¡¥ä¸ - å¤„ç†Pillow 10.0.0+ä¸­ANTIALIASè¢«ç§»é™¤çš„é—®é¢˜
        if not hasattr(Image, 'ANTIALIAS'):
            Image.ANTIALIAS = Image.Resampling.LANCZOS
            Image.NEAREST = Image.Resampling.NEAREST
            Image.BILINEAR = Image.Resampling.BILINEAR
            Image.BICUBIC = Image.Resampling.BICUBIC
            Image.BOX = Image.Resampling.BOX
            Image.HAMMING = Image.Resampling.HAMMING
            Image.LANCZOS = Image.Resampling.LANCZOS
        
        import cv2
        import numpy as np
        import easyocr  # å¿«é€Ÿåˆè¯†åˆ«å¼•æ“  # type: ignore
        from paddleocr import PaddleOCR  # ç²¾ç¡®ç¡®è®¤å¼•æ“  # type: ignore
        paddleocr = PaddleOCR
        return True
    except ImportError as e:
        logger.error(f"OCRåº“å¯¼å…¥å¤±è´¥: {e}")
        logger.info("è¯·å®‰è£…ä¾èµ–åº“:")
        logger.info("pip install easyocr paddlepaddle paddleocr")
        return False

# åˆå§‹åŒ–å…¨å±€å˜é‡
fitz = None
Image = None
cv2 = None
np = None
ImageEnhance = None
easyocr = None
paddleocr = None

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="PDFæ‰¹é‡é‡å‘½åå·¥å…·", description="åŸºäºOCRè¯†åˆ«çš„PDFæ‰¹é‡é‡å‘½åå·¥å…·")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ·»åŠ å…¨å±€å˜é‡æ¥å­˜å‚¨æ–‡ä»¶åæ˜ å°„å…³ç³»
filename_mapping: Dict[str, str] = {}  # é‡å‘½ååæ–‡ä»¶å -> åŸå§‹æ–‡ä»¶å

# å…¨å±€å˜é‡å­˜å‚¨ä¸´æ—¶ZIPæ–‡ä»¶ä¿¡æ¯
temp_zip_files = {}

def clean_original_filename_files():
    """æ¸…ç†downloadsç›®å½•ä¸­çš„åŸå§‹æ–‡ä»¶åæ ¼å¼æ–‡ä»¶ï¼ˆæ—¶é—´æˆ³å¼€å¤´çš„æ–‡ä»¶ï¼‰"""
    downloads_dir = Path("downloads")
    if not downloads_dir.exists():
        return 0

    cleaned_count = 0
    try:
        # è¯†åˆ«åŸå§‹æ–‡ä»¶åæ ¼å¼çš„æ–‡ä»¶ï¼ˆä»¥æ—¶é—´æˆ³å¼€å¤´ï¼Œæ ¼å¼å¦‚ï¼š20250213165300083_xxxx.pdfï¼‰
        import re
        original_pattern = re.compile(r'^\d{17}_\d{4}.*\.pdf$')  # åŒ¹é… 20250213165300083_0022.pdf æ ¼å¼

        for pdf_file in downloads_dir.glob("*.pdf"):
            # æ£€æŸ¥æ˜¯å¦æ˜¯åŸå§‹æ–‡ä»¶åæ ¼å¼
            if original_pattern.match(pdf_file.name):
                try:
                    os.remove(pdf_file)
                    cleaned_count += 1
                    logger.info(f"æ¸…ç†åŸå§‹æ–‡ä»¶åæ ¼å¼æ–‡ä»¶: {pdf_file.name}")
                except Exception as e:
                    logger.warning(f"æ¸…ç†æ–‡ä»¶å¤±è´¥ {pdf_file.name}: {e}")

    except Exception as e:
        logger.error(f"æ¸…ç†åŸå§‹æ–‡ä»¶åæ ¼å¼æ–‡ä»¶æ—¶å‡ºé”™: {e}")

    if cleaned_count > 0:
        logger.info(f"âœ… è‡ªåŠ¨æ¸…ç†äº† {cleaned_count} ä¸ªåŸå§‹æ–‡ä»¶åæ ¼å¼æ–‡ä»¶")

    return cleaned_count

def clean_debug_files():
    """ä¸“é—¨æ¸…ç†è°ƒè¯•æ–‡ä»¶çš„å‡½æ•°"""
    current_dir = Path(".")
    debug_cleaned_count = 0

    # æ‰©å±•çš„è°ƒè¯•æ–‡ä»¶æ¨¡å¼
    debug_patterns = [
        "debug_*.png",      # è°ƒè¯•PNGå›¾ç‰‡
        "debug_*.jpg",      # è°ƒè¯•JPGå›¾ç‰‡
        "temp_*.png",       # ä¸´æ—¶PNGæ–‡ä»¶
        "temp_*.jpg",       # ä¸´æ—¶JPGæ–‡ä»¶
        "temp_*.zip",       # ä¸´æ—¶ZIPæ–‡ä»¶
        "rotated_*.png",    # æ—‹è½¬æµ‹è¯•å›¾ç‰‡
        "processed_*.png",  # å¤„ç†åå›¾ç‰‡
        "enhanced_*.png",   # å¢å¼ºå›¾ç‰‡
        "original_*.png",   # åŸå§‹å›¾ç‰‡
        "test_*.png",       # æµ‹è¯•å›¾ç‰‡
        "digit_*.png",      # æ•°å­—æµ‹è¯•å›¾ç‰‡
        "roi_*.png",        # ROIæµ‹è¯•å›¾ç‰‡
        "*.debug.png",      # .debugåç¼€çš„å›¾ç‰‡
    ]

    try:
        for pattern in debug_patterns:
            for debug_file in current_dir.glob(pattern):
                try:
                    os.remove(debug_file)
                    debug_cleaned_count += 1
                    logger.debug(f"æ¸…ç†è°ƒè¯•æ–‡ä»¶: {debug_file.name}")
                except Exception as e:
                    logger.warning(f"æ¸…ç†è°ƒè¯•æ–‡ä»¶å¤±è´¥ {debug_file.name}: {e}")
    except Exception as e:
        logger.error(f"æ¸…ç†è°ƒè¯•æ–‡ä»¶æ—¶å‡ºé”™: {e}")

    if debug_cleaned_count > 0:
        logger.info(f"ğŸ—‘ï¸ ä¸“é—¨æ¸…ç†äº† {debug_cleaned_count} ä¸ªè°ƒè¯•æ–‡ä»¶")

    return debug_cleaned_count

def clean_all_downloads():
    """æ¸…ç†downloadsç›®å½•ä¸­çš„æ‰€æœ‰PDFæ–‡ä»¶å’Œè°ƒè¯•æ–‡ä»¶ï¼ˆæ¯æ¬¡æ–°è¯†åˆ«å‰æ¸…ç†ä¸Šæ¬¡ç»“æœï¼‰"""
    downloads_dir = Path("downloads")
    current_dir = Path(".")

    cleaned_count = 0
    debug_cleaned_count = 0

    # æ¸…ç†downloadsç›®å½•ä¸­çš„PDFæ–‡ä»¶
    if downloads_dir.exists():
        try:
            for pdf_file in downloads_dir.glob("*.pdf"):
                try:
                    os.remove(pdf_file)
                    cleaned_count += 1
                    logger.info(f"æ¸…ç†ä¸Šæ¬¡å¤„ç†æ–‡ä»¶: {pdf_file.name}")
                except Exception as e:
                    logger.warning(f"æ¸…ç†æ–‡ä»¶å¤±è´¥ {pdf_file.name}: {e}")
        except Exception as e:
            logger.error(f"æ¸…ç†downloadsç›®å½•æ—¶å‡ºé”™: {e}")

    # æ¸…ç†å½“å‰ç›®å½•ä¸­çš„è°ƒè¯•æ–‡ä»¶
    debug_patterns = [
        "debug_*.png",      # è°ƒè¯•PNGå›¾ç‰‡
        "debug_*.jpg",      # è°ƒè¯•JPGå›¾ç‰‡
        "temp_*.png",       # ä¸´æ—¶PNGæ–‡ä»¶
        "temp_*.jpg",       # ä¸´æ—¶JPGæ–‡ä»¶
        "temp_*.zip",       # ä¸´æ—¶ZIPæ–‡ä»¶
        "rotated_*.png",    # æ—‹è½¬æµ‹è¯•å›¾ç‰‡
        "processed_*.png",  # å¤„ç†åå›¾ç‰‡
        "enhanced_*.png",   # å¢å¼ºå›¾ç‰‡
        "original_*.png",   # åŸå§‹å›¾ç‰‡
    ]

    try:
        for pattern in debug_patterns:
            for debug_file in current_dir.glob(pattern):
                try:
                    os.remove(debug_file)
                    debug_cleaned_count += 1
                    logger.debug(f"æ¸…ç†è°ƒè¯•æ–‡ä»¶: {debug_file.name}")
                except Exception as e:
                    logger.warning(f"æ¸…ç†è°ƒè¯•æ–‡ä»¶å¤±è´¥ {debug_file.name}: {e}")
    except Exception as e:
        logger.error(f"æ¸…ç†è°ƒè¯•æ–‡ä»¶æ—¶å‡ºé”™: {e}")

    total_cleaned = cleaned_count + debug_cleaned_count

    if cleaned_count > 0:
        logger.info(f"ğŸ§¹ æ¸…ç†äº† {cleaned_count} ä¸ªä¸Šæ¬¡å¤„ç†çš„æ–‡ä»¶")
    if debug_cleaned_count > 0:
        logger.info(f"ğŸ—‘ï¸ æ¸…ç†äº† {debug_cleaned_count} ä¸ªè°ƒè¯•æ–‡ä»¶")

    return total_cleaned

def get_template_directory():
    """è‡ªåŠ¨æ£€æµ‹æ¨¡æ¿ç›®å½•çš„æ­£ç¡®è·¯å¾„"""
    # è·å–å½“å‰è„šæœ¬çš„ç›®å½•
    script_dir = Path(__file__).parent.absolute()

    # å¯èƒ½çš„æ¨¡æ¿ç›®å½•ä½ç½®
    possible_paths = [
        script_dir / "templates",  # åŒçº§ç›®å½•
        Path.cwd() / "templates",  # å½“å‰å·¥ä½œç›®å½•
        script_dir.parent / "templates",  # ä¸Šçº§ç›®å½•
    ]

    for template_path in possible_paths:
        if template_path.exists() and (template_path / "index.html").exists():
            logger.info(f"æ‰¾åˆ°æ¨¡æ¿ç›®å½•: {template_path}")
            return str(template_path)

    # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œåˆ›å»ºé»˜è®¤ç›®å½•
    default_path = script_dir / "templates"
    default_path.mkdir(exist_ok=True)
    logger.warning(f"æœªæ‰¾åˆ°ç°æœ‰æ¨¡æ¿ç›®å½•ï¼Œåˆ›å»ºé»˜è®¤ç›®å½•: {default_path}")
    return str(default_path)

def get_static_directory():
    """è‡ªåŠ¨æ£€æµ‹é™æ€æ–‡ä»¶ç›®å½•çš„æ­£ç¡®è·¯å¾„"""
    script_dir = Path(__file__).parent.absolute()

    possible_paths = [
        script_dir / "static",
        Path.cwd() / "static",
        script_dir.parent / "static",
    ]

    for static_path in possible_paths:
        if static_path.exists():
            logger.info(f"æ‰¾åˆ°é™æ€æ–‡ä»¶ç›®å½•: {static_path}")
            return str(static_path)

    # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œåˆ›å»ºé»˜è®¤ç›®å½•
    default_path = script_dir / "static"
    default_path.mkdir(exist_ok=True)
    logger.info(f"åˆ›å»ºé™æ€æ–‡ä»¶ç›®å½•: {default_path}")
    return str(default_path)

# åˆ›å»ºå¿…è¦çš„ç›®å½•
script_dir = Path(__file__).parent.absolute()
for dir_name in ["uploads", "backup", "downloads"]:
    dir_path = script_dir / dir_name
    dir_path.mkdir(exist_ok=True)

# è·å–æ­£ç¡®çš„æ¨¡æ¿å’Œé™æ€æ–‡ä»¶ç›®å½•
template_dir = get_template_directory()
static_dir = get_static_directory()

templates = Jinja2Templates(directory=template_dir)
app.mount("/static", StaticFiles(directory=static_dir), name="static")

class PDFProcessor:
    def __init__(self):
        # EasyOCR è¯»å–å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self._easyocr_reader = None
        self._easyocr_initialized = False

        # PaddleOCR è¯»å–å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self._paddleocr_reader = None
        self._paddleocr_initialized = False

        # OCRå¸¸è§é”™è¯¯çŸ«æ­£å­—å…¸ï¼ˆæ•°å­—å®¹æ˜“é”™è¯¯è¯†åˆ«ï¼‰
        self.ocr_correction_map = {
            '8': '0',  # 8å®¹æ˜“è¢«è¯¯è¯†åˆ«ä¸º0
            '0': '8',  # 0å®¹æ˜“è¢«è¯¯è¯†åˆ«ä¸º8
            '6': '5',  # 6å®¹æ˜“è¢«è¯¯è¯†åˆ«ä¸º5
            '5': '6',  # 5å®¹æ˜“è¢«è¯¯è¯†åˆ«ä¸º6
            '1': 'I',  # æ•°å­—1å’Œå­—æ¯Içš„æ··æ·†
            'I': '1',  # å­—æ¯Iå’Œæ•°å­—1çš„æ··æ·†
            'O': '0',  # å­—æ¯Oå’Œæ•°å­—0çš„æ··æ·†
            '0': 'O',  # æ•°å­—0å’Œå­—æ¯Oçš„æ··æ·†
            'Z': '2',  # Zå’Œ2çš„æ··æ·†
            '2': 'Z',  # 2å’ŒZçš„æ··æ·†
        }

        # å¿«é€’å•å·ç‰¹å¾æ¨¡å¼ï¼ˆéœ€è¦æ’é™¤çš„ï¼‰
        self.express_patterns = [
            r'[A-Z]{2}[0-9]{10,15}',  # å¦‚YT1234567890123ï¼ˆåœ†é€šï¼‰
            r'[0-9]{12,15}',  # çº¯12-15ä½æ•°å­—å¿«é€’å•å·
            r'JD[0-9]{13,18}',  # äº¬ä¸œå¿«é€’
            r'SF[0-9]{12}',  # é¡ºä¸°é€Ÿè¿
            r'YTO[0-9]{10,13}',  # åœ†é€šé€Ÿé€’
            r'ZTO[0-9]{12}',  # ä¸­é€šå¿«é€’
            r'STO[0-9]{12}',  # ç”³é€šå¿«é€’
            r'YD[0-9]{13}',  # éŸµè¾¾å¿«é€’
            r'HTKY[0-9]{10}',  # ç™¾ä¸–æ±‡é€š
            r'[0-9]{13}',  # 13ä½çº¯æ•°å­—ï¼ˆå¸¸è§å¿«é€’æ ¼å¼ï¼‰
        ]

        # é”€è´§å‡ºåº“å•å·çš„ç²¾ç¡®è¯†åˆ«æ¨¡å¼ï¼ˆæŒ‰ç½®ä¿¡åº¦æ’åºï¼‰
        self.patterns = [
            # è¶…é«˜ç½®ä¿¡åº¦ï¼šåŒ…å«æ˜ç¡®æ ‡è¯†çš„å®Œæ•´æ ¼å¼
            r'é”€è´§å‡ºåº“å•å·[ï¼š:\s]*([0-9]{4}[-_][0-9]{12})',  # ä¸¥æ ¼4-12ä½æ ¼å¼
            r'é”€è´§å‡ºåº“å•å·[ï¼š:\s]*([0-9]{4}[-_][0-9]{10,15})',  # ä¸­æ–‡+4ä½æ•°å­—-10åˆ°15ä½æ•°å­—
            r'å‡ºåº“å•å·[ï¼š:\s]*([0-9]{4}[-_][0-9]{12})',  # å‡ºåº“å•å·+ä¸¥æ ¼æ ¼å¼

            # é«˜ç½®ä¿¡åº¦ï¼šæ ‡å‡†****-************æ ¼å¼ï¼ˆä½ çš„ä¸»è¦æ ¼å¼ï¼‰
            r'(?<![A-Za-z0-9])([0-9]{4}[-_][0-9]{12})(?![A-Za-z0-9])',  # ä¸¥æ ¼4ä½-12ä½æ•°å­—
            r'(?<![A-Za-z0-9])([0-9]{4}[-_][0-9]{10,15})(?![A-Za-z0-9])',  # 4ä½æ•°å­—-10åˆ°15ä½æ•°å­—

            # ä¸­é«˜ç½®ä¿¡åº¦ï¼šåŒ…å«å…³é”®è¯çš„æ ¼å¼
            r'å•å·[ï¼š:\s]*([0-9]{4}[-_][0-9]{12})',  # "å•å·"+ä¸¥æ ¼æ ¼å¼
            r'ç¼–å·[ï¼š:\s]*([0-9]{4}[-_][0-9]{12})',  # "ç¼–å·"+ä¸¥æ ¼æ ¼å¼
            r'å‡ºåº“[ï¼š:\s]*([0-9]{4}[-_][0-9]{12})',  # "å‡ºåº“"+ä¸¥æ ¼æ ¼å¼

            # ä¸­ç½®ä¿¡åº¦ï¼šç‰¹å®šå‰ç¼€ï¼ˆæ’é™¤å¿«é€’å…¬å¸å‰ç¼€ï¼‰
            r'(?<![A-Za-z0-9])([13579][0-9]{3}[-_][0-9]{12})(?![A-Za-z0-9])',  # å¥‡æ•°å¼€å¤´4ä½-12ä½
            r'(?<![A-Za-z0-9])([1][4][0-9]{2}[-_][0-9]{12})(?![A-Za-z0-9])',  # 14å¼€å¤´çš„æ ¼å¼

            # å®¹é”™æ¨¡å¼ï¼šOCRå¯èƒ½çš„é”™è¯¯è¯†åˆ«ï¼ˆå¢å¼ºç‰ˆï¼‰
            r'é”€[è´§ä¹°è´·][å‡ºäººå±±][åº“å•é‡Œ][å•é‡Œå·][ï¼š:\s]*([0-9]{4}[-_][0-9]{10,15})',  # OCRå®¹é”™
            r'[é”€é”—][è´§è´·ä¹°][å‡ºå±±äºº][åº“é‡Œå•][å•é‡Œå·]å·[ï¼š:\s]*([0-9]{4}[-_][0-9]{10,15})',  # æ›´å¤šOCRå®¹é”™

            # å¢å¼ºå®½æ¾æ ¼å¼ï¼šé€‚åº”æ›´å¤šOCRè¯†åˆ«ç»“æœ
            r'(?<![A-Za-z0-9])([0-9]{4}[ï¼â€”\-_]\s*[0-9]{8,20})(?![A-Za-z0-9])',  # æ”¯æŒå„ç§æ¨ªçº¿ç¬¦å·å’Œç©ºæ ¼
            r'(?<![A-Za-z0-9])([0-9]{3,5}[-_][0-9]{8,20})(?![A-Za-z0-9])',  # 3-5ä½-8-20ä½æ•°å­—
            r'(?<![A-Za-z0-9])([0-9]{4}\s+[0-9]{10,15})(?![A-Za-z0-9])',  # ç©ºæ ¼åˆ†éš”çš„æ ¼å¼

            # è°ƒè¯•æ¨¡å¼ï¼šè¶…å®½æ¾åŒ¹é…ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            r'(?<![A-Za-z])([0-9]{4}[^A-Za-z0-9\u4e00-\u9fff]*[0-9]{8,})',  # 4ä½æ•°å­—+ä»»æ„åˆ†éš”ç¬¦+8ä½ä»¥ä¸Šæ•°å­—
            r'([0-9]{12,20})',  # ç›´æ¥åŒ¹é…12-20ä½è¿ç»­æ•°å­—
        ]

        # éœ€è¦æ’é™¤çš„å¸¸è§å•è¯å’Œæ— æ•ˆæ¨¡å¼ï¼ˆå¤§å¹…æ‰©å±•ï¼‰
        self.excluded_words = {
            # åŸºæœ¬æ’é™¤è¯
            'document', 'order', 'sales', 'number', 'date', 'customer',
            'address', 'phone', 'email', 'total', 'amount', 'price',
            'quantity', 'description', 'product', 'service', 'company',
            'invoice', 'receipt', 'payment', 'balance', 'account',

            # å¿«é€’å…¬å¸ç›¸å…³
            'express', 'delivery', 'courier', 'shipping', 'tracking',
            'jd', 'sf', 'yto', 'zto', 'sto', 'yd', 'htky',

            # æ— æ•ˆæ•°å­—åºåˆ—
            '0000000000000000', '1111111111111111', '2222222222222222',
            '3333333333333333', '4444444444444444', '5555555555555555',
            '6666666666666666', '7777777777777777', '8888888888888888',
            '9999999999999999',

            # æ— æ•ˆå­—æ¯åºåˆ—
            'abcdefghijklmnop', 'qrstuvwxyz', 'xxxxxxxxxxxxxxxxx',
        }

        # å¢å¼ºéªŒè¯è§„åˆ™ï¼ˆæ”¯æŒ1410ç­‰æ–°å‰ç¼€ï¼‰
        self.validation_rules = {
            'min_digits': 4,  # é™ä½æ•°å­—è¦æ±‚ä¾¿äºè°ƒè¯•
            'min_length': 6,  # é™ä½æœ€å°é•¿åº¦è¦æ±‚
            'max_length': 30,  # å¢åŠ æœ€å¤§é•¿åº¦
            'required_separator': ['-', '_', 'ï¼', 'â€”', ' '],  # æ”¯æŒæ›´å¤šåˆ†éš”ç¬¦ç±»å‹
            'min_separator_count': 0,  # ä¸å¼ºåˆ¶è¦æ±‚åˆ†éš”ç¬¦ï¼ˆä¾¿äºè°ƒè¯•ï¼‰
            'sales_order_prefixes': ['1403', '1404', '1405', '1410', '1411', '1412'],  # æ‰©å±•é”€è´§å•å·å‰ç¼€
            'invalid_patterns': [
                r'^[0]+$',  # å…¨é›¶
                r'^[1]+$',  # å…¨ä¸€
                r'^(.)\1{8,}$',  # é‡å¤å­—ç¬¦è¶…è¿‡8æ¬¡
                r'^[A-Z]{2}[0-9]{10,15}$',  # å¿«é€’å•å·æ ¼å¼
                r'^JD[0-9]+$',  # äº¬ä¸œå¿«é€’
                r'^SF[0-9]+$',  # é¡ºä¸°å¿«é€’
                r'^YTO[0-9]+$',  # åœ†é€šå¿«é€’
                r'^ZTO[0-9]+$',  # ä¸­é€šå¿«é€’
                r'^STO[0-9]+$',  # ç”³é€šå¿«é€’
            ]
        }

    def extract_order_number(self, pdf_path: str) -> Tuple[Optional[str], str]:
        """ä»PDFä¸­æå–é”€è´§å‡ºåº“å•å·ï¼Œè¿”å›(è®¢å•å·, å¤„ç†æ—¥å¿—)"""
        log_messages = []

        if not lazy_import_ocr():
            return None, "âŒ OCRåº“å¯¼å…¥å¤±è´¥ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–"

        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(pdf_path):
                return None, "âŒ æ–‡ä»¶ä¸å­˜åœ¨"

            if fitz is None:
                return None, "âŒ PyMuPDFåº“æœªæ­£ç¡®å¯¼å…¥"

            doc = fitz.open(pdf_path)
            text = ""
            for page in doc:
                page_text = page.get_text()
                text += page_text
            doc.close()

            log_messages.append("âœ… PDFæ–‡ä»¶è¯»å–æˆåŠŸ")

            # å…ˆå°è¯•ç›´æ¥æ–‡æœ¬æå–
            order_number = self.find_order_number_in_text(text)
            if order_number:
                log_messages.append(f"âœ… ç›´æ¥æ–‡æœ¬æå–æˆåŠŸ: {order_number}")
                return order_number, "\n".join(log_messages)

            log_messages.append("âš ï¸ ç›´æ¥æ–‡æœ¬æå–å¤±è´¥ï¼Œå¼€å§‹OCRå¤„ç†...")

            # ä½¿ç”¨OCRå¤„ç†
            ocr_result, ocr_logs = self.extract_with_ocr(pdf_path)
            log_messages.extend(ocr_logs)

            return ocr_result, "\n".join(log_messages)

        except Exception as e:
            error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}"
            log_messages.append(error_msg)
            logger.error(error_msg, exc_info=True)
            return None, "\n".join(log_messages)

    def find_order_number_in_text(self, text: str) -> Optional[str]:
        """åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾é”€è´§å‡ºåº“å•å· - é«˜ç²¾åº¦ç‰ˆæœ¬ï¼ˆå¢å¼ºOCRçŸ«æ­£ï¼‰"""
        try:
            candidates = []

            # 1. è·å–OCRçŸ«æ­£åçš„å¤šä¸ªæ–‡æœ¬ç‰ˆæœ¬
            text_variants = self._apply_ocr_correction(text)
            logger.info(f"OCRçŸ«æ­£ç”Ÿæˆ{len(text_variants)}ä¸ªæ–‡æœ¬å˜ä½“")

            # 2. å¯¹æ¯ä¸ªæ–‡æœ¬å˜ä½“è¿›è¡Œæ¨¡å¼åŒ¹é…
            for variant_index, text_variant in enumerate(text_variants):
                variant_label = "åŸæ–‡" if variant_index == 0 else f"çŸ«æ­£{variant_index}"

                # éå†æ‰€æœ‰æ¨¡å¼ï¼ŒæŒ‰ä¼˜å…ˆçº§æ”¶é›†å€™é€‰é¡¹
                for pattern_index, pattern in enumerate(self.patterns):
                    matches = re.finditer(pattern, text_variant, re.IGNORECASE)
                    for match in matches:
                        result = match.group(1).strip()
                        if self._validate_order_number(result):
                            # è®¡ç®—ç»¼åˆç½®ä¿¡åº¦
                            base_confidence = len(self.patterns) - pattern_index
                            # åŸæ–‡åŒ¹é…ç½®ä¿¡åº¦æ›´é«˜
                            variant_bonus = 1.0 if variant_index == 0 else 0.8
                            final_confidence = base_confidence * variant_bonus

                            candidates.append({
                                'number': result,
                                'confidence': final_confidence,
                                'pattern_index': pattern_index,
                                'variant': variant_label,
                                'variant_index': variant_index
                            })

                            logger.info(f"å€™é€‰è®¢å•å·: {result} (æ¨¡å¼{pattern_index}, {variant_label}, ç½®ä¿¡åº¦{final_confidence:.2f})")

            if not candidates:
                logger.info("æœªæ‰¾åˆ°æœ‰æ•ˆçš„é”€è´§å‡ºåº“å•å·å€™é€‰")
                return None

            # 3. æŒ‰ç½®ä¿¡åº¦æ’åºï¼Œè¿”å›æœ€ä½³å€™é€‰é¡¹
            candidates.sort(key=lambda x: (-x['confidence'], x['pattern_index'], x['variant_index']))

            # 4. å»é‡ï¼Œä¼˜å…ˆé€‰æ‹©åŸæ–‡è¯†åˆ«çš„ç»“æœ
            unique_numbers = {}
            for candidate in candidates:
                number = candidate['number']
                if number not in unique_numbers:
                    unique_numbers[number] = candidate
                else:
                    # å¦‚æœæ˜¯åŸæ–‡è¯†åˆ«çš„ï¼Œæ›¿æ¢çŸ«æ­£æ–‡æœ¬çš„ç»“æœ
                    if candidate['variant_index'] == 0 and unique_numbers[number]['variant_index'] > 0:
                        unique_numbers[number] = candidate

            final_candidates = list(unique_numbers.values())
            final_candidates.sort(key=lambda x: (-x['confidence'], x['pattern_index'], x['variant_index']))

            best_candidate = final_candidates[0]

            logger.info(f"æœ€ç»ˆé€‰æ‹©: {best_candidate['number']} (æ¥æº: {best_candidate['variant']}, ç½®ä¿¡åº¦: {best_candidate['confidence']:.2f})")
            logger.info(f"æ€»è®¡æ‰¾åˆ°{len(candidates)}ä¸ªå€™é€‰é¡¹ï¼Œå»é‡å{len(final_candidates)}ä¸ª")

            return best_candidate['number']

        except Exception as e:
            logger.error(f"æ–‡æœ¬æŸ¥æ‰¾å¤±è´¥: {e}")
            return None

    def _apply_ocr_correction(self, text: str) -> List[str]:
        """åº”ç”¨OCRå¸¸è§é”™è¯¯çŸ«æ­£ï¼Œç”Ÿæˆå¤šä¸ªå€™é€‰æ–‡æœ¬"""
        original = text
        corrected_variants = [original]

        # å°è¯•å¸¸è§çš„æ•°å­—çŸ«æ­£
        for wrong, correct in self.ocr_correction_map.items():
            if wrong in text:
                variant = text.replace(wrong, correct)
                if variant != original and variant not in corrected_variants:
                    corrected_variants.append(variant)

        return corrected_variants

    def _is_express_number(self, candidate: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯å¿«é€’å•å·"""
        # æ£€æŸ¥å¿«é€’å•å·ç‰¹å¾æ¨¡å¼
        for pattern in self.express_patterns:
            if re.match(pattern, candidate):
                logger.info(f"è¯†åˆ«ä¸ºå¿«é€’å•å·ï¼ˆæ ¼å¼åŒ¹é…ï¼‰: {candidate}")
                return True

        # æ£€æŸ¥å¿«é€’å…¬å¸å‰ç¼€
        express_prefixes = ['JD', 'SF', 'YTO', 'ZTO', 'STO', 'YD', 'HTKY', 'EMS', 'YZPY', 'YUNDA']
        for prefix in express_prefixes:
            if candidate.upper().startswith(prefix):
                logger.info(f"è¯†åˆ«ä¸ºå¿«é€’å•å·ï¼ˆå‰ç¼€åŒ¹é…ï¼‰: {candidate}")
                return True

        # ç‰¹æ®Šæ ¼å¼æ£€æŸ¥ï¼šçº¯æ•°å­—ä¸”é•¿åº¦ä¸ºå¿«é€’å¸¸ç”¨é•¿åº¦
        if candidate.isdigit():
            if len(candidate) in [13, 15, 18]:  # å¿«é€’å•å·å¸¸ç”¨é•¿åº¦
                logger.info(f"è¯†åˆ«ä¸ºå¿«é€’å•å·ï¼ˆé•¿åº¦ç‰¹å¾ï¼‰: {candidate}")
                return True

        return False

    def _validate_order_number(self, candidate: str) -> bool:
        """éªŒè¯å€™é€‰è®¢å•å·æ˜¯å¦æœ‰æ•ˆï¼ˆè°ƒè¯•æœŸé—´å®½æ¾ç‰ˆæœ¬ï¼‰"""
        if not candidate:
            return False

        # åŸºæœ¬é•¿åº¦æ£€æŸ¥
        rules = self.validation_rules
        if len(candidate) < rules['min_length'] or len(candidate) > rules['max_length']:
            logger.debug(f"é•¿åº¦ä¸ç¬¦åˆè¦æ±‚: {candidate} (é•¿åº¦: {len(candidate)})")
            return False

        # æ’é™¤å¸¸è§æ— æ•ˆè¯æ±‡
        if candidate.lower() in self.excluded_words:
            logger.debug(f"åœ¨æ’é™¤è¯æ±‡ä¸­: {candidate}")
            return False

        # å¿«é€’å•å·æ’é™¤æ£€æŸ¥
        if self._is_express_number(candidate):
            logger.debug(f"è¯†åˆ«ä¸ºå¿«é€’å•å·: {candidate}")
            return False

        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„æ•°å­—
        digit_count = sum(1 for c in candidate if c.isdigit())
        if digit_count < rules['min_digits']:
            logger.debug(f"æ•°å­—æ•°é‡ä¸è¶³: {candidate} (æ•°å­—: {digit_count}, è¦æ±‚: {rules['min_digits']})")
            return False

        # æ£€æŸ¥å¿…éœ€çš„åˆ†éš”ç¬¦ï¼ˆå¦‚æœè¦æ±‚çš„è¯ï¼‰
        if rules.get('min_separator_count', 0) > 0:
            separator_count = sum(1 for sep in rules['required_separator'] if sep in candidate)
            if separator_count < rules.get('min_separator_count', 0):
                logger.debug(f"åˆ†éš”ç¬¦æ•°é‡ä¸è¶³: {candidate}")
                return False

        # æ£€æŸ¥æ— æ•ˆæ¨¡å¼
        for invalid_pattern in rules['invalid_patterns']:
            if re.match(invalid_pattern, candidate):
                logger.debug(f"åŒ¹é…æ— æ•ˆæ¨¡å¼: {candidate}")
                return False

        # é”€è´§å‡ºåº“å•å·ç‰¹å¾æ£€æŸ¥
        # 1. æ£€æŸ¥æ˜¯å¦ç¬¦åˆé”€è´§å•å·æ ¼å¼ (XXXX-XXXXXXXXXXXX)
        separators = ['-', '_', 'ï¼', 'â€”', ' ']
        for sep in separators:
            if sep in candidate:
                parts = candidate.split(sep)
                if len(parts) >= 2:
                    first_part = parts[0].strip()
                    second_part = parts[1].strip()

                    # ç¬¬ä¸€éƒ¨åˆ†åº”è¯¥æ˜¯3-5ä½æ•°å­—ï¼Œç¬¬äºŒéƒ¨åˆ†åº”è¯¥æ˜¯6-20ä½æ•°å­—ï¼ˆæ”¾å®½è¦æ±‚ï¼‰
                    if (first_part.isdigit() and 3 <= len(first_part) <= 5 and
                        second_part.isdigit() and 6 <= len(second_part) <= 20):

                        # æ£€æŸ¥æ˜¯å¦ä»¥é”€è´§å•å·å¸¸è§å‰ç¼€å¼€å¤´
                        sales_prefixes = rules.get('sales_order_prefixes', [])
                        if sales_prefixes and any(first_part.startswith(prefix) for prefix in sales_prefixes):
                            logger.debug(f"é”€è´§å•å·å‰ç¼€åŒ¹é…: {candidate}")
                            return True

                        # æˆ–è€…ç¬¦åˆä¸€èˆ¬æ ¼å¼è¦æ±‚
                        if len(first_part) == 4 and len(second_part) >= 8:
                            logger.debug(f"ç¬¦åˆä¸€èˆ¬æ ¼å¼: {candidate}")
                            return True
                break

        # æ£€æŸ¥çº¯æ•°å­—æ ¼å¼ï¼ˆ12-20ä½ï¼‰
        if candidate.isdigit() and 12 <= len(candidate) <= 20:
            logger.debug(f"çº¯æ•°å­—æ ¼å¼é€šè¿‡: {candidate}")
            return True

        # è°ƒè¯•æœŸé—´ï¼šå¯¹åŒ…å«è¶³å¤Ÿæ•°å­—çš„å€™é€‰æ”¾å®½è¦æ±‚
        if digit_count >= 8:  # å¦‚æœåŒ…å«8ä¸ªä»¥ä¸Šæ•°å­—ï¼Œå¯èƒ½æ˜¯æœ‰æ•ˆçš„
            # é¿å…è¿‡äºç®€å•çš„æ¨¡å¼
            if len(set(candidate)) >= 3:  # è‡³å°‘3ç§ä¸åŒå­—ç¬¦
                # æ£€æŸ¥æ•°å­—å­—æ¯æ¯”ä¾‹æ˜¯å¦åˆç†
                alpha_count = sum(1 for c in candidate if c.isalpha())
                # é”€è´§å•å·ä¸»è¦åº”è¯¥æ˜¯æ•°å­—å’Œåˆ†éš”ç¬¦ï¼Œå…è®¸å°‘é‡å­—æ¯
                if alpha_count <= len(candidate) * 0.3:  # å­—æ¯ä¸è¶…è¿‡30%
                    logger.debug(f"å®½æ¾æ¨¡å¼é€šè¿‡: {candidate} (æ•°å­—: {digit_count}, å­—ç¬¦ç§ç±»: {len(set(candidate))})")
                    return True

        logger.debug(f"éªŒè¯å¤±è´¥: {candidate} (æ•°å­—: {digit_count}, é•¿åº¦: {len(candidate)})")
        return False

    def _validate_strict_format(self, candidate: str) -> bool:
        """ä¸¥æ ¼éªŒè¯è®¢å•å·æ ¼å¼ï¼šå¿…é¡»æ˜¯4ä½-12ä½æ ¼å¼"""
        try:
            if not candidate:
                return False

            # æ¸…ç†å€™é€‰å·ç 
            clean_candidate = re.sub(r'[^\w-]', '', candidate).strip()

            # ä¸¥æ ¼æ ¼å¼æ£€æŸ¥ï¼š4ä½-12ä½
            strict_pattern = r'^(\d{4})[-_](\d{12})$'
            match = re.match(strict_pattern, clean_candidate)

            if not match:
                logger.debug(f"æ ¼å¼ä¸ç¬¦åˆ4ä½-12ä½è¦æ±‚: {candidate}")
                return False

            prefix, suffix = match.groups()

            # æ£€æŸ¥å‰ç¼€æ˜¯å¦ä¸ºæœ‰æ•ˆçš„é”€è´§å•å·å‰ç¼€
            valid_prefixes = ['1403', '1404', '1405', '1410', '1411', '1412']
            if prefix not in valid_prefixes:
                logger.debug(f"å‰ç¼€æ— æ•ˆ: {prefix}ï¼Œæœ‰æ•ˆå‰ç¼€: {valid_prefixes}")
                return False

            # æ£€æŸ¥åç¼€æ˜¯å¦å…¨ä¸ºæ•°å­—
            if not suffix.isdigit():
                logger.debug(f"åç¼€åŒ…å«éæ•°å­—å­—ç¬¦: {suffix}")
                return False

            logger.debug(f"âœ… ä¸¥æ ¼æ ¼å¼éªŒè¯é€šè¿‡: {clean_candidate}")
            return True

        except Exception as e:
            logger.error(f"ä¸¥æ ¼æ ¼å¼éªŒè¯å¤±è´¥: {e}")
            return False

    def find_all_order_candidates(self, text: str) -> list:
        """æ‰¾åˆ°æ–‡æœ¬ä¸­æ‰€æœ‰æ½œåœ¨çš„è®¢å•å·å€™é€‰"""
        candidates = []
        strict_candidates = []  # ç¬¦åˆä¸¥æ ¼æ ¼å¼çš„å€™é€‰

        # éå†æ‰€æœ‰æ¨¡å¼ï¼ŒæŒ‰ä¼˜å…ˆçº§æ”¶é›†å€™é€‰é¡¹
        for pattern_index, pattern in enumerate(self.patterns):
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                result = match.group(1).strip()
                if self._validate_order_number(result):
                    base_confidence = len(self.patterns) - pattern_index

                    # æ£€æŸ¥æ˜¯å¦ç¬¦åˆä¸¥æ ¼æ ¼å¼ï¼ˆ4ä½-12ä½ï¼‰
                    if self._validate_strict_format(result):
                        # ä¸¥æ ¼æ ¼å¼çš„å€™é€‰ç»™äºˆæ›´é«˜çš„ç½®ä¿¡åº¦
                        strict_candidates.append({
                            'number': result,
                            'confidence': base_confidence + 100,  # ä¸¥æ ¼æ ¼å¼é¢å¤–åŠ åˆ†
                            'pattern_index': pattern_index,
                            'format_type': 'strict'
                        })

                    # ä¹Ÿè®°å½•ä¸ºæ™®é€šå€™é€‰ï¼ˆå…¼å®¹æ€§ï¼‰
                    candidates.append({
                        'number': result,
                        'confidence': base_confidence,
                        'pattern_index': pattern_index,
                        'format_type': 'loose'
                    })

        # å¦‚æœæœ‰ä¸¥æ ¼æ ¼å¼çš„å€™é€‰ï¼Œä¼˜å…ˆè¿”å›å®ƒä»¬
        if strict_candidates:
            strict_candidates.sort(key=lambda x: (-x['confidence'], x['pattern_index']))
            logger.debug(f"ğŸ¯ æ‰¾åˆ° {len(strict_candidates)} ä¸ªä¸¥æ ¼æ ¼å¼å€™é€‰")
            return strict_candidates

        # å¦åˆ™è¿”å›æ™®é€šå€™é€‰
        candidates.sort(key=lambda x: (-x['confidence'], x['pattern_index']))
        logger.debug(f"âš ï¸ æœªæ‰¾åˆ°ä¸¥æ ¼æ ¼å¼å€™é€‰ï¼Œè¿”å› {len(candidates)} ä¸ªæ™®é€šå€™é€‰")
        return candidates

    def _compare_ocr_results(self, easyocr_results: list, paddleocr_results: list, log_messages: list) -> Optional[dict]:
        """æ¯”è¾ƒä¸¤ä¸ªOCRå¼•æ“çš„ç»“æœï¼Œé€‰æ‹©æœ€ä½³å€™é€‰"""
        all_candidates = []

        # æ”¶é›†EasyOCRå€™é€‰
        for result in easyocr_results:
            for candidate in result['candidates']:
                all_candidates.append({
                    'text': result['text'],
                    'method': f"EasyOCRåˆè¯†åˆ« ({result['info']})",
                    'number': candidate['number'],
                    'confidence': candidate['confidence'],
                    'source': 'easyocr'
                })

        # æ”¶é›†PaddleOCRå€™é€‰
        for result in paddleocr_results:
            for candidate in result['candidates']:
                all_candidates.append({
                    'text': result['text'],
                    'method': f"PaddleOCRç²¾ç¡®ç¡®è®¤ ({result['info']})",
                    'number': candidate['number'],
                    'confidence': candidate['confidence'],
                    'source': 'paddleocr'
                })

        if not all_candidates:
            return None

        # æ™ºèƒ½é€‰æ‹©ç­–ç•¥ï¼ˆPaddleOCRä¸»åŠ›ç­–ç•¥ï¼‰
        # 1. ä¼˜å…ˆä½¿ç”¨PaddleOCRç»“æœï¼ˆæ•°å­—è¯†åˆ«æ›´å‡†ç¡®ï¼‰
        paddleocr_candidates = [c for c in all_candidates if c['source'] == 'paddleocr']
        if paddleocr_candidates:
            best = max(paddleocr_candidates, key=lambda x: x['confidence'])
            log_messages.append(f"ğŸ”¥ PaddleOCRä¸»åŠ›è¯†åˆ«: {best['number']} (ç½®ä¿¡åº¦: {best['confidence']})")
            return best

        # 2. å¦‚æœPaddleOCRå¤±è´¥ï¼Œæ£€æŸ¥EasyOCRå’ŒPaddleOCRçš„å…±åŒç»“æœ
        easyocr_numbers = {c['number'] for c in all_candidates if c['source'] == 'easyocr'}
        paddleocr_numbers = {c['number'] for c in all_candidates if c['source'] == 'paddleocr'}
        common_numbers = easyocr_numbers & paddleocr_numbers

        if common_numbers:
            # é€‰æ‹©å…±åŒè¯†åˆ«çš„æœ€é«˜ç½®ä¿¡åº¦ç»“æœ
            common_candidates = [c for c in all_candidates if c['number'] in common_numbers]
            best = max(common_candidates, key=lambda x: x['confidence'])
            log_messages.append(f"ğŸ¯ åŒå¼•æ“ç¡®è®¤ç›¸åŒç»“æœ: {best['number']} (ç½®ä¿¡åº¦: {best['confidence']})")
            return best

        # 3. æœ€åå›é€€åˆ°EasyOCRç»“æœï¼ˆä»…ä½œå¤‡ç”¨ï¼‰
        easyocr_candidates = [c for c in all_candidates if c['source'] == 'easyocr']
        if easyocr_candidates:
            best = max(easyocr_candidates, key=lambda x: x['confidence'])
            log_messages.append(f"âš¡ å›é€€EasyOCRç»“æœ: {best['number']} (ç½®ä¿¡åº¦: {best['confidence']})")
            return best

        return None

    def _get_easyocr_reader(self):
        """è·å–EasyOCRè¯»å–å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
        if not self._easyocr_initialized:
            try:
                if not lazy_import_ocr():
                    return None

                logger.info("ğŸš€ åˆå§‹åŒ– EasyOCR å¿«é€Ÿè¯†åˆ«å¼•æ“ï¼ˆé¦–æ¬¡ä½¿ç”¨éœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œè¯·ç¨å€™...ï¼‰")
                # æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡ï¼ŒGPUåŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if easyocr is not None:
                    self._easyocr_reader = easyocr.Reader(['ch_sim', 'en'], gpu=True)
                    self._easyocr_initialized = True
                    logger.info("âœ… EasyOCR å¿«é€Ÿè¯†åˆ«å¼•æ“åˆå§‹åŒ–å®Œæˆ")

            except Exception as e:
                logger.warning(f"EasyOCR GPUæ¨¡å¼åˆå§‹åŒ–å¤±è´¥ï¼Œå°è¯•CPUæ¨¡å¼: {e}")
                try:
                    # å°è¯•ä»…CPUæ¨¡å¼
                    if easyocr is not None:
                        self._easyocr_reader = easyocr.Reader(['ch_sim', 'en'], gpu=False)
                        self._easyocr_initialized = True
                        logger.info("âœ… EasyOCR CPUæ¨¡å¼åˆå§‹åŒ–å®Œæˆ")
                except Exception as e2:
                    logger.error(f"EasyOCR CPUæ¨¡å¼ä¹Ÿå¤±è´¥: {e2}")
                    self._easyocr_reader = None
                    self._easyocr_initialized = True

        return self._easyocr_reader

    def _get_paddleocr_reader(self):
        """è·å–PaddleOCRè¯»å–å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
        if not self._paddleocr_initialized:
            try:
                if not lazy_import_ocr():
                    return None

                logger.info("ğŸ”¥ åˆå§‹åŒ– PaddleOCR ç²¾ç¡®ç¡®è®¤å¼•æ“ï¼ˆé¦–æ¬¡ä½¿ç”¨éœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œè¯·ç¨å€™...ï¼‰")
                
                # ä½¿ç”¨ä¿®å¤åçš„PaddleOCRé…ç½® (å…¼å®¹PaddleOCR 2.7.3)
                from paddleocr_v3_monkeypatch import get_paddle_ocr3_monkeypatch
                fix = get_paddle_ocr3_monkeypatch()
                
                if fix.is_available():
                    self._paddleocr_reader = fix
                    self._paddleocr_initialized = True
                    logger.info("âœ… PaddleOCRç²¾ç¡®ç¡®è®¤å¼•æ“åˆå§‹åŒ–å®Œæˆï¼ˆä½¿ç”¨å…¼å®¹æ€§ä¿®å¤ï¼‰")
                else:
                    logger.error("âŒ PaddleOCR å…¼å®¹æ€§ä¿®å¤å¤±è´¥")
                    self._paddleocr_reader = None
                    self._paddleocr_initialized = True

            except Exception as e:
                logger.error(f"PaddleOCRåˆå§‹åŒ–å®Œå…¨å¤±è´¥: {e}")
                self._paddleocr_reader = None
                self._paddleocr_initialized = True

        return self._paddleocr_reader

    def _extract_text_with_easyocr(self, image) -> tuple:
        """ä½¿ç”¨EasyOCRæå–æ–‡æœ¬"""
        try:
            reader = self._get_easyocr_reader()
            if reader is None:
                return None, "EasyOCRåˆå§‹åŒ–å¤±è´¥"

            # è½¬æ¢PILå›¾åƒä¸ºnumpyæ•°ç»„
            if hasattr(image, 'convert') and cv2 is not None and np is not None:
                # PILå›¾åƒ
                cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
            else:
                # å·²ç»æ˜¯numpyæ•°ç»„
                cv_image = image

            # EasyOCRè¯†åˆ«
            logger.info("âš¡ ä½¿ç”¨ EasyOCR è¿›è¡Œå¿«é€Ÿåˆè¯†åˆ«...")
            results = reader.readtext(cv_image, detail=1, paragraph=False)

            # åˆå¹¶æ‰€æœ‰è¯†åˆ«çš„æ–‡æœ¬
            text_parts = []
            confidence_scores = []

            for (bbox, text, confidence) in results:
                if confidence > 0.3:  # è¿‡æ»¤ä½ç½®ä¿¡åº¦æ–‡æœ¬
                    text_parts.append(text)
                    confidence_scores.append(confidence)

            full_text = ' '.join(text_parts)
            avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0

            info = f"è¯†åˆ«åˆ°{len(text_parts)}ä¸ªæ–‡æœ¬å—ï¼Œå¹³å‡ç½®ä¿¡åº¦:{avg_confidence:.2f}"
            logger.info(f"EasyOCRå¿«é€Ÿè¯†åˆ«ç»“æœ: {info}")

            return full_text, info

        except Exception as e:
            logger.error(f"EasyOCRè¯†åˆ«å¤±è´¥: {e}")
            return None, f"EasyOCRè¯†åˆ«å¤±è´¥: {str(e)}"

    def _extract_text_with_paddleocr(self, image) -> tuple:
        """ä½¿ç”¨PaddleOCRæå–æ–‡æœ¬"""
        try:
            reader = self._get_paddleocr_reader()
            if reader is None:
                return None, "PaddleOCRåˆå§‹åŒ–å¤±è´¥"

            # ä¿å­˜å›¾åƒä¸ºä¸´æ—¶æ–‡ä»¶ï¼Œå› ä¸ºæ–°çš„é€‚é…å™¨éœ€è¦æ–‡ä»¶è·¯å¾„
            import tempfile
            with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as temp_file:
                temp_path = temp_file.name
                
                # è½¬æ¢å¹¶ä¿å­˜å›¾åƒ
                if hasattr(image, 'save'):
                    # PILå›¾åƒ
                    image.save(temp_path)
                else:
                    # numpyæ•°ç»„
                    if Image is not None:
                        pil_image = Image.fromarray(image)
                        pil_image.save(temp_path)
                    else:
                        return None, "æ— æ³•ä¿å­˜ä¸´æ—¶å›¾åƒæ–‡ä»¶"

            try:
                # PaddleOCRè¯†åˆ«
                logger.info("ğŸ”¥ ä½¿ç”¨ PaddleOCR 3.0.1 è¿›è¡Œç²¾ç¡®ç¡®è®¤...")
                
                # ä½¿ç”¨æ–°çš„é€‚é…å™¨è¿›è¡ŒOCRè¯†åˆ«
                results = reader.predict_to_old_format(temp_path)

                # è§£æç»“æœï¼ˆå·²ç»æ˜¯æ—§æ ¼å¼ï¼‰
                text_parts = []
                confidence_scores = []

                if results and results[0]:  # results[0]æ˜¯ç¬¬ä¸€é¡µçš„ç»“æœ
                    for line in results[0]:
                        if len(line) >= 2:
                            text = line[1][0]  # æå–æ–‡æœ¬
                            confidence = line[1][1]  # æå–ç½®ä¿¡åº¦

                            if confidence > 0.3:  # è¿‡æ»¤ä½ç½®ä¿¡åº¦æ–‡æœ¬
                                text_parts.append(text)
                                confidence_scores.append(confidence)

                full_text = ' '.join(text_parts)
                avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0

                info = f"è¯†åˆ«åˆ°{len(text_parts)}ä¸ªæ–‡æœ¬å—ï¼Œå¹³å‡ç½®ä¿¡åº¦:{avg_confidence:.2f}"
                logger.info(f"PaddleOCR 3.0.1ç²¾ç¡®ç¡®è®¤ç»“æœ: {info}")

                return full_text, info
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    import os
                    if os.path.exists(temp_path):
                        os.unlink(temp_path)
                except Exception as cleanup_error:
                    logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {cleanup_error}")

        except Exception as e:
            logger.error(f"PaddleOCRè¯†åˆ«å¤±è´¥: {e}")
            return None, f"PaddleOCRè¯†åˆ«å¤±è´¥: {str(e)}"

    def _enhance_image_for_ocr(self, image):
        """ä¸“é—¨ä¸ºOCRä¼˜åŒ–çš„å›¾åƒå¢å¼ºå¤„ç†ï¼ˆåŸºäºæµ‹è¯•ç»“æœä¼˜åŒ–ï¼‰"""
        try:
            # è½¬æ¢ä¸ºç°åº¦
            if image.mode != 'L':
                gray = image.convert('L')
            else:
                gray = image

            # å¯¹æ¯”åº¦å¢å¼ºï¼ˆåŸºäºæµ‹è¯•ç»“æœï¼Œ1.8å€æ•ˆæœå¥½ï¼‰
            if ImageEnhance is not None:
                enhancer = ImageEnhance.Contrast(gray)
                enhanced = enhancer.enhance(1.8)
            else:
                enhanced = gray

            # è½¬æ¢ä¸ºOpenCVæ ¼å¼
            if np is not None and cv2 is not None:
                cv_image = np.array(enhanced)

                # é«˜æ–¯æ¨¡ç³Šå»å™ªï¼ˆè½»å¾®å»å™ªï¼‰
                blurred = cv2.GaussianBlur(cv_image, (1, 1), 0)

                # OTSUäºŒå€¼åŒ–ï¼ˆæµ‹è¯•è¯æ˜æ•ˆæœæœ€å¥½ï¼‰
                _, binary = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

                # è½»å¾®çš„å½¢æ€å­¦æ“ä½œæ¸…ç†å™ªå£°
                kernel = np.ones((1, 1), np.uint8)
                cleaned = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)

                logger.debug(f"åº”ç”¨ä¼˜åŒ–çš„OCRå›¾åƒå¢å¼ºå¤„ç†")

                if Image is not None:
                    return Image.fromarray(cleaned)

            return enhanced

        except Exception as e:
            logger.warning(f"å›¾åƒå¢å¼ºå¤±è´¥ï¼Œä½¿ç”¨åŸå›¾: {e}")
            return image

    def _enhance_for_digit_recognition(self, image):
        """ä¸“é—¨é’ˆå¯¹æ•°å­—è¯†åˆ«çš„å›¾åƒå¢å¼º"""
        try:
            if np is None or cv2 is None:
                logger.warning("numpyæˆ–cv2æœªå¯¼å…¥ï¼Œè·³è¿‡æ•°å­—è¯†åˆ«å¢å¼º")
                return image

            # è½¬æ¢ä¸ºç°åº¦
            if image.mode != 'L':
                gray = image.convert('L')
            else:
                gray = image

            cv_image = np.array(gray)

            # 1. é«˜æ–¯æ¨¡ç³Šå»å™ª
            denoised = cv2.GaussianBlur(cv_image, (1, 1), 0)

            # 2. ç›´æ–¹å›¾å‡è¡¡åŒ–å¢å¼ºå¯¹æ¯”åº¦
            equalized = cv2.equalizeHist(denoised)

            # 3. åŒè¾¹æ»¤æ³¢ä¿æŒè¾¹ç¼˜çš„åŒæ—¶å»å™ª
            bilateral = cv2.bilateralFilter(equalized, 9, 75, 75)

            # 4. å¤šçº§é˜ˆå€¼å¤„ç†
            # OTSUè‡ªåŠ¨é˜ˆå€¼
            _, otsu = cv2.threshold(bilateral, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

            # 5. å½¢æ€å­¦å¼€è¿ç®—å»é™¤å°å™ªå£°
            kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))
            opening = cv2.morphologyEx(otsu, cv2.MORPH_OPEN, kernel)

            # 6. è†¨èƒ€æ“ä½œå¢å¼ºæ•°å­—ç¬”ç”»
            kernel_dilate = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 1))
            final = cv2.dilate(opening, kernel_dilate, iterations=1)

            if Image is not None:
                return Image.fromarray(final)
            return image

        except Exception as e:
            logger.warning(f"æ•°å­—è¯†åˆ«å¢å¼ºå¤±è´¥: {e}")
            return image

    def detect_text_orientation(self, image) -> float:
        """ç²¾ç»†çš„æ–‡æœ¬æ–¹å‘æ£€æµ‹ï¼Œè¿”å›ç²¾ç¡®è§’åº¦ï¼ˆå°æ•°ï¼‰"""
        try:
            if cv2 is None or np is None:
                logger.warning("cv2æˆ–numpyæœªå¯¼å…¥ï¼Œè·³è¿‡æ–¹å‘æ£€æµ‹")
                return 0.0

            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„ä¾èµ–
            try:
                import scipy  # type: ignore
                import sklearn  # type: ignore
                use_advanced_detection = True
            except ImportError as import_error:
                logger.warning(f"é«˜çº§è§’åº¦æ£€æµ‹ä¾èµ–ç¼ºå¤±: {import_error}ï¼Œå°†ä½¿ç”¨åŸºç¡€æ£€æµ‹")
                use_advanced_detection = False

            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

            if use_advanced_detection:
                # ä½¿ç”¨æ–°çš„ç²¾ç»†æ£€æµ‹æ–¹æ³•
                try:
                    # æ–¹æ³•1: ç²¾ç»†éœå¤«çº¿å˜æ¢è§’åº¦æ£€æµ‹
                    angle1, confidence1, method1_info = self._detect_precise_angle_by_hough(gray)
                    logger.debug(f"ç²¾ç»†éœå¤«çº¿æ£€æµ‹: {angle1:.2f}Â° (ç½®ä¿¡åº¦: {confidence1:.3f}) - {method1_info}")

                    # æ–¹æ³•2: åŸºäºä¸»æˆåˆ†åˆ†æçš„è§’åº¦æ£€æµ‹
                    angle2, confidence2, method2_info = self._detect_angle_by_pca(gray)
                    logger.debug(f"PCAæ£€æµ‹: {angle2:.2f}Â° (ç½®ä¿¡åº¦: {confidence2:.3f}) - {method2_info}")

                    # æ–¹æ³•3: åŸºäºè¾¹ç¼˜æ–¹å‘çš„è§’åº¦æ£€æµ‹
                    angle3, confidence3, method3_info = self._detect_angle_by_edge_direction(gray)
                    logger.debug(f"è¾¹ç¼˜æ–¹å‘æ£€æµ‹: {angle3:.2f}Â° (ç½®ä¿¡åº¦: {confidence3:.3f}) - {method3_info}")

                    # åŠ æƒå¹³å‡ï¼Œç½®ä¿¡åº¦é«˜çš„æ–¹æ³•æƒé‡æ›´å¤§
                    angles = [angle1, angle2, angle3]
                    confidences = [confidence1, confidence2, confidence3]
                    methods_info = [method1_info, method2_info, method3_info]

                    # è¿‡æ»¤æ‰ç½®ä¿¡åº¦è¿‡ä½çš„ç»“æœ
                    valid_results = [(angle, conf, info) for angle, conf, info in zip(angles, confidences, methods_info) if conf > 0.1]

                    if valid_results:
                        # åŠ æƒå¹³å‡è®¡ç®—æœ€ç»ˆè§’åº¦
                        total_weight = sum(conf for _, conf, _ in valid_results)
                        if total_weight > 0:
                            weighted_angle = sum(angle * conf for angle, conf, _ in valid_results) / total_weight

                            # è®°å½•è¯¦ç»†çš„æ£€æµ‹ä¿¡æ¯
                            logger.debug(f"è§’åº¦æ£€æµ‹è¯¦æƒ…: éœå¤«çº¿={angle1:.2f}Â°(ç½®ä¿¡åº¦{confidence1:.3f}), PCA={angle2:.2f}Â°(ç½®ä¿¡åº¦{confidence2:.3f}), è¾¹ç¼˜={angle3:.2f}Â°(ç½®ä¿¡åº¦{confidence3:.3f})")
                            logger.debug(f"åŠ æƒå¹³å‡è§’åº¦: {weighted_angle:.2f}Â°")

                            # å¯¹è§’åº¦è¿›è¡Œåˆç†æ€§æ£€æŸ¥å’Œä¿®æ­£
                            final_angle = self._normalize_detected_angle(weighted_angle)
                            logger.debug(f"æœ€ç»ˆæ ‡å‡†åŒ–è§’åº¦: {final_angle:.2f}Â°")

                            return final_angle

                except Exception as detection_error:
                    logger.warning(f"ç²¾ç»†è§’åº¦æ£€æµ‹å¤±è´¥: {detection_error}ï¼Œå›é€€åˆ°åŸºç¡€æ£€æµ‹")
                    use_advanced_detection = False

            if not use_advanced_detection:
                # å›é€€åˆ°åŸºç¡€è§’åº¦æ£€æµ‹
                logger.info("ä½¿ç”¨åŸºç¡€è§’åº¦æ£€æµ‹æ–¹æ³•")
                return self._basic_angle_detection(gray)

            return 0.0

        except Exception as e:
            logger.warning(f"æ–‡æœ¬æ–¹å‘æ£€æµ‹å¤±è´¥: {e}")
            return 0.0

    def _basic_angle_detection(self, gray) -> float:
        """åŸºç¡€è§’åº¦æ£€æµ‹æ–¹æ³•ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        try:
            if cv2 is None or np is None:
                return 0.0

            # ç®€å•çš„éœå¤«çº¿æ£€æµ‹
            edges = cv2.Canny(gray, 50, 150)
            lines = cv2.HoughLines(edges, 1, np.pi/180, threshold=100)

            if lines is not None and len(lines) > 5:
                angles = []
                for line in lines:
                    rho, theta = line[0]
                    angle_deg = np.degrees(theta) - 90
                    if angle_deg > 45:
                        angle_deg -= 90
                    elif angle_deg < -45:
                        angle_deg += 90
                    angles.append(angle_deg)

                # ä½¿ç”¨ä¸­ä½æ•°ä½œä¸ºä»£è¡¨è§’åº¦
                import statistics
                median_angle = statistics.median(angles)
                logger.debug(f"åŸºç¡€æ£€æµ‹è§’åº¦: {median_angle:.2f}Â°")
                return median_angle

            return 0.0

        except Exception as e:
            logger.warning(f"åŸºç¡€è§’åº¦æ£€æµ‹å¤±è´¥: {e}")
            return 0.0

    def _detect_precise_angle_by_hough(self, gray) -> tuple:
        """ä½¿ç”¨ç²¾ç»†éœå¤«çº¿å˜æ¢æ£€æµ‹è§’åº¦"""
        try:
            if cv2 is None or np is None:
                return 0.0, 0.0, "cv2æˆ–numpyæœªå¯¼å…¥"

            # å¤šçº§é¢„å¤„ç†ä»¥å¢å¼ºçº¿æ¡æ£€æµ‹
            # 1. é«˜æ–¯æ¨¡ç³Šå»å™ª
            blurred = cv2.GaussianBlur(gray, (3, 3), 0)

            # 2. è‡ªé€‚åº”é˜ˆå€¼äºŒå€¼åŒ–
            binary = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                         cv2.THRESH_BINARY, 15, 10)

            # 3. å½¢æ€å­¦æ“ä½œå¢å¼ºçº¿æ¡
            kernel_horizontal = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 1))
            kernel_vertical = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 5))

            # åˆ†åˆ«å¢å¼ºæ°´å¹³å’Œå‚ç›´çº¿æ¡
            enhanced_h = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel_horizontal)
            enhanced_v = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel_vertical)
            enhanced = cv2.bitwise_or(enhanced_h, enhanced_v)

            # 4. è¾¹ç¼˜æ£€æµ‹
            edges = cv2.Canny(enhanced, 30, 100, apertureSize=3)

            # 5. ç²¾ç»†éœå¤«çº¿å˜æ¢ï¼ˆæ›´é«˜çš„è§’åº¦åˆ†è¾¨ç‡ï¼‰
            lines = cv2.HoughLines(edges, 1, np.pi/360, threshold=30)  # 0.5åº¦ç²¾åº¦

            if lines is not None and len(lines) >= 5:
                angle_data = []

                for line in lines:
                    rho, theta = line[0]
                    # å°†thetaè½¬æ¢ä¸ºè§’åº¦ï¼ŒèŒƒå›´[-90, 90]
                    angle_deg = np.degrees(theta) - 90

                    # æ ‡å‡†åŒ–è§’åº¦åˆ°[-45, 45]èŒƒå›´
                    if angle_deg > 45:
                        angle_deg -= 90
                    elif angle_deg < -45:
                        angle_deg += 90

                    angle_data.append(angle_deg)

                if angle_data:
                    # ä½¿ç”¨ç›´æ–¹å›¾åˆ†ææ‰¾åˆ°ä¸»å¯¼è§’åº¦
                    hist, bin_edges = np.histogram(angle_data, bins=180, range=(-45, 45))

                    # å¹³æ»‘ç›´æ–¹å›¾
                    try:
                        from scipy import ndimage
                        smoothed_hist = ndimage.gaussian_filter1d(hist.astype(float), sigma=1.0)
                    except ImportError:
                        # å¦‚æœæ²¡æœ‰scipyï¼Œä½¿ç”¨ç®€å•çš„ç§»åŠ¨å¹³å‡
                        smoothed_hist = hist.astype(float)
                        for i in range(1, len(smoothed_hist)-1):
                            smoothed_hist[i] = (hist[i-1] + hist[i] + hist[i+1]) / 3.0

                    # æ‰¾åˆ°å³°å€¼
                    peak_idx = np.argmax(smoothed_hist)
                    peak_angle = bin_edges[peak_idx] + (bin_edges[1] - bin_edges[0]) / 2
                    peak_strength = smoothed_hist[peak_idx]

                    # è®¡ç®—ç½®ä¿¡åº¦
                    total_strength = np.sum(smoothed_hist)
                    confidence = peak_strength / total_strength if total_strength > 0 else 0

                    # ä½¿ç”¨é‡å¿ƒæ³•è¿›ä¸€æ­¥ç²¾ç»†åŒ–è§’åº¦
                    window_size = 5
                    start_idx = max(0, peak_idx - window_size)
                    end_idx = min(len(smoothed_hist), peak_idx + window_size + 1)

                    weights = smoothed_hist[start_idx:end_idx]
                    angles = bin_edges[start_idx:end_idx] + (bin_edges[1] - bin_edges[0]) / 2

                    if np.sum(weights) > 0:
                        refined_angle = np.average(angles, weights=weights)
                    else:
                        refined_angle = peak_angle

                    info = f"æ£€æµ‹åˆ°{len(lines)}æ¡çº¿ï¼Œä¸»å¯¼è§’åº¦{refined_angle:.2f}Â°ï¼Œç½®ä¿¡åº¦{confidence:.3f}"

                    return refined_angle, confidence, info

            return 0.0, 0.0, f"æ£€æµ‹åˆ°{len(lines) if lines is not None else 0}æ¡çº¿ï¼Œä¸è¶³ä»¥åˆ¤æ–­"

        except Exception as e:
            logger.warning(f"ç²¾ç»†éœå¤«çº¿å˜æ¢è§’åº¦æ£€æµ‹å¤±è´¥: {e}")
            return 0.0, 0.0, f"æ£€æµ‹å¤±è´¥: {str(e)}"

    def _detect_angle_by_pca(self, gray) -> tuple:
        """ä½¿ç”¨ä¸»æˆåˆ†åˆ†ææ£€æµ‹æ–‡æœ¬æ–¹å‘"""
        try:
            # äºŒå€¼åŒ–
            _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

            # æ‰¾åˆ°æ‰€æœ‰å‰æ™¯åƒç´ ç‚¹
            coords = np.column_stack(np.where(binary > 0))

            if len(coords) < 100:  # éœ€è¦è¶³å¤Ÿçš„ç‚¹è¿›è¡ŒPCA
                return 0.0, 0.0, "å‰æ™¯åƒç´ ç‚¹ä¸è¶³"

            # éšæœºé‡‡æ ·å‡å°‘è®¡ç®—é‡
            if len(coords) > 10000:
                indices = np.random.choice(len(coords), 10000, replace=False)
                coords = coords[indices]

            # æ‰§è¡ŒPCA
            try:
                from sklearn.decomposition import PCA
                pca = PCA(n_components=2)
                pca.fit(coords)
            except ImportError:
                return 0.0, 0.0, "sklearnæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨PCAæ£€æµ‹"

            # è·å–ä¸»æˆåˆ†æ–¹å‘
            principal_component = pca.components_[0]

            # è®¡ç®—è§’åº¦
            angle_rad = np.arctan2(principal_component[1], principal_component[0])
            angle_deg = np.degrees(angle_rad)

            # æ ‡å‡†åŒ–è§’åº¦åˆ°[-45, 45]èŒƒå›´
            if angle_deg > 45:
                angle_deg -= 90
            elif angle_deg < -45:
                angle_deg += 90

            # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºæ–¹å·®æ¯”ï¼‰
            explained_variance_ratio = pca.explained_variance_ratio_
            confidence = explained_variance_ratio[0] - explained_variance_ratio[1]
            confidence = max(0, min(1, confidence))  # é™åˆ¶åœ¨[0,1]èŒƒå›´

            info = f"PCAåˆ†æ{len(coords)}ä¸ªç‚¹ï¼Œç¬¬ä¸€ä¸»æˆåˆ†è§£é‡Š{explained_variance_ratio[0]:.3f}æ–¹å·®"

            return angle_deg, confidence, info

        except Exception as e:
            logger.warning(f"PCAè§’åº¦æ£€æµ‹å¤±è´¥: {e}")
            return 0.0, 0.0, f"PCAæ£€æµ‹å¤±è´¥: {str(e)}"

    def _detect_angle_by_edge_direction(self, gray) -> tuple:
        """åŸºäºè¾¹ç¼˜æ–¹å‘çš„è§’åº¦æ£€æµ‹"""
        try:
            # Sobelæ¢¯åº¦è®¡ç®—
            grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
            grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)

            # è®¡ç®—æ¢¯åº¦å¹…å€¼å’Œæ–¹å‘
            magnitude = np.sqrt(grad_x**2 + grad_y**2)
            direction = np.arctan2(grad_y, grad_x)

            # åªä¿ç•™å¼ºè¾¹ç¼˜
            threshold = np.percentile(magnitude, 85)
            strong_edges = magnitude > threshold

            if np.sum(strong_edges) < 100:
                return 0.0, 0.0, "å¼ºè¾¹ç¼˜ç‚¹ä¸è¶³"

            # è·å–å¼ºè¾¹ç¼˜çš„æ–¹å‘
            strong_directions = direction[strong_edges]

            # å°†æ–¹å‘è½¬æ¢ä¸ºè§’åº¦ï¼ˆåº¦ï¼‰
            angles_deg = np.degrees(strong_directions)

            # å°†è§’åº¦æ˜ å°„åˆ°æ–‡æœ¬æ–¹å‘ï¼ˆå‚ç›´äºè¾¹ç¼˜æ–¹å‘ï¼‰
            text_angles = angles_deg + 90

            # æ ‡å‡†åŒ–åˆ°[-45, 45]èŒƒå›´
            text_angles = text_angles % 180
            text_angles[text_angles > 90] -= 180
            text_angles[text_angles > 45] -= 90
            text_angles[text_angles < -45] += 90

            # ä½¿ç”¨ç›´æ–¹å›¾æ‰¾åˆ°ä¸»å¯¼æ–¹å‘
            hist, bin_edges = np.histogram(text_angles, bins=180, range=(-45, 45))

            # å¹³æ»‘ç›´æ–¹å›¾
            try:
                from scipy import ndimage
                smoothed_hist = ndimage.gaussian_filter1d(hist.astype(float), sigma=1.0)
            except ImportError:
                # å¦‚æœæ²¡æœ‰scipyï¼Œä½¿ç”¨ç®€å•çš„ç§»åŠ¨å¹³å‡
                smoothed_hist = hist.astype(float)
                for i in range(1, len(smoothed_hist)-1):
                    smoothed_hist[i] = (hist[i-1] + hist[i] + hist[i+1]) / 3.0

            # æ‰¾åˆ°å³°å€¼
            peak_idx = np.argmax(smoothed_hist)
            peak_angle = bin_edges[peak_idx] + (bin_edges[1] - bin_edges[0]) / 2
            peak_strength = smoothed_hist[peak_idx]

            # è®¡ç®—ç½®ä¿¡åº¦
            total_strength = np.sum(smoothed_hist)
            confidence = peak_strength / total_strength if total_strength > 0 else 0

            info = f"åˆ†æ{np.sum(strong_edges)}ä¸ªå¼ºè¾¹ç¼˜ç‚¹ï¼Œä¸»å¯¼æ–¹å‘{peak_angle:.2f}Â°"

            return peak_angle, confidence, info

        except Exception as e:
            logger.warning(f"è¾¹ç¼˜æ–¹å‘è§’åº¦æ£€æµ‹å¤±è´¥: {e}")
            return 0.0, 0.0, f"è¾¹ç¼˜æ£€æµ‹å¤±è´¥: {str(e)}"

    def _normalize_detected_angle(self, angle: float) -> float:
        """æ ‡å‡†åŒ–æ£€æµ‹åˆ°çš„è§’åº¦"""
        try:
            # é™åˆ¶è§’åº¦åœ¨åˆç†èŒƒå›´å†…
            angle = angle % 360

            # è½¬æ¢åˆ°[-180, 180]èŒƒå›´
            if angle > 180:
                angle -= 360

            # å¯¹äºæ–‡æ¡£æ‰«æï¼Œé€šå¸¸å€¾æ–œè§’åº¦ä¸ä¼šè¶…è¿‡Â±45åº¦
            # å¦‚æœè§’åº¦å¤ªå¤§ï¼Œå¯èƒ½æ˜¯æ£€æµ‹é”™è¯¯
            if abs(angle) > 45:
                # å¯èƒ½æ˜¯90åº¦çš„å€æ•° + å°è§’åº¦
                if angle > 45 and angle < 135:
                    angle = 90 + (angle - 90)
                elif angle > 135 or angle < -135:
                    angle = 180 + (angle - 180) if angle > 0 else 180 + (angle + 180)
                elif angle < -45 and angle > -135:
                    angle = -90 + (angle + 90)

            # æœ€ç»ˆé™åˆ¶åœ¨[-45, 45]èŒƒå›´ï¼Œå¯¹äºOCRæ¥è¯´è¿™æ˜¯åˆç†çš„
            if angle > 45:
                angle = 45
            elif angle < -45:
                angle = -45

            return angle

        except Exception as e:
            logger.warning(f"è§’åº¦æ ‡å‡†åŒ–å¤±è´¥: {e}")
            return 0.0

    def _generate_angle_sequence(self, detected_angle: float) -> list:
        """ç”Ÿæˆæ™ºèƒ½çš„è§’åº¦å°è¯•åºåˆ—"""
        try:
            angles_to_try = []

            # 1. ä¼˜å…ˆå°è¯•æ£€æµ‹åˆ°çš„ç²¾ç¡®è§’åº¦
            if abs(detected_angle) > 0.1:  # åªæœ‰å½“æ£€æµ‹è§’åº¦æœ‰æ„ä¹‰æ—¶æ‰åŠ å…¥
                angles_to_try.append(detected_angle)

            # 2. æ·»åŠ æ£€æµ‹è§’åº¦çš„ç²¾ç»†è°ƒæ•´
            if abs(detected_angle) > 0.5:
                # å¯¹äºè¾ƒå¤§çš„æ£€æµ‹è§’åº¦ï¼Œå°è¯•Â±0.5Â°çš„å¾®è°ƒ
                for delta in [-0.5, 0.5, -1.0, 1.0]:
                    adjusted_angle = detected_angle + delta
                    if abs(adjusted_angle) <= 45:  # ä¿æŒåœ¨åˆç†èŒƒå›´å†…
                        angles_to_try.append(adjusted_angle)

            # 3. æ€»æ˜¯åŒ…å«0åº¦ï¼ˆæ­£å¸¸æ–¹å‘ï¼‰
            if 0.0 not in angles_to_try:
                angles_to_try.append(0.0)

            # 4. æ£€æŸ¥æ˜¯å¦æ¥è¿‘å¸¸è§æ—‹è½¬è§’åº¦
            common_angles = [90, 180, 270, -90, -180]
            for common_angle in common_angles:
                diff = abs(detected_angle - common_angle)
                if diff < 45:  # å¦‚æœæ£€æµ‹è§’åº¦æ¥è¿‘æŸä¸ªå¸¸è§è§’åº¦
                    # æ·»åŠ è¯¥å¸¸è§è§’åº¦åŠå…¶å¾®è°ƒ
                    if common_angle not in angles_to_try:
                        angles_to_try.append(common_angle)

                    # ä¸ºå¸¸è§è§’åº¦æ·»åŠ ç²¾ç»†è°ƒæ•´
                    for delta in [-2, -1, -0.5, 0.5, 1, 2]:
                        adjusted = common_angle + delta
                        if adjusted not in angles_to_try and abs(adjusted) <= 180:
                            angles_to_try.append(adjusted)

            # 5. æ·»åŠ å…¶ä»–å¯èƒ½çš„æ—‹è½¬è§’åº¦ï¼ˆå¦‚æœè¿˜æ²¡æœ‰åŒ…å«ï¼‰
            standard_angles = [0, 90, 180, 270, -90, -180]
            for angle in standard_angles:
                if angle not in angles_to_try:
                    angles_to_try.append(angle)

            # 6. å¦‚æœæ£€æµ‹è§’åº¦å¾ˆå°ï¼Œæ·»åŠ ä¸€äº›å°è§’åº¦çš„ç³»ç»Ÿæ€§å°è¯•
            if abs(detected_angle) < 5:
                small_angles = [-3, -2, -1, -0.5, 0.5, 1, 2, 3]
                for small_angle in small_angles:
                    if small_angle not in angles_to_try:
                        angles_to_try.append(small_angle)

            # 7. å»é‡å¹¶æ’åºï¼ˆä¿æŒæ£€æµ‹è§’åº¦ä¼˜å…ˆï¼‰
            # æŒ‰ä¸æ£€æµ‹è§’åº¦çš„æ¥è¿‘ç¨‹åº¦æ’åº
            def angle_priority(angle):
                if abs(angle - detected_angle) < 0.1:
                    return 0  # æ£€æµ‹è§’åº¦æœ¬èº«æœ€é«˜ä¼˜å…ˆçº§
                elif abs(angle) < 0.1:
                    return 1  # 0åº¦æ¬¡ä¼˜å…ˆçº§
                else:
                    return abs(angle - detected_angle)  # å…¶ä»–æŒ‰æ¥è¿‘ç¨‹åº¦æ’åº

            # å»é‡
            unique_angles = []
            for angle in angles_to_try:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸è¿‘çš„è§’åº¦
                is_duplicate = False
                for existing in unique_angles:
                    if abs(angle - existing) < 0.1:
                        is_duplicate = True
                        break
                if not is_duplicate:
                    unique_angles.append(angle)

            # æ’åº
            unique_angles.sort(key=angle_priority)

            # é™åˆ¶å°è¯•æ¬¡æ•°ï¼ˆé¿å…è¿‡åº¦è®¡ç®—ï¼‰
            max_attempts = 12
            final_angles = unique_angles[:max_attempts]

            return final_angles

        except Exception as e:
            logger.warning(f"ç”Ÿæˆè§’åº¦åºåˆ—å¤±è´¥: {e}")
            # å›é€€åˆ°åŸºæœ¬åºåˆ—
            return [0, detected_angle, 90, 270, 180] if abs(detected_angle) > 0.1 else [0, 90, 270, 180]

    def rotate_image(self, image, angle):
        """å®‰å…¨åœ°æ—‹è½¬å›¾åƒ"""
        try:
            if angle == 0:
                return image

            # è·å–å›¾åƒå°ºå¯¸
            height, width = image.shape[:2]
            center = (width // 2, height // 2)

            # åˆ›å»ºæ—‹è½¬çŸ©é˜µ
            rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)

            # è®¡ç®—æ–°çš„è¾¹ç•Œæ¡†å°ºå¯¸
            cos_val = np.abs(rotation_matrix[0, 0])
            sin_val = np.abs(rotation_matrix[0, 1])
            new_width = int((height * sin_val) + (width * cos_val))
            new_height = int((height * cos_val) + (width * sin_val))

            # è°ƒæ•´æ—‹è½¬çŸ©é˜µçš„å¹³ç§»éƒ¨åˆ†
            rotation_matrix[0, 2] += (new_width / 2) - center[0]
            rotation_matrix[1, 2] += (new_height / 2) - center[1]

            # æ‰§è¡Œæ—‹è½¬
            rotated = cv2.warpAffine(image, rotation_matrix, (new_width, new_height),
                                   flags=cv2.INTER_CUBIC,
                                   borderMode=cv2.BORDER_CONSTANT,
                                   borderValue=(255, 255, 255))

            return rotated

        except Exception as e:
            logger.warning(f"å›¾åƒæ—‹è½¬å¤±è´¥: {e}")
            return image

    def extract_with_ocr(self, pdf_path: str) -> Tuple[Optional[str], List[str]]:
        """ä½¿ç”¨OCRä»PDFä¸­æå–æ–‡æœ¬å¹¶æŸ¥æ‰¾é”€è´§å‡ºåº“å•å·ï¼ŒåŒ…å«æ”¹è¿›çš„æ—‹è½¬æ£€æµ‹"""
        log_messages = []

        if not lazy_import_ocr():
            return None, ["âŒ OCRåº“å¯¼å…¥å¤±è´¥"]

        try:
            doc = fitz.open(pdf_path)

            for page_num in range(len(doc)):
                page = doc[page_num]
                log_messages.append(f"ğŸ“„ å¤„ç†ç¬¬{page_num + 1}é¡µ")

                # æé«˜åˆ†è¾¨ç‡ä»¥æ”¹å–„OCRæ•ˆæœ
                mat = fitz.Matrix(2.0, 2.0)  # 200 DPI
                pix = page.get_pixmap(matrix=mat)
                img_data = pix.tobytes("png")
                pix = None

                # è½¬æ¢ä¸ºPILå›¾åƒ
                pil_image = Image.open(io.BytesIO(img_data))

                # é™åˆ¶å›¾åƒå¤§å°ä»¥èŠ‚çœå†…å­˜
                max_size = 1500
                if max(pil_image.size) > max_size:
                    ratio = max_size / max(pil_image.size)
                    new_size = tuple(int(dim * ratio) for dim in pil_image.size)
                    pil_image = pil_image.resize(new_size, Image.Resampling.LANCZOS)

                # ç¡®ä¿å›¾åƒæ˜¯RGBæ¨¡å¼
                if pil_image.mode != 'RGB':
                    pil_image = pil_image.convert('RGB')

                # è½¬æ¢ä¸ºOpenCVæ ¼å¼
                cv_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)

                # ç²¾ç»†è§’åº¦æ£€æµ‹
                detected_angle = self.detect_text_orientation(cv_image)
                log_messages.append(f"ğŸ” ç²¾ç»†è§’åº¦æ£€æµ‹ç»“æœ: {detected_angle:.2f}Â°")

                # ç”Ÿæˆæ™ºèƒ½è§’åº¦å°è¯•åºåˆ—ï¼Œä¼˜å…ˆè€ƒè™‘å¤§è§’åº¦æ—‹è½¬
                angles_to_try = self._generate_angle_sequence(detected_angle)

                # æ ¹æ®æµ‹è¯•ç»“æœï¼Œä¼˜å…ˆå°è¯•270Â°å’Œ90Â°ï¼ˆè¿™äº›PDFå¯èƒ½éœ€è¦å¤§è§’åº¦æ—‹è½¬ï¼‰
                priority_angles = [270, 90, 0, 180]

                # å°†ä¼˜å…ˆè§’åº¦æ”¾åœ¨å‰é¢ï¼Œç„¶åæ˜¯æ£€æµ‹åˆ°çš„ç²¾ç»†è§’åº¦
                final_angles = []
                for angle in priority_angles:
                    if angle not in final_angles:
                        final_angles.append(angle)

                # æ·»åŠ æ£€æµ‹åˆ°çš„ç²¾ç»†è§’åº¦
                for angle in angles_to_try:
                    if angle not in final_angles:
                        final_angles.append(angle)

                angles_to_try = final_angles
                log_messages.append(f"ğŸ“ è§’åº¦å°è¯•åºåˆ—: {[f'{a:.1f}Â°' for a in angles_to_try[:8]]}...")

                # å¦‚æœæ£€æµ‹è§’åº¦è¾ƒå°ï¼Œæå‰å°è¯•ç²¾ç»†æ ¡æ­£
                if abs(detected_angle) < 0.5:
                    log_messages.append("ğŸ’¡ æ£€æµ‹åˆ°å¾®å°å€¾æ–œï¼Œå°†é‡ç‚¹å°è¯•ç²¾ç»†æ ¡æ­£")

                for angle in angles_to_try:
                    try:
                        # æ—‹è½¬å›¾åƒ
                        if angle != 0:
                            rotated_cv = self.rotate_image(cv_image, angle)
                            rotated_pil = Image.fromarray(cv2.cvtColor(rotated_cv, cv2.COLOR_BGR2RGB))
                        else:
                            rotated_pil = pil_image

                        # æ™ºèƒ½åŒºåŸŸè¯†åˆ«ä¼˜å…ˆç­–ç•¥
                        try:
                            from smart_region_ocr import smart_ocr

                            # å…ˆå°è¯•æ™ºèƒ½åŒºåŸŸè¯†åˆ«
                            smart_result, smart_logs = smart_ocr.smart_extract_order_number(rotated_pil)

                            # æ·»åŠ æ™ºèƒ½è¯†åˆ«æ—¥å¿—
                            log_messages.extend(smart_logs)

                            if smart_result:
                                # æ™ºèƒ½åŒºåŸŸè¯†åˆ«æˆåŠŸï¼ŒéªŒè¯ç»“æœ
                                candidates = self.find_all_order_candidates(smart_result)
                                if candidates:
                                    best_candidate = candidates[0]
                                    order_number = best_candidate['number']
                                    log_messages.append(f"ğŸ¯ æ™ºèƒ½åŒºåŸŸè¯†åˆ«æˆåŠŸ: {order_number} (ç½®ä¿¡åº¦: {best_candidate['confidence']})")
                                    doc.close()
                                    return order_number, log_messages
                                else:
                                    log_messages.append(f"âš ï¸ æ™ºèƒ½åŒºåŸŸè¯†åˆ«ç»“æœéªŒè¯å¤±è´¥: {smart_result}")

                        except ImportError:
                            log_messages.append("âš ï¸ æ™ºèƒ½åŒºåŸŸè¯†åˆ«æ¨¡å—æœªåŠ è½½ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
                        except Exception as smart_error:
                            log_messages.append(f"âš ï¸ æ™ºèƒ½åŒºåŸŸè¯†åˆ«å¤±è´¥: {smart_error}")

                        # å¦‚æœæ™ºèƒ½è¯†åˆ«å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
                        log_messages.append("ğŸ”„ å›é€€åˆ°ä¼ ç»Ÿå…¨å›¾OCRè¯†åˆ«")

                        # å¤šå±‚å›¾åƒå¢å¼ºå¤„ç†
                        enhanced_image = self._enhance_image_for_ocr(rotated_pil)

                        # PaddleOCRä¸»åŠ›ç­–ç•¥ï¼šPaddleOCRä¼˜å…ˆ + EasyOCRå¤‡ç”¨
                        easyocr_results = []
                        paddleocr_results = []
                        final_text = ""
                        ocr_method_used = ""

                        # ç¬¬ä¸€æ­¥ï¼šPaddleOCRä¸»åŠ›è¯†åˆ«
                        log_messages.append(f"ğŸ”¥ ç¬¬ä¸€æ­¥ï¼šPaddleOCRä¸»åŠ›è¯†åˆ« (è§’åº¦: {angle:.1f}Â°)...")
                        paddleocr_text, paddleocr_info = self._extract_text_with_paddleocr(enhanced_image)

                        paddleocr_found_candidates = False
                        if paddleocr_text and paddleocr_text.strip():
                            # æ·»åŠ è¯¦ç»†çš„OCRæ–‡æœ¬è°ƒè¯•ä¿¡æ¯
                            log_messages.append(f"ğŸ” PaddleOCRè¯†åˆ«æ–‡æœ¬: {paddleocr_text[:200]}...")

                            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ½œåœ¨çš„è®¢å•å·å€™é€‰
                            potential_orders = self.find_all_order_candidates(paddleocr_text)
                            if potential_orders:
                                paddleocr_results.append({
                                    'text': paddleocr_text,
                                    'info': paddleocr_info,
                                    'candidates': potential_orders
                                })
                                paddleocr_found_candidates = True
                                log_messages.append(f"âœ… PaddleOCRæ‰¾åˆ°{len(potential_orders)}ä¸ªå€™é€‰: {[c['number'] for c in potential_orders[:3]]}")

                                # PaddleOCRæˆåŠŸï¼Œç›´æ¥ä½¿ç”¨ç»“æœ
                                final_text = paddleocr_text
                                ocr_method_used = f"PaddleOCRä¸»åŠ›è¯†åˆ« ({paddleocr_info})"
                            else:
                                # æ˜¾ç¤ºä¸ºä»€ä¹ˆæ²¡æœ‰æ‰¾åˆ°å€™é€‰çš„è¯¦ç»†ä¿¡æ¯
                                log_messages.append(f"âš ï¸ PaddleOCRè¯†åˆ«åˆ°æ–‡æœ¬ä½†æ— æœ‰æ•ˆå€™é€‰")
                                log_messages.append(f"ğŸ“ PaddleOCRå®Œæ•´æ–‡æœ¬: {paddleocr_text}")

                                # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯
                                keywords = ['é”€è´§', 'å‡ºåº“', 'å•å·', 'è®¢å•', 'ç¼–å·']
                                found_keywords = [kw for kw in keywords if kw in paddleocr_text]
                                if found_keywords:
                                    log_messages.append(f"ğŸ’¡ PaddleOCRå‘ç°å…³é”®è¯: {found_keywords}")

                                # æ£€æŸ¥æ˜¯å¦åŒ…å«æ•°å­—æ¨¡å¼
                                import re
                                number_patterns = re.findall(r'\d{3,}[-_]?\d*', paddleocr_text)
                                if number_patterns:
                                    log_messages.append(f"ğŸ”¢ PaddleOCRå‘ç°æ•°å­—æ¨¡å¼: {number_patterns[:5]}")
                        else:
                            log_messages.append(f"âš ï¸ PaddleOCRæœªè¯†åˆ«åˆ°æœ‰æ•ˆæ–‡æœ¬")

                        # ç¬¬äºŒæ­¥ï¼šå¦‚æœPaddleOCRå¤±è´¥ï¼Œä½¿ç”¨EasyOCRå¤‡ç”¨
                        if not paddleocr_found_candidates:
                            log_messages.append(f"âš¡ ç¬¬äºŒæ­¥ï¼šPaddleOCRæ— å€™é€‰ï¼ŒEasyOCRå¤‡ç”¨è¯†åˆ«...")
                            easyocr_text, easyocr_info = self._extract_text_with_easyocr(enhanced_image)

                            if easyocr_text and easyocr_text.strip():
                                # æ·»åŠ EasyOCRçš„è°ƒè¯•ä¿¡æ¯
                                log_messages.append(f"ğŸ” EasyOCRå¤‡ç”¨è¯†åˆ«æ–‡æœ¬: {easyocr_text[:200]}...")

                                # æ£€æŸ¥EasyOCRçš„å€™é€‰
                                easyocr_candidates = self.find_all_order_candidates(easyocr_text)
                                if easyocr_candidates:
                                    easyocr_results.append({
                                        'text': easyocr_text,
                                        'info': easyocr_info,
                                        'candidates': easyocr_candidates
                                    })
                                    final_text = easyocr_text
                                    ocr_method_used = f"EasyOCRå¤‡ç”¨è¯†åˆ« ({easyocr_info})"
                                    log_messages.append(f"âœ… EasyOCRæ‰¾åˆ°{len(easyocr_candidates)}ä¸ªå€™é€‰: {[c['number'] for c in easyocr_candidates[:3]]}")
                                else:
                                    final_text = easyocr_text
                                    ocr_method_used = f"EasyOCRå¤‡ç”¨è¯†åˆ« ({easyocr_info})"
                                    log_messages.append(f"âš ï¸ EasyOCRè¯†åˆ«åˆ°æ–‡æœ¬ä½†æ— æœ‰æ•ˆå€™é€‰")
                                    log_messages.append(f"ğŸ“ EasyOCRå®Œæ•´æ–‡æœ¬: {easyocr_text}")

                                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯
                                    keywords = ['é”€è´§', 'å‡ºåº“', 'å•å·', 'è®¢å•', 'ç¼–å·']
                                    found_keywords = [kw for kw in keywords if kw in easyocr_text]
                                    if found_keywords:
                                        log_messages.append(f"ğŸ’¡ EasyOCRå‘ç°å…³é”®è¯: {found_keywords}")

                                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ•°å­—æ¨¡å¼
                                    import re
                                    number_patterns = re.findall(r'\d{3,}[-_]?\d*', easyocr_text)
                                    if number_patterns:
                                        log_messages.append(f"ğŸ”¢ EasyOCRå‘ç°æ•°å­—æ¨¡å¼: {number_patterns[:5]}")
                            else:
                                log_messages.append(f"âŒ PaddleOCRå’ŒEasyOCRå‡æœªè¯†åˆ«åˆ°æœ‰æ•ˆæ–‡æœ¬")
                        else:
                            log_messages.append(f"ğŸ¯ PaddleOCRä¸»åŠ›è¯†åˆ«æˆåŠŸï¼Œè·³è¿‡EasyOCRå¤‡ç”¨")

                        text = final_text

                        if text.strip():
                            # åº”ç”¨æ•°å­—é”™è¯¯çº æ­£
                            corrected_text = text
                            try:
                                from digit_enhancement import digit_enhancer
                                corrected_text = digit_enhancer.correct_common_digit_errors(text)
                                if corrected_text != text:
                                    log_messages.append(f"ğŸ”§ åº”ç”¨æ•°å­—çº æ­£: {repr(text[:100])} -> {repr(corrected_text[:100])}")
                            except ImportError:
                                log_messages.append("âš ï¸ æ•°å­—å¢å¼ºæ¨¡å—æœªåŠ è½½")
                            except Exception as correction_error:
                                log_messages.append(f"âš ï¸ æ•°å­—çº æ­£å¤±è´¥: {correction_error}")

                            log_messages.append(f"ğŸ“ ä½¿ç”¨{ocr_method_used}è¯†åˆ«æ–‡æœ¬ç‰‡æ®µ: {corrected_text[:150]}...")

                            # æŸ¥æ‰¾é”€è´§å‡ºåº“å•å·ï¼ˆä½¿ç”¨çº æ­£åçš„æ–‡æœ¬ï¼‰
                            candidates = self.find_all_order_candidates(corrected_text)
                            if candidates:
                                best_candidate = candidates[0]  # å·²æŒ‰ç½®ä¿¡åº¦æ’åº
                                order_number = best_candidate['number']

                                # æ£€æŸ¥æ˜¯å¦ä¸ºä¸¥æ ¼æ ¼å¼ï¼ˆ4ä½-12ä½ï¼‰
                                if self._validate_strict_format(order_number):
                                    log_messages.append(f"âœ… æ‰¾åˆ°æ ‡å‡†æ ¼å¼é”€è´§å‡ºåº“å•å·: {order_number} (ä½¿ç”¨{ocr_method_used}ï¼Œç½®ä¿¡åº¦: {best_candidate['confidence']})")
                                    doc.close()
                                    return order_number, log_messages
                                else:
                                    # éä¸¥æ ¼æ ¼å¼ï¼Œè®°å½•ä½†ç»§ç»­å°è¯•å…¶ä»–è§’åº¦
                                    log_messages.append(f"âš ï¸ æ‰¾åˆ°éæ ‡å‡†æ ¼å¼è®¢å•å·: {order_number} (ä¸ç¬¦åˆ4ä½-12ä½æ ¼å¼ï¼Œç»§ç»­å°è¯•å…¶ä»–è§’åº¦)")
                                    # å¯ä»¥é€‰æ‹©åœ¨æœ€åå›é€€åˆ°è¿™ä¸ªç»“æœ
                                    if not hasattr(self, '_fallback_candidate'):
                                        self._fallback_candidate = (order_number, log_messages.copy())
                        else:
                            log_messages.append(f"âš ï¸ è§’åº¦{angle:.1f}Â°æ‰€æœ‰OCRæ–¹æ³•éƒ½æœªè¯†åˆ«åˆ°æ–‡æœ¬")

                    except Exception as e:
                        log_messages.append(f"âš ï¸ è§’åº¦{angle:.1f}Â°å¤„ç†å¤±è´¥: {str(e)}")
                        continue

            doc.close()

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸¥æ ¼æ ¼å¼ï¼Œä½†æœ‰éæ ‡å‡†æ ¼å¼çš„å€™é€‰ï¼Œä½œä¸ºæœ€åçš„å›é€€
            if hasattr(self, '_fallback_candidate'):
                fallback_number, fallback_logs = self._fallback_candidate
                delattr(self, '_fallback_candidate')  # æ¸…ç†ä¸´æ—¶å±æ€§
                log_messages.append(f"ğŸ”„ å›é€€åˆ°éæ ‡å‡†æ ¼å¼ç»“æœ: {fallback_number} (æ ¼å¼ä¸å®Œå…¨ç¬¦åˆ4ä½-12ä½è¦æ±‚)")
                return fallback_number, log_messages

            log_messages.append("âŒ æ‰€æœ‰é¡µé¢å’Œè§’åº¦éƒ½æœªæ‰¾åˆ°é”€è´§å‡ºåº“å•å·")
            return None, log_messages

        except Exception as e:
            error_msg = f"âŒ OCRå¤„ç†å¤±è´¥: {str(e)}"
            log_messages.append(error_msg)
            logger.error(error_msg, exc_info=True)
            return None, log_messages

    def clean_filename(self, order_number: str) -> str:
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦"""
        try:
            # ç§»é™¤æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦
            cleaned = re.sub(r'[<>:"/\\|?*]', '_', order_number)
            # ç§»é™¤é¦–å°¾ç©ºæ ¼
            cleaned = cleaned.strip()
            # ç¡®ä¿ä¸ä¸ºç©º
            if not cleaned:
                cleaned = "unknown_order"
            return cleaned
        except Exception as e:
            logger.error(f"æ–‡ä»¶åæ¸…ç†å¤±è´¥: {e}")
            return "unknown_order"

processor = PDFProcessor()

@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    """ä¸»é¡µ"""
    return templates.TemplateResponse("index.html", {"request": request})

@app.get("/favicon.ico")
async def favicon():
    """æä¾›favicon.icoï¼Œé¿å…404é”™è¯¯"""
    # è¿”å›ä¸€ä¸ªç®€å•çš„SVGå›¾æ ‡
    svg_content = """<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100">
        <rect width="100" height="100" fill="#ff6b9d"/>
        <text x="50" y="65" text-anchor="middle" font-size="60" fill="white">ğŸ“„</text>
    </svg>"""

    from fastapi.responses import Response
    return Response(content=svg_content, media_type="image/svg+xml")

def create_backup(file_path: str, original_filename: str) -> str:
    """åˆ›å»ºæ–‡ä»¶å¤‡ä»½"""
    try:
        from datetime import datetime
        import shutil

        # åˆ›å»ºæ—¥æœŸæ–‡ä»¶å¤¹
        today = datetime.now().strftime("%Y-%m-%d")
        backup_dir = Path(f"backup/{today}")
        backup_dir.mkdir(parents=True, exist_ok=True)

        # ç”Ÿæˆå¤‡ä»½æ–‡ä»¶è·¯å¾„
        backup_path = backup_dir / original_filename

        # å¦‚æœå¤‡ä»½æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ æ—¶é—´æˆ³
        if backup_path.exists():
            timestamp = datetime.now().strftime("%H%M%S")
            name_part = backup_path.stem
            ext_part = backup_path.suffix
            backup_path = backup_dir / f"{name_part}_{timestamp}{ext_part}"

        # å¤åˆ¶æ–‡ä»¶åˆ°å¤‡ä»½ç›®å½•
        shutil.copy2(file_path, backup_path)

        logger.info(f"ğŸ“‚ å¤‡ä»½åˆ›å»ºæˆåŠŸ: {backup_path}")
        return str(backup_path)

    except Exception as e:
        logger.error(f"å¤‡ä»½åˆ›å»ºå¤±è´¥: {e}")
        raise

@app.post("/upload")
async def upload_files(files: List[UploadFile] = File(...), enableBackup: str = "true"):
    """ä¸Šä¼ æ–‡ä»¶å¹¶æå–é”€è´§å‡ºåº“å•å·è¿›è¡Œé‡å‘½å"""
    global filename_mapping

    # å¼€å§‹å¤„ç†å‰æ¸…ç†downloadsæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶
    cleaned_count = clean_all_downloads()
    if cleaned_count > 0:
        logger.info(f"ğŸ§¹ å¤„ç†å¼€å§‹å‰æ¸…ç†äº† {cleaned_count} ä¸ªæ–‡ä»¶ï¼ˆåŒ…æ‹¬ä¸Šæ¬¡å¤„ç†çš„æ–‡ä»¶å’Œè°ƒè¯•æ–‡ä»¶ï¼‰")
    else:
        logger.info("ğŸ§¹ downloadsæ–‡ä»¶å¤¹ä¸ºç©ºï¼Œæ— éœ€æ¸…ç†")

    # è®°å½•å¤„ç†å¼€å§‹æ—¶downloadsç›®å½•çŠ¶æ€
    downloads_dir = Path("downloads")
    if downloads_dir.exists():
        existing_files = list(downloads_dir.glob("*.pdf"))
        logger.info(f"ğŸ“Š æ‰¹æ¬¡å¤„ç†å¼€å§‹ - downloadsç›®å½•ç°æœ‰æ–‡ä»¶: {len(existing_files)}ä¸ª")
        for existing_file in existing_files:
            logger.info(f"  å·²å­˜åœ¨: {existing_file.name}")
    else:
        logger.info(f"ğŸ“Š æ‰¹æ¬¡å¤„ç†å¼€å§‹ - downloadsç›®å½•ä¸å­˜åœ¨")

    if not files:
        raise HTTPException(status_code=400, detail="æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶")

    processor = PDFProcessor()
    results = []
    processed_count = 0
    backup_enabled = enableBackup.lower() == "true"

    logger.info(f"å¤„ç†æ¨¡å¼: {'å¯ç”¨å¤‡ä»½' if backup_enabled else 'ç¦ç”¨å¤‡ä»½'}")

    for file in files:
        if not file.filename or not file.filename.lower().endswith('.pdf'):
            results.append({
                "filename": file.filename or "unknown",
                "success": False,
                "message": "âŒ ä¸æ˜¯PDFæ–‡ä»¶"
            })
            continue

        upload_path = None
        try:
            # å¤„ç†æ–‡ä»¶åï¼Œå»é™¤è·¯å¾„ä¿¡æ¯ï¼ˆæ–‡ä»¶å¤¹ä¸Šä¼ æ—¶çš„è·¯å¾„ï¼‰
            clean_filename = os.path.basename(file.filename) if file.filename else "unknown.pdf"

            # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°uploadsæ ¹ç›®å½•
            upload_path = f"uploads/{clean_filename}"

            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ æ—¶é—´æˆ³é¿å…å†²çª
            if os.path.exists(upload_path):
                from datetime import datetime
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                name_part = os.path.splitext(clean_filename)[0]
                ext_part = os.path.splitext(clean_filename)[1]
                upload_path = f"uploads/{name_part}_{timestamp}{ext_part}"

            with open(upload_path, "wb") as buffer:
                content = await file.read()
                if not content:
                    raise ValueError("æ–‡ä»¶å†…å®¹ä¸ºç©º")
                buffer.write(content)

            logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file.filename} -> {clean_filename}")

            # å¤„ç†PDFæ–‡ä»¶
            order_number, log = processor.extract_order_number(upload_path)

            if order_number:
                # æ¸…ç†æ–‡ä»¶å
                clean_order = processor.clean_filename(order_number)
                new_filename = f"{clean_order}.pdf"

                # æ ¹æ®å¤‡ä»½æ¨¡å¼å†³å®šæ˜¯å¦åˆ›å»ºå¤‡ä»½
                backup_path = "æœªå¯ç”¨å¤‡ä»½"
                if backup_enabled:
                    # ä½¿ç”¨åŸå§‹æ–‡ä»¶åè¿›è¡Œå¤‡ä»½ï¼Œä¿æŒæ–‡ä»¶å¤¹ç»“æ„ä¿¡æ¯
                    backup_filename = file.filename if file.filename else clean_filename
                    backup_path = create_backup(upload_path, backup_filename)

                # å¤„ç†é‡åæ–‡ä»¶ - ç¡®ä¿downloadsç›®å½•å­˜åœ¨
                os.makedirs("downloads", exist_ok=True)
                counter = 1
                base_name = clean_order
                final_filename = new_filename

                # åŠ å¼ºæ–‡ä»¶åå†²çªæ£€æµ‹ - æŸ¥æ‰¾æ•´ä¸ªdownloadsç›®å½•ï¼Œä¸ä»…æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§è¿˜è€ƒè™‘å¤„ç†é˜Ÿåˆ—ä¸­çš„å…¶ä»–æ–‡ä»¶
                existing_downloads = set(os.path.basename(f) for f in glob.glob("downloads/*.pdf"))
                in_process_names = set()  # è·Ÿè¸ªå½“å‰æ‰¹æ¬¡ä¸­å·²ä½¿ç”¨çš„æ–‡ä»¶å

                # æ£€æŸ¥resultsä¸­æ˜¯å¦å·²æœ‰ç›¸åŒæ–‡ä»¶åï¼ˆåœ¨å½“å‰æ‰¹æ¬¡ä¸­ï¼‰
                for result in results:
                    if result.get("new_filename"):
                        in_process_names.add(result["new_filename"])

                logger.info(f"æ–‡ä»¶åæ£€æŸ¥ - åŸºç¡€åç§°: {base_name}, åˆå§‹æ–‡ä»¶å: {final_filename}")
                logger.info(f"å·²å­˜åœ¨çš„ä¸‹è½½æ–‡ä»¶: {len(existing_downloads)}ä¸ª, å½“å‰æ‰¹æ¬¡ä¸­å·²ä½¿ç”¨åç§°: {len(in_process_names)}ä¸ª")

                # ç¡®ä¿æ–‡ä»¶åä¸å†²çª
                while final_filename in existing_downloads or final_filename in in_process_names:
                    counter += 1
                    final_filename = f"{base_name}_{counter}.pdf"
                    logger.info(f"æ–‡ä»¶åå†²çªï¼Œå°è¯•æ–°åç§°: {final_filename}")

                # å­˜å‚¨æ–‡ä»¶åæ˜ å°„å…³ç³» - é‡å‘½åååˆ°åŸå§‹æ–‡ä»¶åçš„æ˜ å°„
                original_filename = os.path.basename(upload_path)
                filename_mapping[final_filename] = original_filename

                # åˆ›å»ºé‡å‘½åå‰¯æœ¬åˆ°downloadsæ–‡ä»¶å¤¹ - å¢å¼ºæ–‡ä»¶æ“ä½œç¨³å®šæ€§
                download_path = f"downloads/{final_filename}"
                try:
                    import shutil
                    import time

                    # ç¡®ä¿downloadsç›®å½•å­˜åœ¨ä¸”å¯å†™
                    downloads_dir = Path("downloads")
                    downloads_dir.mkdir(exist_ok=True)

                    # å¤šæ¬¡å°è¯•æ–‡ä»¶å¤åˆ¶ï¼Œæé«˜æˆåŠŸç‡
                    copy_success = False
                    max_copy_attempts = 3

                    for attempt in range(max_copy_attempts):
                        try:
                            # å¤åˆ¶æ–‡ä»¶å‰é¢å¤–æ£€æŸ¥
                            if not os.path.exists(upload_path) or os.path.getsize(upload_path) == 0:
                                raise Exception(f"æºæ–‡ä»¶æ— æ•ˆæˆ–å¤§å°ä¸º0: {upload_path}")

                            # å¤åˆ¶å‰æ£€æŸ¥ç›®æ ‡æ˜¯å¦å·²å­˜åœ¨ï¼ˆåŒé‡æ£€æŸ¥ï¼‰
                            if os.path.exists(download_path):
                                logger.warning(f"ç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œå°†è¢«è¦†ç›–: {download_path}")
                                # æ·»åŠ é¢å¤–çš„å®‰å…¨ä¿éšœ - é‡å‘½åå·²å­˜åœ¨çš„æ–‡ä»¶è€Œä¸æ˜¯è¦†ç›–
                                backup_filename = f"{download_path}.bak.{int(time.time())}"
                                os.rename(download_path, backup_filename)
                                logger.info(f"å·²å­˜åœ¨çš„æ–‡ä»¶å·²å¤‡ä»½ä¸º: {backup_filename}")

                            # å¤åˆ¶æ–‡ä»¶
                            shutil.copy2(upload_path, download_path)

                            # éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
                            if os.path.exists(download_path) and os.path.getsize(download_path) > 0:
                                # é¢å¤–éªŒè¯ï¼šæ£€æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦ä¸€è‡´
                                if os.path.getsize(upload_path) == os.path.getsize(download_path):
                                    # æ–‡ä»¶å†…å®¹éªŒè¯
                                    with open(upload_path, "rb") as src_file:
                                        src_data = src_file.read(1024)  # è¯»å–å‰1KBåšå®Œæ•´æ€§éªŒè¯
                                    with open(download_path, "rb") as dst_file:
                                        dst_data = dst_file.read(1024)

                                    if src_data == dst_data:
                                        copy_success = True
                                        logger.info(f"æ–‡ä»¶å·²æˆåŠŸä¿å­˜åˆ°ä¸‹è½½ç›®å½•: {download_path} (å°è¯•{attempt+1}/{max_copy_attempts})")

                                        # é¢å¤–æ£€æŸ¥æ–‡ä»¶æ˜¯å¦çœŸå®å­˜åœ¨ï¼ˆé˜²æ­¢NFSç¼“å­˜ç­‰é—®é¢˜ï¼‰
                                        time.sleep(0.1)  # çŸ­æš‚ç­‰å¾…æ–‡ä»¶ç³»ç»ŸåŒæ­¥
                                        if os.path.exists(download_path):
                                            logger.info(f"æ–‡ä»¶ç¡®è®¤å­˜åœ¨: {download_path}, å¤§å°: {os.path.getsize(download_path)}å­—èŠ‚")
                                        else:
                                            raise Exception("æ–‡ä»¶ç³»ç»ŸåŒæ­¥åæ–‡ä»¶ä¸å­˜åœ¨")

                                        break
                                    else:
                                        raise Exception("æ–‡ä»¶å†…å®¹éªŒè¯å¤±è´¥ï¼Œå¯èƒ½å¤åˆ¶ä¸å®Œæ•´")
                                else:
                                    raise Exception("æ–‡ä»¶å¤§å°ä¸ä¸€è‡´ï¼Œå¯èƒ½å¤åˆ¶ä¸å®Œæ•´")
                            else:
                                raise Exception("æ–‡ä»¶å¤åˆ¶åä¸å­˜åœ¨æˆ–å¤§å°ä¸º0")

                        except Exception as attempt_error:
                            logger.warning(f"æ–‡ä»¶å¤åˆ¶å°è¯•{attempt+1}å¤±è´¥: {attempt_error}")
                            # æ¸…ç†å¯èƒ½çš„ä¸å®Œæ•´æ–‡ä»¶
                            if os.path.exists(download_path):
                                try:
                                    os.remove(download_path)
                                    logger.info(f"å·²æ¸…ç†ä¸å®Œæ•´æ–‡ä»¶: {download_path}")
                                except:
                                    logger.warning(f"æ¸…ç†ä¸å®Œæ•´æ–‡ä»¶å¤±è´¥: {download_path}")

                            if attempt < max_copy_attempts - 1:
                                time.sleep(0.5 * (attempt + 1))  # é€’å¢ç­‰å¾…æ—¶é—´åé‡è¯•
                                logger.info(f"å°†åœ¨{0.5 * (attempt + 1)}ç§’åé‡è¯•")
                            else:
                                raise attempt_error

                    if not copy_success:
                        raise Exception(f"ç»è¿‡{max_copy_attempts}æ¬¡å°è¯•ï¼Œæ–‡ä»¶å¤åˆ¶ä»ç„¶å¤±è´¥")

                    # å®‰å…¨åˆ é™¤uploadsä¸­çš„ä¸´æ—¶æ–‡ä»¶
                    if os.path.exists(upload_path):
                        try:
                            os.remove(upload_path)
                            logger.info(f"ä¸´æ—¶æ–‡ä»¶å·²åˆ é™¤: {upload_path}")
                        except Exception as del_error:
                            logger.warning(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥ï¼ˆä½†ä¸å½±å“ä¸»æµç¨‹ï¼‰: {del_error}")

                except Exception as copy_error:
                    logger.error(f"æ–‡ä»¶æ“ä½œå¤±è´¥: {copy_error}")
                    # å°è¯•æ¸…ç†å¯èƒ½çš„ä¸å®Œæ•´æ–‡ä»¶
                    if os.path.exists(download_path):
                        try:
                            os.remove(download_path)
                        except:
                            pass
                    raise Exception(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {copy_error}")

                results.append({
                    "filename": file.filename,
                    "success": True,
                    "message": f"âœ… æ–‡ä»¶å¤„ç†æˆåŠŸ: {file.filename} â†’ {final_filename}",
                    "new_filename": final_filename,
                    "backup_path": backup_path,
                    "order_number": order_number,
                    "log": log,
                    "download_ready": True
                })

                logger.info(f"æ–‡ä»¶å¤„ç†æˆåŠŸ: {file.filename} -> {final_filename}")
                processed_count += 1

            else:
                results.append({
                    "filename": file.filename,
                    "success": False,
                    "message": "âŒ æœªæ‰¾åˆ°é”€è´§å‡ºåº“å•å·",
                    "log": log
                })

                logger.warning(f"æœªæ‰¾åˆ°é”€è´§å‡ºåº“å•å·: {file.filename}")

        except Exception as e:
            error_msg = f"å¤„ç†å¤±è´¥: {str(e)}"
            results.append({
                "filename": file.filename or "unknown",
                "success": False,
                "message": f"âŒ {error_msg}",
                "log": f"ç³»ç»Ÿé”™è¯¯: {str(e)}"
            })
            logger.error(f"æ–‡ä»¶å¤„ç†å¤±è´¥ {file.filename}: {error_msg}", exc_info=True)

        finally:
            # æ¸…ç†uploadsä¸­çš„ä¸´æ—¶æ–‡ä»¶
            if upload_path and os.path.exists(upload_path) and upload_path.startswith("uploads/"):
                try:
                    os.remove(upload_path)
                except Exception as e:
                    logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")

            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            import gc
            gc.collect()

    logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {processed_count} ä¸ªæ–‡ä»¶")

    # è®°å½•å¤„ç†ç»“æŸæ—¶downloadsç›®å½•çŠ¶æ€
    if downloads_dir.exists():
        final_files = list(downloads_dir.glob("*.pdf"))
        logger.info(f"ğŸ“Š æ‰¹æ¬¡å¤„ç†ç»“æŸ - downloadsç›®å½•ç°æœ‰æ–‡ä»¶: {len(final_files)}ä¸ª")
        for final_file in final_files:
            logger.info(f"  æœ€ç»ˆå­˜åœ¨: {final_file.name}")
    else:
        logger.info(f"ğŸ“Š æ‰¹æ¬¡å¤„ç†ç»“æŸ - downloadsç›®å½•ä¸å­˜åœ¨")

    # ç”Ÿæˆå¤„ç†ä¿¡æ¯
    process_info = None
    if processed_count > 0:
        from datetime import datetime
        download_info = f"ğŸ“¥ å¤„ç†å®Œæˆï¼Œ{processed_count} ä¸ªæ–‡ä»¶å¯ä¾›ä¸‹è½½"
        if backup_enabled:
            backup_info = f"å¤‡ä»½ä¿å­˜åœ¨: backup/{datetime.now().strftime('%Y-%m-%d')}/"
            process_info = f"{download_info}ï¼Œ{backup_info}"
        else:
            process_info = download_info

    return JSONResponse({
        "results": results,
        "processed_count": processed_count,
        "total_count": len(files),
        "process_info": process_info,
        "download_available": processed_count > 0
    })

@app.get("/backup-info")
async def get_backup_info():
    """è·å–å¤‡ä»½ä¿¡æ¯"""
    try:
        backup_root = Path("backup")
        if not backup_root.exists():
            return JSONResponse({
                "backup_folders": [],
                "total_backups": 0,
                "message": "æš‚æ— å¤‡ä»½æ–‡ä»¶"
            })

        backup_folders = []
        total_backups = 0

        # è·å–æ‰€æœ‰æ—¥æœŸæ–‡ä»¶å¤¹
        for date_folder in sorted(backup_root.iterdir(), reverse=True):
            if date_folder.is_dir():
                pdf_files = list(date_folder.glob("*.pdf"))
                if pdf_files:
                    backup_folders.append({
                        "date": date_folder.name,
                        "file_count": len(pdf_files),
                        "files": [f.name for f in pdf_files]
                    })
                    total_backups += len(pdf_files)

        return JSONResponse({
            "backup_folders": backup_folders,
            "total_backups": total_backups,
            "message": f"å…±æœ‰ {len(backup_folders)} å¤©çš„å¤‡ä»½ï¼Œæ€»è®¡ {total_backups} ä¸ªæ–‡ä»¶"
        })

    except Exception as e:
        logger.error(f"è·å–å¤‡ä»½ä¿¡æ¯å¤±è´¥: {e}")
        return JSONResponse({
            "backup_folders": [],
            "total_backups": 0,
            "message": f"è·å–å¤‡ä»½ä¿¡æ¯å¤±è´¥: {str(e)}"
        })

@app.get("/download-list")
async def get_download_list():
    """è·å–å¯ä¸‹è½½æ–‡ä»¶åˆ—è¡¨"""
    downloads_dir = Path("downloads")
    files = []

    try:
        if downloads_dir.exists():
            pdf_files = list(downloads_dir.glob("*.pdf"))

            # æŸ¥æ‰¾æ‰€æœ‰ä¸‹è½½æ–‡ä»¶ï¼Œä¼˜å…ˆä½¿ç”¨é‡å‘½ååæ–‡ä»¶å
            renamed_files = []
            for file in pdf_files:
                # æ£€æŸ¥æ­¤æ–‡ä»¶æ˜¯å¦æ˜¯é‡å‘½ååçš„æ–‡ä»¶ï¼ˆé€šè¿‡æ£€æŸ¥å‘½åæ¨¡å¼ï¼‰
                if "-" in file.stem and file.stem.count("-") == 1:
                    renamed_files.append(file.name)
                else:
                    # å¯¹äºåŸå§‹æ–‡ä»¶åæ ¼å¼ï¼ŒæŸ¥æ‰¾æ˜¯å¦æœ‰å¯¹åº”çš„é‡å‘½åæ˜ å°„
                    found = False
                    for renamed, original in filename_mapping.items():
                        if file.name == original:
                            renamed_files.append(renamed)
                            found = True
                            break
                    if not found:
                        # å¦‚æœæ²¡æœ‰æ˜ å°„å…³ç³»ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ–‡ä»¶å
                        renamed_files.append(file.name)

            files = renamed_files
            logger.info(f"ä¸‹è½½ç›®å½•æ£€æŸ¥: æ‰¾åˆ° {len(files)} ä¸ªPDFæ–‡ä»¶")

            # è¯¦ç»†æ—¥å¿—
            for file in files:
                logger.info(f"  - {file}")
        else:
            logger.warning("ä¸‹è½½ç›®å½•ä¸å­˜åœ¨")

        return JSONResponse({
            "files": files,
            "count": len(files),
            "message": f"æ‰¾åˆ° {len(files)} ä¸ªå¯ä¸‹è½½æ–‡ä»¶",
            "downloads_dir": str(downloads_dir),
            "dir_exists": downloads_dir.exists()
        })
    except Exception as e:
        logger.error(f"è·å–ä¸‹è½½åˆ—è¡¨å¤±è´¥: {e}")
        return JSONResponse({
            "files": [],
            "count": 0,
            "message": f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}",
            "error": str(e)
        })

@app.get("/download/{filename}")
async def download_single_file(filename: str):
    """ä¸‹è½½å•ä¸ªæ–‡ä»¶"""
    try:
        downloads_dir = Path("downloads")

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ–‡ä»¶åæ˜ å°„
        actual_filename = filename
        mapped_original = None

        # å°è¯•ä»æ˜ å°„ä¸­æŸ¥æ‰¾å¯¹åº”çš„åŸå§‹æ–‡ä»¶å
        if filename in filename_mapping:
            mapped_original = filename_mapping[filename]
            logger.info(f"æ–‡ä»¶åæ˜ å°„: {filename} -> {mapped_original}")

        # å…ˆå°è¯•ç›´æ¥æŸ¥æ‰¾é‡å‘½ååçš„æ–‡ä»¶
        file_path = downloads_dir / filename
        if not file_path.exists() and mapped_original:
            # å¦‚æœé‡å‘½ååçš„æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•æŸ¥æ‰¾åŸå§‹æ–‡ä»¶
            file_path = downloads_dir / mapped_original
            actual_filename = mapped_original
            logger.info(f"ä½¿ç”¨åŸå§‹æ–‡ä»¶åè·¯å¾„: {file_path}")

        # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œå°è¯•æŸ¥æ‰¾å¯èƒ½åŒ¹é…çš„æ–‡ä»¶
        if not file_path.exists():
            logger.info(f"å°è¯•æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶ï¼ŒåŸå§‹åç§°: {filename}")
            # å°è¯•æ‰¾åˆ°å«æœ‰ç›¸ä¼¼å‘½åçš„æ–‡ä»¶
            matching_files = []

            for potential_file in downloads_dir.glob("*.pdf"):
                # 1. å°è¯•åŸå§‹æ–‡ä»¶åä¸­åŒ…å«è®¢å•å·çš„éƒ¨åˆ†
                if "-" in filename:
                    order_number = filename.split("-")[1].replace(".pdf", "")
                    if order_number in potential_file.name:
                        matching_files.append(potential_file)
                        logger.info(f"æ‰¾åˆ°è®¢å•å·åŒ¹é…æ–‡ä»¶: {potential_file.name} (åŒ…å« {order_number})")

                # 2. å°è¯•åŒ¹é…å‰ç¼€éƒ¨åˆ†
                if filename.startswith(potential_file.stem[:5]):
                    matching_files.append(potential_file)
                    logger.info(f"æ‰¾åˆ°å‰ç¼€åŒ¹é…æ–‡ä»¶: {potential_file.name}")

            # å¦‚æœæœ‰åŒ¹é…çš„æ–‡ä»¶ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª
            if matching_files:
                file_path = matching_files[0]
                actual_filename = file_path.name
                logger.info(f"ä½¿ç”¨æœ€ä½³åŒ¹é…æ–‡ä»¶: {file_path}")

        # è¯¦ç»†æ—¥å¿—è®°å½•
        logger.info(f"è¯·æ±‚ä¸‹è½½æ–‡ä»¶: {filename} -> å®é™…æ–‡ä»¶å: {actual_filename}")
        logger.info(f"å¯»æ‰¾è·¯å¾„: {file_path}")

        # å®‰å…¨æ€§æ£€æŸ¥ï¼šç¡®ä¿æ–‡ä»¶åä¸åŒ…å«è·¯å¾„éå†æ”»å‡»
        if ".." in filename or "/" in filename or "\\" in filename:
            logger.warning(f"ä¸‹è½½è¯·æ±‚åŒ…å«æ— æ•ˆçš„æ–‡ä»¶å: {filename}")
            raise HTTPException(status_code=400, detail="æ— æ•ˆçš„æ–‡ä»¶å (åŒ…å«éæ³•å­—ç¬¦)")

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not downloads_dir.exists():
            logger.warning(f"ä¸‹è½½ç›®å½•ä¸å­˜åœ¨: {downloads_dir}")
            raise HTTPException(
                status_code=404,
                detail=f"ä¸‹è½½ç›®å½•ä¸å­˜åœ¨ï¼Œè¯·å…ˆå¤„ç†æ–‡ä»¶æˆ–ä½¿ç”¨ä¿®å¤åŠŸèƒ½"
            )

        if not file_path.exists():
            logger.warning(f"è¯·æ±‚çš„æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

            # æŸ¥æ‰¾æ‰€æœ‰å¯ç”¨æ–‡ä»¶
            available_files = list(downloads_dir.glob("*.pdf"))
            available_names = [f.name for f in available_files]

            detail_msg = f"æ–‡ä»¶ '{filename}' ä¸å­˜åœ¨"
            if available_files:
                detail_msg += f"ã€‚å¯ç”¨æ–‡ä»¶: {', '.join(available_names[:5])}"
                if len(available_names) > 5:
                    detail_msg += f" ç­‰å…± {len(available_names)} ä¸ªæ–‡ä»¶"

            raise HTTPException(status_code=404, detail=detail_msg)

        if not file_path.is_file():
            logger.warning(f"è·¯å¾„å­˜åœ¨ä½†ä¸æ˜¯æ–‡ä»¶: {file_path}")
            raise HTTPException(status_code=404, detail=f"'{filename}' ä¸æ˜¯ä¸€ä¸ªæ–‡ä»¶")

        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        if not file_path.name.lower().endswith('.pdf'):
            logger.warning(f"è¯·æ±‚ä¸‹è½½éPDFæ–‡ä»¶: {file_path.name}")
            raise HTTPException(status_code=400, detail="åªæ”¯æŒä¸‹è½½PDFæ–‡ä»¶")

        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = file_path.stat().st_size
        if file_size == 0:
            logger.warning(f"æ–‡ä»¶å¤§å°ä¸º0: {file_path}")
            raise HTTPException(status_code=400, detail="æ–‡ä»¶å¤§å°ä¸º0ï¼Œå¯èƒ½å·²æŸå")

        logger.info(f"å•ä¸ªæ–‡ä»¶ä¸‹è½½æˆåŠŸ: {filename} -> {file_path.name}, å¤§å°: {file_size} å­—èŠ‚")

        # è®¾ç½®å“åº”å¤´ç¡®ä¿æµè§ˆå™¨ç›´æ¥ä¸‹è½½è€Œä¸æ˜¯é¢„è§ˆ
        headers = {
            "Content-Disposition": f'attachment; filename="{filename}"',
            "Content-Type": "application/pdf",
            "Cache-Control": "no-cache, no-store, must-revalidate",
            "Pragma": "no-cache",
            "Expires": "0"
        }

        # è¿”å›å®é™…æ‰¾åˆ°çš„æ–‡ä»¶ï¼Œä½†ä¿æŒç”¨æˆ·è¯·æ±‚çš„ä¸‹è½½æ–‡ä»¶å
        # å¼ºåˆ¶æµè§ˆå™¨ä¸‹è½½è€Œä¸æ˜¯é¢„è§ˆï¼Œä½¿ç”¨æ­£ç¡®çš„å¤´éƒ¨ä¿¡æ¯
        headers.update({
            "Content-Type": "application/octet-stream",  # ä½¿ç”¨é€šç”¨äºŒè¿›åˆ¶æµç±»å‹å¼ºåˆ¶ä¸‹è½½
            "X-Content-Type-Options": "nosniff"          # é˜²æ­¢æµè§ˆå™¨çŒœæµ‹å†…å®¹ç±»å‹
        })
        return FileResponse(
            file_path,
            filename=filename,  # ä½¿ç”¨ç”¨æˆ·è¯·æ±‚çš„æ–‡ä»¶åä½œä¸ºä¸‹è½½åç§°
            media_type="application/octet-stream",  # ä¼˜å…ˆä½¿ç”¨é€šç”¨ç±»å‹å¼ºåˆ¶ä¸‹è½½
            headers=headers
        )

    except HTTPException:
        # ç»§ç»­æŠ›å‡ºHTTPå¼‚å¸¸
        raise
    except Exception as e:
        # æ•è·å…¶ä»–å¼‚å¸¸å¹¶è®°å½•
        logger.error(f"ä¸‹è½½æ–‡ä»¶æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ä¸‹è½½æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

@app.post("/download-all")
async def download_all():
    """æ‰¹é‡ä¸‹è½½æ‰€æœ‰é‡å‘½åæ–‡ä»¶ï¼ˆZIPæ ¼å¼ï¼‰- å…¼å®¹æ—§æ¥å£ï¼Œé‡å®šå‘åˆ°æ–°çš„ZIPåˆ›å»ºæ–¹å¼"""
    try:
        # ç›´æ¥è°ƒç”¨æ–°çš„ZIPåˆ›å»ºæ¥å£
        return await create_zip_all()
    except Exception as e:
        logger.error(f"ZIPä¸‹è½½å¤±è´¥ï¼Œå›é€€åˆ°ç›´æ¥å“åº”: {e}")
        # å¦‚æœæ–°æ–¹å¼å¤±è´¥ï¼Œå›é€€åˆ°åŸæ¥çš„ç›´æ¥å“åº”æ–¹å¼
        downloads_dir = Path("downloads")
        if not downloads_dir.exists() or not list(downloads_dir.glob("*.pdf")):
            raise HTTPException(status_code=404, detail="æ²¡æœ‰å¯ä¸‹è½½çš„æ–‡ä»¶")

        pdf_files = list(downloads_dir.glob("*.pdf"))
        
        from datetime import datetime
        zip_filename = f"renamed_pdfs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
        zip_path = f"temp_{zip_filename}"

        try:
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED, compresslevel=6) as zipf:
                for pdf_file in pdf_files:
                    zipf.write(pdf_file, pdf_file.name)

            zip_size = os.path.getsize(zip_path)

            def cleanup_zip():
                try:
                    os.remove(zip_path)
                except:
                    pass

            headers = {
                "Content-Disposition": f'attachment; filename="{zip_filename}"',
                "Content-Type": "application/zip",
                "Content-Length": str(zip_size),
                "X-Content-Type-Options": "nosniff",
                "Cache-Control": "no-cache, no-store, must-revalidate",
                "Pragma": "no-cache",
                "Expires": "0",
                "Accept-Ranges": "bytes"
            }

            return FileResponse(
                zip_path,
                filename=zip_filename,
                media_type="application/zip",
                headers=headers,
                background=BackgroundTask(cleanup_zip)
            )

        except Exception as e2:
            if os.path.exists(zip_path):
                os.remove(zip_path)
            raise HTTPException(status_code=500, detail=f"åˆ›å»ºä¸‹è½½æ–‡ä»¶å¤±è´¥: {str(e2)}")

@app.post("/download-selected")
async def download_selected(filenames: List[str]):
    """é€‰æ‹©æ€§ä¸‹è½½æŒ‡å®šæ–‡ä»¶ï¼ˆZIPæ ¼å¼ï¼‰- å…¼å®¹æ—§æ¥å£ï¼Œé‡å®šå‘åˆ°æ–°çš„ZIPåˆ›å»ºæ–¹å¼"""
    try:
        # ç›´æ¥è°ƒç”¨æ–°çš„ZIPåˆ›å»ºæ¥å£
        return await create_zip_selected(filenames)
    except Exception as e:
        logger.error(f"é€‰æ‹©æ€§ZIPä¸‹è½½å¤±è´¥ï¼Œå›é€€åˆ°ç›´æ¥å“åº”: {e}")
        # å›é€€åˆ°åŸæ¥çš„æ–¹å¼ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
        if not filenames:
            raise HTTPException(status_code=400, detail="æ²¡æœ‰é€‰æ‹©æ–‡ä»¶")

        downloads_dir = Path("downloads")
        existing_files = []
        for filename in filenames:
            file_path = downloads_dir / filename
            if file_path.exists() and file_path.suffix.lower() == '.pdf':
                existing_files.append(file_path)

        if not existing_files:
            raise HTTPException(status_code=404, detail="é€‰æ‹©çš„æ–‡ä»¶éƒ½ä¸å­˜åœ¨")

        from datetime import datetime
        zip_filename = f"selected_pdfs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
        zip_path = f"temp_{zip_filename}"

        try:
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED, compresslevel=6) as zipf:
                for pdf_file in existing_files:
                    zipf.write(pdf_file, pdf_file.name)

            zip_size = os.path.getsize(zip_path)

            def cleanup_zip():
                try:
                    os.remove(zip_path)
                except:
                    pass

            headers = {
                "Content-Disposition": f'attachment; filename="{zip_filename}"',
                "Content-Type": "application/zip",
                "Content-Length": str(zip_size),
                "X-Content-Type-Options": "nosniff",
                "Cache-Control": "no-cache, no-store, must-revalidate",
                "Pragma": "no-cache",
                "Expires": "0",
                "Accept-Ranges": "bytes"
            }

            return FileResponse(
                zip_path,
                filename=zip_filename,
                media_type="application/zip",
                headers=headers,
                background=BackgroundTask(cleanup_zip)
            )

        except Exception as e2:
            if os.path.exists(zip_path):
                os.remove(zip_path)
            raise HTTPException(status_code=500, detail=f"åˆ›å»ºä¸‹è½½æ–‡ä»¶å¤±è´¥: {str(e2)}")

@app.post("/download-direct-all")
async def download_direct_all():
    """æ‰¹é‡ç›´æ¥ä¸‹è½½æ‰€æœ‰PDFæ–‡ä»¶ï¼ˆä¸æ‰“åŒ…ï¼‰- è¿”å›æ–‡ä»¶åˆ—è¡¨ä¾›å‰ç«¯é€ä¸ªä¸‹è½½"""
    downloads_dir = Path("downloads")

    if not downloads_dir.exists() or not list(downloads_dir.glob("*.pdf")):
        raise HTTPException(status_code=404, detail="æ²¡æœ‰å¯ä¸‹è½½çš„æ–‡ä»¶")

    pdf_files = list(downloads_dir.glob("*.pdf"))
    
    # è¿”å›æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
    file_list = []
    total_size = 0
    
    for pdf_file in pdf_files:
        file_size = pdf_file.stat().st_size
        total_size += file_size
        file_list.append({
            "filename": pdf_file.name,
            "size": file_size,
            "size_mb": round(file_size / 1024 / 1024, 2),
            "download_url": f"/download/{pdf_file.name}"
        })
    
    logger.info(f"å‡†å¤‡æ‰¹é‡ç›´æ¥ä¸‹è½½: {len(pdf_files)} ä¸ªæ–‡ä»¶, æ€»å¤§å°: {round(total_size / 1024 / 1024, 2)}MB")
    
    return JSONResponse({
        "success": True,
        "files": file_list,
        "total_files": len(pdf_files),
        "total_size": total_size,
        "total_size_mb": round(total_size / 1024 / 1024, 2)
    })

@app.post("/download-direct-selected")
async def download_direct_selected(filenames: List[str]):
    """æ‰¹é‡ç›´æ¥ä¸‹è½½é€‰å®šçš„PDFæ–‡ä»¶ï¼ˆä¸æ‰“åŒ…ï¼‰- è¿”å›æ–‡ä»¶åˆ—è¡¨ä¾›å‰ç«¯é€ä¸ªä¸‹è½½"""
    if not filenames:
        raise HTTPException(status_code=400, detail="æ²¡æœ‰é€‰æ‹©æ–‡ä»¶")

    downloads_dir = Path("downloads")

    # éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    existing_files = []
    for filename in filenames:
        file_path = downloads_dir / filename
        if file_path.exists() and file_path.suffix.lower() == '.pdf':
            existing_files.append(file_path)

    if not existing_files:
        raise HTTPException(status_code=404, detail="é€‰æ‹©çš„æ–‡ä»¶éƒ½ä¸å­˜åœ¨")

    # è¿”å›æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
    file_list = []
    total_size = 0
    
    for pdf_file in existing_files:
        file_size = pdf_file.stat().st_size
        total_size += file_size
        file_list.append({
            "filename": pdf_file.name,
            "size": file_size,
            "size_mb": round(file_size / 1024 / 1024, 2),
            "download_url": f"/download/{pdf_file.name}"
        })
    
    logger.info(f"å‡†å¤‡æ‰¹é‡ç›´æ¥ä¸‹è½½é€‰å®šæ–‡ä»¶: {len(existing_files)} ä¸ªæ–‡ä»¶, æ€»å¤§å°: {round(total_size / 1024 / 1024, 2)}MB")
    
    return JSONResponse({
        "success": True,
        "files": file_list,
        "total_files": len(existing_files),
        "total_size": total_size,
        "total_size_mb": round(total_size / 1024 / 1024, 2)
    })

@app.get("/debug-downloads")
async def debug_downloads():
    """è°ƒè¯•ä¸‹è½½ç›®å½• - è¯¦ç»†ä¿¡æ¯"""
    downloads_dir = Path("downloads")

    try:
        debug_info = {
            "downloads_dir": str(downloads_dir.absolute()),
            "dir_exists": downloads_dir.exists(),
            "is_directory": downloads_dir.is_dir() if downloads_dir.exists() else False,
            "permissions": oct(downloads_dir.stat().st_mode)[-3:] if downloads_dir.exists() else "N/A",
            "all_files": [],
            "pdf_files": [],
            "file_sizes": {},
            "total_files": 0,
            "total_size": 0
        }

        if downloads_dir.exists():
            # è·å–æ‰€æœ‰æ–‡ä»¶
            all_files = list(downloads_dir.iterdir())
            debug_info["all_files"] = [f.name for f in all_files if f.is_file()]

            # è·å–PDFæ–‡ä»¶
            pdf_files = list(downloads_dir.glob("*.pdf"))
            debug_info["pdf_files"] = [f.name for f in pdf_files]

            # æ–‡ä»¶å¤§å°ä¿¡æ¯
            for file in pdf_files:
                size = file.stat().st_size
                debug_info["file_sizes"][file.name] = {
                    "size_bytes": size,
                    "size_mb": round(size / 1024 / 1024, 2),
                    "modified": file.stat().st_mtime
                }
                debug_info["total_size"] += size

            debug_info["total_files"] = len(pdf_files)

        logger.info(f"è°ƒè¯•ä¸‹è½½ç›®å½•: {debug_info}")
        return JSONResponse(debug_info)

    except Exception as e:
        logger.error(f"è°ƒè¯•ä¸‹è½½ç›®å½•å¤±è´¥: {e}")
        return JSONResponse({
            "error": str(e),
            "downloads_dir": str(downloads_dir),
            "dir_exists": False
        })

@app.post("/auto-fix")
async def auto_fix_uploads():
    """è‡ªåŠ¨ä¿®å¤ï¼šå°†uploadsä¸­é—ç•™çš„å¤„ç†å¥½çš„PDFæ–‡ä»¶ç§»åŠ¨åˆ°downloadsç›®å½•ï¼Œå¹¶ä»å¤‡ä»½ç›®å½•æ¢å¤ç¼ºå¤±æ–‡ä»¶"""
    uploads_dir = Path("uploads")
    downloads_dir = Path("downloads")
    backup_dir = Path("backup")

    # ç¡®ä¿downloadsç›®å½•å­˜åœ¨
    downloads_dir.mkdir(exist_ok=True)

    fixed_count = 0
    backup_files_added = 0
    results = []

    # ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥uploadsä¸­çš„æ–‡ä»¶
    if uploads_dir.exists():
        pdf_files = list(uploads_dir.glob("*.pdf"))

        for file in pdf_files:
            try:
                # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦æ˜¯å¤„ç†åçš„æ ¼å¼ï¼ˆåŒ…å«-çš„é”€è´§å•å·ï¼‰
                if "-" in file.stem and (file.stem.count("-") == 1):
                    # è¿™æ˜¯ä¸€ä¸ªå¤„ç†åçš„æ–‡ä»¶ï¼Œåº”è¯¥ç§»åŠ¨åˆ°downloads
                    destination = downloads_dir / file.name

                    # å¤„ç†é‡åæƒ…å†µ
                    counter = 1
                    original_stem = file.stem
                    original_suffix = file.suffix
                    while destination.exists():
                        new_name = f"{original_stem}_{counter}{original_suffix}"
                        destination = downloads_dir / new_name
                        counter += 1

                    # ç§»åŠ¨æ–‡ä»¶
                    import shutil
                    shutil.copy2(str(file), str(destination))

                    fixed_count += 1
                    results.append({
                        "original": file.name,
                        "moved_to": destination.name,
                        "success": True
                    })
                    logger.info(f"è‡ªåŠ¨ä¿®å¤: {file.name} -> {destination.name}")

                else:
                    # ä¿ç•™æœªå¤„ç†çš„æ–‡ä»¶
                    results.append({
                        "original": file.name,
                        "moved_to": None,
                        "success": False,
                        "reason": "éå¤„ç†åæ–‡ä»¶ï¼Œä¿ç•™åœ¨uploads"
                    })

            except Exception as e:
                results.append({
                    "original": file.name,
                    "moved_to": None,
                    "success": False,
                    "reason": f"ç§»åŠ¨å¤±è´¥: {str(e)}"
                })
                logger.error(f"è‡ªåŠ¨ä¿®å¤å¤±è´¥ {file.name}: {e}")

    # ç¬¬äºŒæ­¥ï¼šä»å¤‡ä»½ç›®å½•å¤åˆ¶æ–‡ä»¶åˆ°downloads
    backup_date_dirs = []
    if backup_dir.exists():
        backup_date_dirs = [d for d in backup_dir.iterdir() if d.is_dir()]
        # æŒ‰æ—¥æœŸå€’åºæ’åˆ—ï¼Œç¡®ä¿ä½¿ç”¨æœ€æ–°çš„å¤‡ä»½
        backup_date_dirs.sort(reverse=True)

    for date_dir in backup_date_dirs:
        logger.info(f"æ£€æŸ¥å¤‡ä»½ç›®å½•: {date_dir}")
        backup_files = list(date_dir.glob("*.pdf"))

        for backup_file in backup_files:
            try:
                # å°è¯•ä»åŸå§‹æ–‡ä»¶æ¢å¤å¤„ç†åæ–‡ä»¶
                if "-" in backup_file.stem and backup_file.stem.count("-") == 1:
                    # è¿™æ˜¯ä¸€ä¸ªå¤„ç†åçš„æ–‡ä»¶åæ ¼å¼ (å¦‚1403-20250110...)
                    destination = downloads_dir / backup_file.name

                    if not destination.exists():
                        import shutil
                        shutil.copy2(str(backup_file), str(destination))
                        backup_files_added += 1
                        results.append({
                            "original": backup_file.name,
                            "moved_to": backup_file.name,
                            "success": True,
                            "source": "backup-direct-processed"
                        })
                        logger.info(f"ä»å¤‡ä»½æ¢å¤å¤„ç†åæ–‡ä»¶: {backup_file.name}")
            except Exception as e:
                logger.error(f"ä»å¤‡ä»½æ¢å¤å¤±è´¥ {backup_file.name}: {e}")

    # ç¬¬ä¸‰æ­¥ï¼šå¦‚æœè¿˜æ˜¯æ²¡æœ‰æ–‡ä»¶ï¼Œä»å¤‡ä»½å¤åˆ¶é‡å‘½åæ ¼å¼çš„PDFæ–‡ä»¶ï¼ˆæ’é™¤åŸå§‹æ–‡ä»¶åæ ¼å¼ï¼‰
    if backup_files_added == 0 and fixed_count == 0 and backup_date_dirs:
        latest_backup = backup_date_dirs[0]
        logger.info(f"å°è¯•ä»æœ€æ–°å¤‡ä»½ç›®å½•å¤åˆ¶é‡å‘½åæ–‡ä»¶: {latest_backup}")

        import re
        # åªå¤åˆ¶é‡å‘½åæ ¼å¼çš„æ–‡ä»¶ï¼ˆåŒ…å«-æˆ–_çš„é”€è´§å•å·æ ¼å¼ï¼‰
        renamed_pattern = re.compile(r'^[0-9]{4}[-_][0-9]{8,}.*\.pdf$')
        original_pattern = re.compile(r'^\d{17}_\d{4}.*\.pdf$')  # æ’é™¤åŸå§‹æ–‡ä»¶åæ ¼å¼

        for backup_file in latest_backup.glob("*.pdf"):
            try:
                # æ£€æŸ¥æ˜¯å¦æ˜¯é‡å‘½åæ ¼å¼çš„æ–‡ä»¶ï¼Œæ’é™¤åŸå§‹æ–‡ä»¶åæ ¼å¼
                if (renamed_pattern.match(backup_file.name) and
                    not original_pattern.match(backup_file.name)):

                    destination = downloads_dir / backup_file.name
                    if not destination.exists():
                        import shutil
                        shutil.copy2(str(backup_file), str(destination))
                        backup_files_added += 1
                        results.append({
                            "original": backup_file.name,
                            "moved_to": backup_file.name,
                            "success": True,
                            "source": "backup-renamed-only"
                        })
                        logger.info(f"ä»å¤‡ä»½å¤åˆ¶é‡å‘½åæ–‡ä»¶: {backup_file.name}")
                    else:
                        logger.info(f"è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶: {backup_file.name}")
                else:
                    logger.debug(f"è·³è¿‡åŸå§‹æ–‡ä»¶åæ ¼å¼: {backup_file.name}")
            except Exception as e:
                logger.error(f"ä»å¤‡ä»½å¤åˆ¶å¤±è´¥ {backup_file.name}: {e}")

    total_fixed = fixed_count + backup_files_added

    # è‡ªåŠ¨ä¿®å¤å®Œæˆåæ¸…ç†ä»»ä½•æ®‹ç•™çš„åŸå§‹æ–‡ä»¶åæ ¼å¼æ–‡ä»¶ï¼ˆå¤‡ä»½æ¢å¤å¯èƒ½äº§ç”Ÿï¼‰
    auto_cleaned_count = clean_original_filename_files()
    if auto_cleaned_count > 0:
        logger.info(f"ğŸ§¹ è‡ªåŠ¨ä¿®å¤åæ¸…ç†äº† {auto_cleaned_count} ä¸ªåŸå§‹æ–‡ä»¶åæ ¼å¼æ–‡ä»¶")

    message = f"âœ… è‡ªåŠ¨ä¿®å¤å®Œæˆï¼Œå…±å¤„ç† {total_fixed} ä¸ªæ–‡ä»¶"
    if fixed_count > 0:
        message += f"ï¼ˆä»uploadsä¿®å¤: {fixed_count}ä¸ªï¼‰"
    if backup_files_added > 0:
        message += f"ï¼ˆä»å¤‡ä»½æ¢å¤: {backup_files_added}ä¸ªï¼‰"
    if auto_cleaned_count > 0:
        message += f"ï¼ˆæ¸…ç†åŸå§‹æ–‡ä»¶: {auto_cleaned_count}ä¸ªï¼‰"

    logger.info(message)

    return JSONResponse({
        "message": message,
        "fixed_count": fixed_count,
        "backup_files_added": backup_files_added,
        "cleaned_count": auto_cleaned_count,
        "total_fixed": total_fixed,
        "details": results
    })

@app.post("/clear")
async def clear_downloads():
    """æ¸…ç†ä¸‹è½½æ–‡ä»¶"""
    global filename_mapping

    # ä½¿ç”¨ç»Ÿä¸€çš„æ¸…ç†å‡½æ•°
    cleared_count = clean_all_downloads()

    # æ¸…ç©ºæ–‡ä»¶åæ˜ å°„å…³ç³»
    filename_mapping.clear()

    logger.info(f"æ‰‹åŠ¨æ¸…ç†äº† {cleared_count} ä¸ªä¸‹è½½æ–‡ä»¶")
    return JSONResponse({"message": f"âœ… å·²æ¸…ç† {cleared_count} ä¸ªä¸‹è½½æ–‡ä»¶"})

@app.post("/clear-debug")
async def clear_debug():
    """ä¸“é—¨æ¸…ç†è°ƒè¯•æ–‡ä»¶"""
    debug_cleared_count = clean_debug_files()

    message = f"âœ… å·²æ¸…ç† {debug_cleared_count} ä¸ªè°ƒè¯•æ–‡ä»¶" if debug_cleared_count > 0 else "âœ… æ²¡æœ‰è°ƒè¯•æ–‡ä»¶éœ€è¦æ¸…ç†"
    logger.info(f"æ‰‹åŠ¨æ¸…ç†äº† {debug_cleared_count} ä¸ªè°ƒè¯•æ–‡ä»¶")
    return JSONResponse({"message": message, "cleared_count": debug_cleared_count})

@app.post("/selective-backup")
async def selective_backup(files: List[UploadFile] = File(...)):
    """é€‰æ‹©æ€§å¤‡ä»½åŠŸèƒ½"""
    if not files:
        raise HTTPException(status_code=400, detail="æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶")

    results = []
    backup_count = 0

    for file in files:
        if not file.filename or not file.filename.lower().endswith('.pdf'):
            results.append({
                "filename": file.filename or "unknown",
                "success": False,
                "message": "âŒ ä¸æ˜¯PDFæ–‡ä»¶"
            })
            continue

        try:
            # å¤„ç†æ–‡ä»¶åï¼Œå»é™¤è·¯å¾„ä¿¡æ¯
            clean_filename = os.path.basename(file.filename) if file.filename else "unknown.pdf"

            # ä¸´æ—¶ä¿å­˜æ–‡ä»¶
            temp_path = f"temp_{clean_filename}"
            with open(temp_path, "wb") as buffer:
                content = await file.read()
                if not content:
                    raise ValueError("æ–‡ä»¶å†…å®¹ä¸ºç©º")
                buffer.write(content)

            # åˆ›å»ºå¤‡ä»½ï¼Œä½¿ç”¨åŸå§‹æ–‡ä»¶åä¿æŒæ–‡ä»¶å¤¹ç»“æ„ä¿¡æ¯
            backup_filename = file.filename if file.filename else clean_filename
            backup_path = create_backup(temp_path, backup_filename)

            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            os.remove(temp_path)

            results.append({
                "filename": file.filename,
                "success": True,
                "message": "âœ… å¤‡ä»½æˆåŠŸ",
                "backup_path": backup_path
            })
            backup_count += 1

        except Exception as e:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            clean_filename = os.path.basename(file.filename) if file.filename else "unknown.pdf"
            temp_path = f"temp_{clean_filename}"
            if os.path.exists(temp_path):
                try:
                    os.remove(temp_path)
                except:
                    pass

            results.append({
                "filename": file.filename or "unknown",
                "success": False,
                "message": f"âŒ å¤‡ä»½å¤±è´¥: {str(e)}"
            })

    logger.info(f"é€‰æ‹©æ€§å¤‡ä»½å®Œæˆï¼ŒæˆåŠŸå¤‡ä»½ {backup_count} ä¸ªæ–‡ä»¶")

    return JSONResponse({
        "results": results,
        "backup_count": backup_count,
        "total_count": len(files),
        "message": f"âœ… é€‰æ‹©æ€§å¤‡ä»½å®Œæˆï¼ŒæˆåŠŸå¤‡ä»½ {backup_count}/{len(files)} ä¸ªæ–‡ä»¶"
    })

@app.post("/clear-backup")
async def clear_backup(date: Optional[str] = None):
    """æ¸…ç†å¤‡ä»½æ–‡ä»¶"""
    try:
        backup_root = Path("backup")
        cleared_count = 0

        if not backup_root.exists():
            return JSONResponse({"message": "âš ï¸ å¤‡ä»½ç›®å½•ä¸å­˜åœ¨"})

        if date:
            # æ¸…ç†æŒ‡å®šæ—¥æœŸçš„å¤‡ä»½
            date_folder = backup_root / date
            if date_folder.exists():
                for file in date_folder.glob("*.pdf"):
                    try:
                        os.remove(file)
                        cleared_count += 1
                    except Exception as e:
                        logger.warning(f"æ¸…ç†å¤‡ä»½æ–‡ä»¶å¤±è´¥ {file}: {e}")

                # å¦‚æœæ–‡ä»¶å¤¹ä¸ºç©ºï¼Œåˆ é™¤æ–‡ä»¶å¤¹
                try:
                    if not any(date_folder.iterdir()):
                        date_folder.rmdir()
                except:
                    pass

                logger.info(f"æ¸…ç†äº† {date} çš„ {cleared_count} ä¸ªå¤‡ä»½æ–‡ä»¶")
                return JSONResponse({"message": f"âœ… å·²æ¸…ç† {date} çš„ {cleared_count} ä¸ªå¤‡ä»½æ–‡ä»¶"})
            else:
                return JSONResponse({"message": f"âš ï¸ æ—¥æœŸ {date} çš„å¤‡ä»½ä¸å­˜åœ¨"})
        else:
            # æ¸…ç†æ‰€æœ‰å¤‡ä»½
            for date_folder in backup_root.iterdir():
                if date_folder.is_dir():
                    for file in date_folder.glob("*.pdf"):
                        try:
                            os.remove(file)
                            cleared_count += 1
                        except Exception as e:
                            logger.warning(f"æ¸…ç†å¤‡ä»½æ–‡ä»¶å¤±è´¥ {file}: {e}")

                    # åˆ é™¤ç©ºæ–‡ä»¶å¤¹
                    try:
                        if not any(date_folder.iterdir()):
                            date_folder.rmdir()
                    except:
                        pass

            logger.info(f"æ¸…ç†äº†æ‰€æœ‰ {cleared_count} ä¸ªå¤‡ä»½æ–‡ä»¶")
            return JSONResponse({"message": f"âœ… å·²æ¸…ç†æ‰€æœ‰ {cleared_count} ä¸ªå¤‡ä»½æ–‡ä»¶"})

    except Exception as e:
        logger.error(f"æ¸…ç†å¤‡ä»½å¤±è´¥: {e}")
        return JSONResponse({"message": f"âŒ æ¸…ç†å¤‡ä»½å¤±è´¥: {str(e)}"})

def find_available_port(start_port: int = 8000, max_attempts: int = 10) -> int:
    """æŸ¥æ‰¾å¯ç”¨ç«¯å£"""
    import socket

    for port in range(start_port, start_port + max_attempts):
        try:
            # å°è¯•ç»‘å®šç«¯å£
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                sock.bind(('localhost', port))
                logger.info(f"âœ… æ‰¾åˆ°å¯ç”¨ç«¯å£: {port}")
                return port
        except OSError:
            logger.debug(f"ç«¯å£ {port} å·²è¢«å ç”¨")
            continue

    # å¦‚æœæ‰€æœ‰ç«¯å£éƒ½è¢«å ç”¨ï¼Œè¿”å›ä¸€ä¸ªéšæœºç«¯å£
    import random
    random_port = random.randint(8010, 8999)
    logger.warning(f"âš ï¸ å‰{max_attempts}ä¸ªç«¯å£éƒ½è¢«å ç”¨ï¼Œå°è¯•éšæœºç«¯å£: {random_port}")
    return random_port

def start_server():
    """å¯åŠ¨æœåŠ¡å™¨"""
    logger.info("ğŸš€ å¯åŠ¨PDFæ‰¹é‡é‡å‘½åå·¥å…·...")

    # å¯åŠ¨æ—¶æ¸…ç†è°ƒè¯•æ–‡ä»¶
    debug_cleaned = clean_debug_files()
    if debug_cleaned > 0:
        logger.info(f"ğŸ—‘ï¸ å¯åŠ¨æ—¶æ¸…ç†äº† {debug_cleaned} ä¸ªè°ƒè¯•æ–‡ä»¶")

    # æŸ¥æ‰¾å¯ç”¨ç«¯å£
    available_port = find_available_port()

    try:
        logger.info(f"ğŸŒ æœåŠ¡å°†åœ¨ä»¥ä¸‹åœ°å€å¯åŠ¨:")
        logger.info(f"   - æœ¬åœ°è®¿é—®: http://localhost:{available_port}")
        logger.info(f"   - ç½‘ç»œè®¿é—®: http://0.0.0.0:{available_port}")
        logger.info("=" * 50)

        uvicorn.run(app, host="0.0.0.0", port=available_port)

    except Exception as e:
        logger.error(f"âŒ æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")

        # å°è¯•å¤‡ç”¨ç«¯å£
        logger.info("ğŸ”„ å°è¯•å¤‡ç”¨ç«¯å£...")
        backup_port = find_available_port(9000, 5)

        try:
            logger.info(f"ğŸŒ ä½¿ç”¨å¤‡ç”¨ç«¯å£: {backup_port}")
            uvicorn.run(app, host="0.0.0.0", port=backup_port)
        except Exception as backup_e:
            logger.error(f"âŒ å¤‡ç”¨ç«¯å£å¯åŠ¨ä¹Ÿå¤±è´¥: {backup_e}")
            logger.error("ğŸ’¡ è¯·æ‰‹åŠ¨æŒ‡å®šç«¯å£å·æˆ–æ£€æŸ¥ç½‘ç»œé…ç½®")

@app.post("/create-zip-all")
async def create_zip_all():
    """åˆ›å»ºæ‰€æœ‰æ–‡ä»¶çš„ZIPåŒ…å¹¶è¿”å›ä¸‹è½½é“¾æ¥"""
    downloads_dir = Path("downloads")

    if not downloads_dir.exists() or not list(downloads_dir.glob("*.pdf")):
        raise HTTPException(status_code=404, detail="æ²¡æœ‰å¯ä¸‹è½½çš„æ–‡ä»¶")

    pdf_files = list(downloads_dir.glob("*.pdf"))
    
    # åˆ›å»ºZIPæ–‡ä»¶
    from datetime import datetime
    import uuid
    
    zip_id = str(uuid.uuid4())
    zip_filename = f"renamed_pdfs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
    zip_path = Path("temp") / f"zip_{zip_id}.zip"
    
    # ç¡®ä¿tempç›®å½•å­˜åœ¨
    zip_path.parent.mkdir(exist_ok=True)

    try:
        # åˆ›å»ºZIPæ–‡ä»¶
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED, compresslevel=6) as zipf:
            for pdf_file in pdf_files:
                zipf.write(pdf_file, pdf_file.name)

        # è·å–ZIPæ–‡ä»¶ä¿¡æ¯
        zip_size = zip_path.stat().st_size
        zip_size_mb = round(zip_size / 1024 / 1024, 2)

        # å­˜å‚¨ZIPæ–‡ä»¶ä¿¡æ¯ï¼ˆ30åˆ†é’Ÿåè‡ªåŠ¨æ¸…ç†ï¼‰
        import time
        temp_zip_files[zip_id] = {
            "filename": zip_filename,
            "path": str(zip_path),
            "size": zip_size,
            "size_mb": zip_size_mb,
            "created_at": time.time(),
            "file_count": len(pdf_files)
        }

        logger.info(f"åˆ›å»ºæ‰¹é‡ä¸‹è½½ZIP: {zip_filename}, åŒ…å« {len(pdf_files)} ä¸ªæ–‡ä»¶, å¤§å°: {zip_size_mb}MB")

        return JSONResponse({
            "success": True,
            "zip_id": zip_id,
            "filename": zip_filename,
            "download_url": f"/download-zip/{zip_id}",
            "size": zip_size,
            "size_mb": zip_size_mb,
            "file_count": len(pdf_files)
        })

    except Exception as e:
        logger.error(f"åˆ›å»ºZIPæ–‡ä»¶å¤±è´¥: {e}")
        if zip_path.exists():
            zip_path.unlink()
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºä¸‹è½½æ–‡ä»¶å¤±è´¥: {str(e)}")

@app.post("/create-zip-selected")
async def create_zip_selected(filenames: List[str]):
    """åˆ›å»ºé€‰å®šæ–‡ä»¶çš„ZIPåŒ…å¹¶è¿”å›ä¸‹è½½é“¾æ¥"""
    if not filenames:
        raise HTTPException(status_code=400, detail="æ²¡æœ‰é€‰æ‹©æ–‡ä»¶")

    downloads_dir = Path("downloads")

    # éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    existing_files = []
    for filename in filenames:
        file_path = downloads_dir / filename
        if file_path.exists() and file_path.suffix.lower() == '.pdf':
            existing_files.append(file_path)

    if not existing_files:
        raise HTTPException(status_code=404, detail="é€‰æ‹©çš„æ–‡ä»¶éƒ½ä¸å­˜åœ¨")

    # åˆ›å»ºZIPæ–‡ä»¶
    from datetime import datetime
    import uuid
    
    zip_id = str(uuid.uuid4())
    zip_filename = f"selected_pdfs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
    zip_path = Path("temp") / f"zip_{zip_id}.zip"
    
    # ç¡®ä¿tempç›®å½•å­˜åœ¨
    zip_path.parent.mkdir(exist_ok=True)

    try:
        # åˆ›å»ºZIPæ–‡ä»¶
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED, compresslevel=6) as zipf:
            for pdf_file in existing_files:
                zipf.write(pdf_file, pdf_file.name)

        # è·å–ZIPæ–‡ä»¶ä¿¡æ¯
        zip_size = zip_path.stat().st_size
        zip_size_mb = round(zip_size / 1024 / 1024, 2)

        # å­˜å‚¨ZIPæ–‡ä»¶ä¿¡æ¯
        import time
        temp_zip_files[zip_id] = {
            "filename": zip_filename,
            "path": str(zip_path),
            "size": zip_size,
            "size_mb": zip_size_mb,
            "created_at": time.time(),
            "file_count": len(existing_files)
        }

        logger.info(f"åˆ›å»ºé€‰æ‹©æ€§ä¸‹è½½ZIP: {zip_filename}, åŒ…å« {len(existing_files)} ä¸ªæ–‡ä»¶, å¤§å°: {zip_size_mb}MB")

        return JSONResponse({
            "success": True,
            "zip_id": zip_id,
            "filename": zip_filename,
            "download_url": f"/download-zip/{zip_id}",
            "size": zip_size,
            "size_mb": zip_size_mb,
            "file_count": len(existing_files)
        })

    except Exception as e:
        logger.error(f"åˆ›å»ºZIPæ–‡ä»¶å¤±è´¥: {e}")
        if zip_path.exists():
            zip_path.unlink()
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºä¸‹è½½æ–‡ä»¶å¤±è´¥: {str(e)}")

@app.get("/download-zip/{zip_id}")
async def download_zip_file(zip_id: str):
    """ä¸‹è½½æŒ‡å®šçš„ZIPæ–‡ä»¶"""
    if zip_id not in temp_zip_files:
        raise HTTPException(status_code=404, detail="ZIPæ–‡ä»¶ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ")
    
    zip_info = temp_zip_files[zip_id]
    zip_path = Path(zip_info["path"])
    
    if not zip_path.exists():
        # æ¸…ç†è¿‡æœŸè®°å½•
        del temp_zip_files[zip_id]
        raise HTTPException(status_code=404, detail="ZIPæ–‡ä»¶å·²è¢«æ¸…ç†")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦è¿‡æœŸï¼ˆ30åˆ†é’Ÿï¼‰
    import time
    if time.time() - zip_info["created_at"] > 1800:  # 30åˆ†é’Ÿ
        # æ¸…ç†è¿‡æœŸæ–‡ä»¶
        try:
            zip_path.unlink()
            del temp_zip_files[zip_id]
        except:
            pass
        raise HTTPException(status_code=404, detail="ZIPæ–‡ä»¶å·²è¿‡æœŸ")
    
    def cleanup_zip():
        try:
            # å»¶è¿Ÿæ¸…ç†ï¼Œç»™ä¸‹è½½è¶³å¤Ÿæ—¶é—´
            import threading
            import time
            def delayed_cleanup():
                time.sleep(60)  # 1åˆ†é’Ÿåæ¸…ç†
                try:
                    if zip_path.exists():
                        zip_path.unlink()
                    if zip_id in temp_zip_files:
                        del temp_zip_files[zip_id]
                    logger.debug(f"æ¸…ç†ä¸´æ—¶ZIPæ–‡ä»¶: {zip_path}")
                except:
                    pass
            
            thread = threading.Thread(target=delayed_cleanup)
            thread.daemon = True
            thread.start()
        except:
            pass

    # è®¾ç½®ä¸‹è½½å“åº”å¤´
    headers = {
        "Content-Disposition": f'attachment; filename="{zip_info["filename"]}"',
        "Content-Type": "application/zip",
        "Content-Length": str(zip_info["size"]),
        "X-Content-Type-Options": "nosniff",
        "Cache-Control": "no-cache, no-store, must-revalidate",
        "Pragma": "no-cache",
        "Expires": "0",
        "Accept-Ranges": "bytes"
    }

    return FileResponse(
        str(zip_path),
        filename=zip_info["filename"],
        media_type="application/zip",
        headers=headers,
        background=BackgroundTask(cleanup_zip)
    )

if __name__ == "__main__":
    start_server()
