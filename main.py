#!/usr/bin/env python3
"""
PDFæ‰¹é‡é‡å‘½åå·¥å…· - WebUIç‰ˆæœ¬
æ”¯æŒOCRè¯†åˆ«é”€è´§å‡ºåº“å•å·ï¼ŒåŒ…å«æ—‹è½¬æ£€æµ‹åŠŸèƒ½
"""

import os
import re
import io
import tempfile
import zipfile
from pathlib import Path
from typing import List, Optional, Tuple
import logging
import glob
import time

# è®¾ç½®ç¯å¢ƒå˜é‡è§£å†³NumExprè­¦å‘Š
os.environ['NUMEXPR_MAX_THREADS'] = '8'
os.environ['OMP_NUM_THREADS'] = '8'

from fastapi import FastAPI, File, UploadFile, Request, HTTPException
from fastapi.responses import HTMLResponse, FileResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

def lazy_import_ocr():
    """å»¶è¿Ÿå¯¼å…¥OCRç›¸å…³åº“ä»¥å‡å°‘å¯åŠ¨å†…å­˜"""
    global fitz, Image, pytesseract, cv2, np, ImageEnhance, easyocr
    try:
        import fitz  # PyMuPDF
        from PIL import Image, ImageEnhance
        import pytesseract
        import cv2
        import numpy as np
        import easyocr  # æ–°å¢é«˜ç²¾åº¦OCRå¼•æ“
        return True
    except ImportError as e:
        logger.error(f"OCRåº“å¯¼å…¥å¤±è´¥: {e}")
        logger.info("è¯·å®‰è£… EasyOCR: pip install easyocr")
        return False

fitz = None
Image = None
pytesseract = None
cv2 = None
np = None
ImageEnhance = None
easyocr = None

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="PDFæ‰¹é‡é‡å‘½åå·¥å…·", description="åŸºäºOCRè¯†åˆ«çš„PDFæ‰¹é‡é‡å‘½åå·¥å…·")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ·»åŠ å…¨å±€å˜é‡æ¥å­˜å‚¨æ–‡ä»¶åæ˜ å°„å…³ç³»
filename_mapping = {}  # é‡å‘½ååæ–‡ä»¶å -> åŸå§‹æ–‡ä»¶å

def clean_original_filename_files():
    """æ¸…ç†downloadsç›®å½•ä¸­çš„åŸå§‹æ–‡ä»¶åæ ¼å¼æ–‡ä»¶ï¼ˆæ—¶é—´æˆ³å¼€å¤´çš„æ–‡ä»¶ï¼‰"""
    downloads_dir = Path("downloads")
    if not downloads_dir.exists():
        return 0
    
    cleaned_count = 0
    try:
        # è¯†åˆ«åŸå§‹æ–‡ä»¶åæ ¼å¼çš„æ–‡ä»¶ï¼ˆä»¥æ—¶é—´æˆ³å¼€å¤´ï¼Œæ ¼å¼å¦‚ï¼š20250213165300083_xxxx.pdfï¼‰
        import re
        original_pattern = re.compile(r'^\d{17}_\d{4}.*\.pdf$')  # åŒ¹é… 20250213165300083_0022.pdf æ ¼å¼
        
        for pdf_file in downloads_dir.glob("*.pdf"):
            # æ£€æŸ¥æ˜¯å¦æ˜¯åŸå§‹æ–‡ä»¶åæ ¼å¼
            if original_pattern.match(pdf_file.name):
                try:
                    os.remove(pdf_file)
                    cleaned_count += 1
                    logger.info(f"æ¸…ç†åŸå§‹æ–‡ä»¶åæ ¼å¼æ–‡ä»¶: {pdf_file.name}")
                except Exception as e:
                    logger.warning(f"æ¸…ç†æ–‡ä»¶å¤±è´¥ {pdf_file.name}: {e}")
    
    except Exception as e:
        logger.error(f"æ¸…ç†åŸå§‹æ–‡ä»¶åæ ¼å¼æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    if cleaned_count > 0:
        logger.info(f"âœ… è‡ªåŠ¨æ¸…ç†äº† {cleaned_count} ä¸ªåŸå§‹æ–‡ä»¶åæ ¼å¼æ–‡ä»¶")
    
    return cleaned_count

def clean_all_downloads():
    """æ¸…ç†downloadsç›®å½•ä¸­çš„æ‰€æœ‰PDFæ–‡ä»¶ï¼ˆæ¯æ¬¡æ–°è¯†åˆ«å‰æ¸…ç†ä¸Šæ¬¡ç»“æœï¼‰"""
    downloads_dir = Path("downloads")
    if not downloads_dir.exists():
        return 0
    
    cleaned_count = 0
    try:
        for pdf_file in downloads_dir.glob("*.pdf"):
            try:
                os.remove(pdf_file)
                cleaned_count += 1
                logger.info(f"æ¸…ç†ä¸Šæ¬¡å¤„ç†æ–‡ä»¶: {pdf_file.name}")
            except Exception as e:
                logger.warning(f"æ¸…ç†æ–‡ä»¶å¤±è´¥ {pdf_file.name}: {e}")
    
    except Exception as e:
        logger.error(f"æ¸…ç†downloadsç›®å½•æ—¶å‡ºé”™: {e}")
    
    if cleaned_count > 0:
        logger.info(f"ğŸ§¹ æ¸…ç†äº† {cleaned_count} ä¸ªä¸Šæ¬¡å¤„ç†çš„æ–‡ä»¶")
    
    return cleaned_count

os.makedirs("static", exist_ok=True)
os.makedirs("templates", exist_ok=True)
os.makedirs("uploads", exist_ok=True)
os.makedirs("backup", exist_ok=True)

templates = Jinja2Templates(directory="templates")
app.mount("/static", StaticFiles(directory="static"), name="static")

class PDFProcessor:
    def __init__(self):
        # EasyOCR è¯»å–å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self._easyocr_reader = None
        self._easyocr_initialized = False
        
        # OCRå¸¸è§é”™è¯¯çŸ«æ­£å­—å…¸ï¼ˆæ•°å­—å®¹æ˜“é”™è¯¯è¯†åˆ«ï¼‰
        self.ocr_correction_map = {
            '8': '0',  # 8å®¹æ˜“è¢«è¯¯è¯†åˆ«ä¸º0
            '0': '8',  # 0å®¹æ˜“è¢«è¯¯è¯†åˆ«ä¸º8
            '6': '5',  # 6å®¹æ˜“è¢«è¯¯è¯†åˆ«ä¸º5
            '5': '6',  # 5å®¹æ˜“è¢«è¯¯è¯†åˆ«ä¸º6
            '1': 'I',  # æ•°å­—1å’Œå­—æ¯Içš„æ··æ·†
            'I': '1',  # å­—æ¯Iå’Œæ•°å­—1çš„æ··æ·†
            'O': '0',  # å­—æ¯Oå’Œæ•°å­—0çš„æ··æ·†
            '0': 'O',  # æ•°å­—0å’Œå­—æ¯Oçš„æ··æ·†
            'Z': '2',  # Zå’Œ2çš„æ··æ·†
            '2': 'Z',  # 2å’ŒZçš„æ··æ·†
        }
        
        # å¿«é€’å•å·ç‰¹å¾æ¨¡å¼ï¼ˆéœ€è¦æ’é™¤çš„ï¼‰
        self.express_patterns = [
            r'[A-Z]{2}[0-9]{10,15}',  # å¦‚YT1234567890123ï¼ˆåœ†é€šï¼‰
            r'[0-9]{12,15}',  # çº¯12-15ä½æ•°å­—å¿«é€’å•å·
            r'JD[0-9]{13,18}',  # äº¬ä¸œå¿«é€’
            r'SF[0-9]{12}',  # é¡ºä¸°é€Ÿè¿  
            r'YTO[0-9]{10,13}',  # åœ†é€šé€Ÿé€’
            r'ZTO[0-9]{12}',  # ä¸­é€šå¿«é€’
            r'STO[0-9]{12}',  # ç”³é€šå¿«é€’
            r'YD[0-9]{13}',  # éŸµè¾¾å¿«é€’
            r'HTKY[0-9]{10}',  # ç™¾ä¸–æ±‡é€š
            r'[0-9]{13}',  # 13ä½çº¯æ•°å­—ï¼ˆå¸¸è§å¿«é€’æ ¼å¼ï¼‰
        ]
        
        # é”€è´§å‡ºåº“å•å·çš„ç²¾ç¡®è¯†åˆ«æ¨¡å¼ï¼ˆæŒ‰ç½®ä¿¡åº¦æ’åºï¼‰
        self.patterns = [
            # è¶…é«˜ç½®ä¿¡åº¦ï¼šåŒ…å«æ˜ç¡®æ ‡è¯†çš„å®Œæ•´æ ¼å¼
            r'é”€è´§å‡ºåº“å•å·[ï¼š:\s]*([0-9]{4}[-_][0-9]{12})',  # ä¸¥æ ¼4-12ä½æ ¼å¼
            r'é”€è´§å‡ºåº“å•å·[ï¼š:\s]*([0-9]{4}[-_][0-9]{10,15})',  # ä¸­æ–‡+4ä½æ•°å­—-10åˆ°15ä½æ•°å­—
            r'å‡ºåº“å•å·[ï¼š:\s]*([0-9]{4}[-_][0-9]{12})',  # å‡ºåº“å•å·+ä¸¥æ ¼æ ¼å¼
            
            # é«˜ç½®ä¿¡åº¦ï¼šæ ‡å‡†****-************æ ¼å¼ï¼ˆä½ çš„ä¸»è¦æ ¼å¼ï¼‰
            r'(?<![A-Za-z0-9])([0-9]{4}[-_][0-9]{12})(?![A-Za-z0-9])',  # ä¸¥æ ¼4ä½-12ä½æ•°å­—
            r'(?<![A-Za-z0-9])([0-9]{4}[-_][0-9]{10,15})(?![A-Za-z0-9])',  # 4ä½æ•°å­—-10åˆ°15ä½æ•°å­—
            
            # ä¸­é«˜ç½®ä¿¡åº¦ï¼šåŒ…å«å…³é”®è¯çš„æ ¼å¼
            r'å•å·[ï¼š:\s]*([0-9]{4}[-_][0-9]{12})',  # "å•å·"+ä¸¥æ ¼æ ¼å¼
            r'ç¼–å·[ï¼š:\s]*([0-9]{4}[-_][0-9]{12})',  # "ç¼–å·"+ä¸¥æ ¼æ ¼å¼
            r'å‡ºåº“[ï¼š:\s]*([0-9]{4}[-_][0-9]{12})',  # "å‡ºåº“"+ä¸¥æ ¼æ ¼å¼
            
            # ä¸­ç½®ä¿¡åº¦ï¼šç‰¹å®šå‰ç¼€ï¼ˆæ’é™¤å¿«é€’å…¬å¸å‰ç¼€ï¼‰
            r'(?<![A-Za-z0-9])([13579][0-9]{3}[-_][0-9]{12})(?![A-Za-z0-9])',  # å¥‡æ•°å¼€å¤´4ä½-12ä½
            r'(?<![A-Za-z0-9])([1][4][0-9]{2}[-_][0-9]{12})(?![A-Za-z0-9])',  # 14å¼€å¤´çš„æ ¼å¼
            
            # å®¹é”™æ¨¡å¼ï¼šOCRå¯èƒ½çš„é”™è¯¯è¯†åˆ«ï¼ˆå¢å¼ºç‰ˆï¼‰
            r'é”€[è´§ä¹°è´·][å‡ºäººå±±][åº“å•é‡Œ][å•é‡Œå·][ï¼š:\s]*([0-9]{4}[-_][0-9]{10,15})',  # OCRå®¹é”™
            r'[é”€é”—][è´§è´·ä¹°][å‡ºå±±äºº][åº“é‡Œå•][å•é‡Œå·]å·[ï¼š:\s]*([0-9]{4}[-_][0-9]{10,15})',  # æ›´å¤šOCRå®¹é”™
            
            # ä½ä¼˜å…ˆçº§ï¼šå®½æ¾æ ¼å¼ï¼ˆä¸¥æ ¼è¿‡æ»¤ï¼‰
            r'(?<![A-Za-z0-9])([0-9]{3,5}[-_][0-9]{8,20})(?![A-Za-z0-9])',  # 3-5ä½-8-20ä½æ•°å­—
        ]
        
        # éœ€è¦æ’é™¤çš„å¸¸è§å•è¯å’Œæ— æ•ˆæ¨¡å¼ï¼ˆå¤§å¹…æ‰©å±•ï¼‰
        self.excluded_words = {
            # åŸºæœ¬æ’é™¤è¯
            'document', 'order', 'sales', 'number', 'date', 'customer', 
            'address', 'phone', 'email', 'total', 'amount', 'price',
            'quantity', 'description', 'product', 'service', 'company',
            'invoice', 'receipt', 'payment', 'balance', 'account',
            
            # å¿«é€’å…¬å¸ç›¸å…³
            'express', 'delivery', 'courier', 'shipping', 'tracking',
            'jd', 'sf', 'yto', 'zto', 'sto', 'yd', 'htky',
            
            # æ— æ•ˆæ•°å­—åºåˆ—
            '0000000000000000', '1111111111111111', '2222222222222222',
            '3333333333333333', '4444444444444444', '5555555555555555',
            '6666666666666666', '7777777777777777', '8888888888888888',
            '9999999999999999',
            
            # æ— æ•ˆå­—æ¯åºåˆ—
            'abcdefghijklmnop', 'qrstuvwxyz', 'xxxxxxxxxxxxxxxxx',
        }
        
        # å¢å¼ºéªŒè¯è§„åˆ™
        self.validation_rules = {
            'min_digits': 6,  # è‡³å°‘åŒ…å«6ä¸ªæ•°å­—ï¼ˆæé«˜è¦æ±‚ï¼‰
            'min_length': 8,  # æœ€å°é•¿åº¦ï¼ˆæé«˜è¦æ±‚ï¼‰
            'max_length': 25,  # æœ€å¤§é•¿åº¦
            'required_separator': ['-', '_'],  # å¿…é¡»åŒ…å«çš„åˆ†éš”ç¬¦
            'min_separator_count': 1,  # è‡³å°‘éœ€è¦1ä¸ªåˆ†éš”ç¬¦
            'sales_order_prefixes': ['1403', '1404', '1405'],  # é”€è´§å•å·å¸¸è§å‰ç¼€
            'invalid_patterns': [
                r'^[0]+$',  # å…¨é›¶
                r'^[1]+$',  # å…¨ä¸€
                r'^(.)\1+$',  # é‡å¤å­—ç¬¦
                r'^[A-Z]{2}[0-9]{10,15}$',  # å¿«é€’å•å·æ ¼å¼
                r'^[0-9]{13}$',  # 13ä½çº¯æ•°å­—ï¼ˆå¿«é€’å¸¸ç”¨ï¼‰
                r'^[0-9]{15}$',  # 15ä½çº¯æ•°å­—ï¼ˆå¿«é€’å¸¸ç”¨ï¼‰
                r'^JD[0-9]+$',  # äº¬ä¸œå¿«é€’
                r'^SF[0-9]+$',  # é¡ºä¸°å¿«é€’
                r'^YTO[0-9]+$',  # åœ†é€šå¿«é€’
                r'^ZTO[0-9]+$',  # ä¸­é€šå¿«é€’
                r'^STO[0-9]+$',  # ç”³é€šå¿«é€’
            ]
        }
        
    def extract_order_number(self, pdf_path: str) -> Tuple[Optional[str], str]:
        """ä»PDFä¸­æå–é”€è´§å‡ºåº“å•å·ï¼Œè¿”å›(è®¢å•å·, å¤„ç†æ—¥å¿—)"""
        log_messages = []
        
        if not lazy_import_ocr():
            return None, "âŒ OCRåº“å¯¼å…¥å¤±è´¥ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–"
        
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(pdf_path):
                return None, "âŒ æ–‡ä»¶ä¸å­˜åœ¨"
            
            doc = fitz.open(pdf_path)
            text = ""
            for page in doc:
                page_text = page.get_text()
                text += page_text
            doc.close()
            
            log_messages.append("âœ… PDFæ–‡ä»¶è¯»å–æˆåŠŸ")
            
            # å…ˆå°è¯•ç›´æ¥æ–‡æœ¬æå–
            order_number = self.find_order_number_in_text(text)
            if order_number:
                log_messages.append(f"âœ… ç›´æ¥æ–‡æœ¬æå–æˆåŠŸ: {order_number}")
                return order_number, "\n".join(log_messages)
                
            log_messages.append("âš ï¸ ç›´æ¥æ–‡æœ¬æå–å¤±è´¥ï¼Œå¼€å§‹OCRå¤„ç†...")
            
            # ä½¿ç”¨OCRå¤„ç†
            ocr_result, ocr_logs = self.extract_with_ocr(pdf_path)
            log_messages.extend(ocr_logs)
            
            return ocr_result, "\n".join(log_messages)
            
        except Exception as e:
            error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}"
            log_messages.append(error_msg)
            logger.error(error_msg, exc_info=True)
            return None, "\n".join(log_messages)
    
    def find_order_number_in_text(self, text: str) -> Optional[str]:
        """åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾é”€è´§å‡ºåº“å•å· - é«˜ç²¾åº¦ç‰ˆæœ¬ï¼ˆå¢å¼ºOCRçŸ«æ­£ï¼‰"""
        try:
            candidates = []
            
            # 1. è·å–OCRçŸ«æ­£åçš„å¤šä¸ªæ–‡æœ¬ç‰ˆæœ¬
            text_variants = self._apply_ocr_correction(text)
            logger.info(f"OCRçŸ«æ­£ç”Ÿæˆ{len(text_variants)}ä¸ªæ–‡æœ¬å˜ä½“")
            
            # 2. å¯¹æ¯ä¸ªæ–‡æœ¬å˜ä½“è¿›è¡Œæ¨¡å¼åŒ¹é…
            for variant_index, text_variant in enumerate(text_variants):
                variant_label = "åŸæ–‡" if variant_index == 0 else f"çŸ«æ­£{variant_index}"
                
                # éå†æ‰€æœ‰æ¨¡å¼ï¼ŒæŒ‰ä¼˜å…ˆçº§æ”¶é›†å€™é€‰é¡¹
                for pattern_index, pattern in enumerate(self.patterns):
                    matches = re.finditer(pattern, text_variant, re.IGNORECASE)
                    for match in matches:
                        result = match.group(1).strip()
                        if self._validate_order_number(result):
                            # è®¡ç®—ç»¼åˆç½®ä¿¡åº¦
                            base_confidence = len(self.patterns) - pattern_index
                            # åŸæ–‡åŒ¹é…ç½®ä¿¡åº¦æ›´é«˜
                            variant_bonus = 1.0 if variant_index == 0 else 0.8
                            final_confidence = base_confidence * variant_bonus
                            
                            candidates.append({
                                'number': result,
                                'confidence': final_confidence,
                                'pattern_index': pattern_index,
                                'variant': variant_label,
                                'variant_index': variant_index
                            })
                            
                            logger.info(f"å€™é€‰è®¢å•å·: {result} (æ¨¡å¼{pattern_index}, {variant_label}, ç½®ä¿¡åº¦{final_confidence:.2f})")
            
            if not candidates:
                logger.info("æœªæ‰¾åˆ°æœ‰æ•ˆçš„é”€è´§å‡ºåº“å•å·å€™é€‰")
                return None
            
            # 3. æŒ‰ç½®ä¿¡åº¦æ’åºï¼Œè¿”å›æœ€ä½³å€™é€‰é¡¹
            candidates.sort(key=lambda x: (-x['confidence'], x['pattern_index'], x['variant_index']))
            
            # 4. å»é‡ï¼Œä¼˜å…ˆé€‰æ‹©åŸæ–‡è¯†åˆ«çš„ç»“æœ
            unique_numbers = {}
            for candidate in candidates:
                number = candidate['number']
                if number not in unique_numbers:
                    unique_numbers[number] = candidate
                else:
                    # å¦‚æœæ˜¯åŸæ–‡è¯†åˆ«çš„ï¼Œæ›¿æ¢çŸ«æ­£æ–‡æœ¬çš„ç»“æœ
                    if candidate['variant_index'] == 0 and unique_numbers[number]['variant_index'] > 0:
                        unique_numbers[number] = candidate
            
            final_candidates = list(unique_numbers.values())
            final_candidates.sort(key=lambda x: (-x['confidence'], x['pattern_index'], x['variant_index']))
            
            best_candidate = final_candidates[0]
            
            logger.info(f"æœ€ç»ˆé€‰æ‹©: {best_candidate['number']} (æ¥æº: {best_candidate['variant']}, ç½®ä¿¡åº¦: {best_candidate['confidence']:.2f})")
            logger.info(f"æ€»è®¡æ‰¾åˆ°{len(candidates)}ä¸ªå€™é€‰é¡¹ï¼Œå»é‡å{len(final_candidates)}ä¸ª")
            
            return best_candidate['number']
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬æŸ¥æ‰¾å¤±è´¥: {e}")
            return None
    
    def _apply_ocr_correction(self, text: str) -> str:
        """åº”ç”¨OCRå¸¸è§é”™è¯¯çŸ«æ­£ï¼Œç”Ÿæˆå¤šä¸ªå€™é€‰æ–‡æœ¬"""
        original = text
        corrected_variants = [original]
        
        # å°è¯•å¸¸è§çš„æ•°å­—çŸ«æ­£
        for wrong, correct in self.ocr_correction_map.items():
            if wrong in text:
                variant = text.replace(wrong, correct)
                if variant != original and variant not in corrected_variants:
                    corrected_variants.append(variant)
        
        return corrected_variants
    
    def _is_express_number(self, candidate: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯å¿«é€’å•å·"""
        # æ£€æŸ¥å¿«é€’å•å·ç‰¹å¾æ¨¡å¼
        for pattern in self.express_patterns:
            if re.match(pattern, candidate):
                logger.info(f"è¯†åˆ«ä¸ºå¿«é€’å•å·ï¼ˆæ ¼å¼åŒ¹é…ï¼‰: {candidate}")
                return True
        
        # æ£€æŸ¥å¿«é€’å…¬å¸å‰ç¼€
        express_prefixes = ['JD', 'SF', 'YTO', 'ZTO', 'STO', 'YD', 'HTKY', 'EMS', 'YZPY', 'YUNDA']
        for prefix in express_prefixes:
            if candidate.upper().startswith(prefix):
                logger.info(f"è¯†åˆ«ä¸ºå¿«é€’å•å·ï¼ˆå‰ç¼€åŒ¹é…ï¼‰: {candidate}")
                return True
        
        # ç‰¹æ®Šæ ¼å¼æ£€æŸ¥ï¼šçº¯æ•°å­—ä¸”é•¿åº¦ä¸ºå¿«é€’å¸¸ç”¨é•¿åº¦
        if candidate.isdigit():
            if len(candidate) in [13, 15, 18]:  # å¿«é€’å•å·å¸¸ç”¨é•¿åº¦
                logger.info(f"è¯†åˆ«ä¸ºå¿«é€’å•å·ï¼ˆé•¿åº¦ç‰¹å¾ï¼‰: {candidate}")
                return True
        
        return False
    
    def _validate_order_number(self, candidate: str) -> bool:
        """éªŒè¯å€™é€‰è®¢å•å·æ˜¯å¦æœ‰æ•ˆï¼ˆå¢å¼ºç‰ˆï¼‰"""
        if not candidate:
            return False
        
        # åŸºæœ¬é•¿åº¦æ£€æŸ¥
        rules = self.validation_rules
        if len(candidate) < rules['min_length'] or len(candidate) > rules['max_length']:
            return False
        
        # æ’é™¤å¸¸è§æ— æ•ˆè¯æ±‡
        if candidate.lower() in self.excluded_words:
            return False
        
        # å¿«é€’å•å·æ’é™¤æ£€æŸ¥
        if self._is_express_number(candidate):
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„æ•°å­—
        digit_count = sum(1 for c in candidate if c.isdigit())
        if digit_count < rules['min_digits']:
            return False
        
        # æ£€æŸ¥å¿…éœ€çš„åˆ†éš”ç¬¦
        separator_count = sum(1 for sep in rules['required_separator'] if sep in candidate)
        if separator_count < rules.get('min_separator_count', 0):
            return False
        
        # æ£€æŸ¥æ— æ•ˆæ¨¡å¼
        for invalid_pattern in rules['invalid_patterns']:
            if re.match(invalid_pattern, candidate):
                return False
        
        # é”€è´§å‡ºåº“å•å·ç‰¹å¾æ£€æŸ¥
        # 1. æ£€æŸ¥æ˜¯å¦ç¬¦åˆé”€è´§å•å·æ ¼å¼ (XXXX-XXXXXXXXXXXX)
        if '-' in candidate or '_' in candidate:
            parts = re.split(r'[-_]', candidate)
            if len(parts) >= 2:
                first_part = parts[0]
                second_part = parts[1]
                
                # ç¬¬ä¸€éƒ¨åˆ†åº”è¯¥æ˜¯3-5ä½æ•°å­—ï¼Œç¬¬äºŒéƒ¨åˆ†åº”è¯¥æ˜¯8-15ä½æ•°å­—
                if (first_part.isdigit() and 3 <= len(first_part) <= 5 and
                    second_part.isdigit() and 8 <= len(second_part) <= 15):
                    
                    # æ£€æŸ¥æ˜¯å¦ä»¥é”€è´§å•å·å¸¸è§å‰ç¼€å¼€å¤´
                    sales_prefixes = rules.get('sales_order_prefixes', [])
                    if sales_prefixes and any(first_part.startswith(prefix) for prefix in sales_prefixes):
                        logger.info(f"é”€è´§å•å·å‰ç¼€åŒ¹é…: {candidate}")
                        return True
                    
                    # æˆ–è€…ç¬¦åˆä¸€èˆ¬æ ¼å¼è¦æ±‚
                    if len(first_part) == 4 and len(second_part) >= 10:
                        return True
        
        # é¢å¤–çš„è´¨é‡æ£€æŸ¥
        # 1. é¿å…è¿‡äºç®€å•çš„æ¨¡å¼
        if len(set(candidate)) < 4:  # å­—ç¬¦ç§ç±»å¤ªå°‘ï¼ˆæé«˜è¦æ±‚ï¼‰
            return False
        
        # 2. æ£€æŸ¥æ•°å­—å­—æ¯æ¯”ä¾‹æ˜¯å¦åˆç†
        alpha_count = sum(1 for c in candidate if c.isalpha())
        special_count = sum(1 for c in candidate if not c.isalnum())
        
        # é”€è´§å•å·ä¸»è¦åº”è¯¥æ˜¯æ•°å­—å’Œåˆ†éš”ç¬¦
        if alpha_count > len(candidate) * 0.2:  # å­—æ¯ä¸åº”è¶…è¿‡20%
            return False
        
        # å¦‚æœæ˜¯çº¯æ•°å­—ä½†å¤ªçŸ­ï¼Œæ‹’ç»
        if alpha_count == 0 and special_count == 0 and len(candidate) < 10:
            return False
        
        return True
    
    def find_all_order_candidates(self, text: str) -> list:
        """æ‰¾åˆ°æ–‡æœ¬ä¸­æ‰€æœ‰æ½œåœ¨çš„è®¢å•å·å€™é€‰"""
        candidates = []
        
        # éå†æ‰€æœ‰æ¨¡å¼ï¼ŒæŒ‰ä¼˜å…ˆçº§æ”¶é›†å€™é€‰é¡¹
        for pattern_index, pattern in enumerate(self.patterns):
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                result = match.group(1).strip()
                if self._validate_order_number(result):
                    # è®°å½•å€™é€‰é¡¹åŠå…¶ç½®ä¿¡åº¦ï¼ˆæ¨¡å¼ç´¢å¼•è¶Šå°ç½®ä¿¡åº¦è¶Šé«˜ï¼‰
                    candidates.append({
                        'number': result,
                        'confidence': len(self.patterns) - pattern_index,  # ç½®ä¿¡åº¦åˆ†æ•°
                        'pattern_index': pattern_index
                    })
        
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        candidates.sort(key=lambda x: (-x['confidence'], x['pattern_index']))
        return candidates
    
    def _compare_ocr_results(self, tesseract_results: list, easyocr_results: list, log_messages: list) -> dict:
        """æ¯”è¾ƒä¸¤ä¸ªOCRå¼•æ“çš„ç»“æœï¼Œé€‰æ‹©æœ€ä½³å€™é€‰"""
        all_candidates = []
        
        # æ”¶é›†Tesseractå€™é€‰
        for result in tesseract_results:
            for candidate in result['candidates']:
                all_candidates.append({
                    'text': result['text'],
                    'method': f"Tesseracté…ç½®{result['config']}",
                    'number': candidate['number'],
                    'confidence': candidate['confidence'],
                    'source': 'tesseract'
                })
        
        # æ”¶é›†EasyOCRå€™é€‰
        for result in easyocr_results:
            for candidate in result['candidates']:
                all_candidates.append({
                    'text': result['text'],
                    'method': f"EasyOCRéªŒè¯ ({result['info']})",
                    'number': candidate['number'],
                    'confidence': candidate['confidence'],
                    'source': 'easyocr'
                })
        
        if not all_candidates:
            return None
        
        # æ™ºèƒ½é€‰æ‹©ç­–ç•¥
        # 1. å¦‚æœEasyOCRå’ŒTesseractéƒ½æ‰¾åˆ°ç›¸åŒçš„è®¢å•å·ï¼Œä¼˜å…ˆé€‰æ‹©
        easyocr_numbers = {c['number'] for c in all_candidates if c['source'] == 'easyocr'}
        tesseract_numbers = {c['number'] for c in all_candidates if c['source'] == 'tesseract'}
        common_numbers = easyocr_numbers & tesseract_numbers
        
        if common_numbers:
            # é€‰æ‹©å…±åŒè¯†åˆ«çš„æœ€é«˜ç½®ä¿¡åº¦ç»“æœ
            common_candidates = [c for c in all_candidates if c['number'] in common_numbers]
            best = max(common_candidates, key=lambda x: x['confidence'])
            log_messages.append(f"ğŸ¯ åŒå¼•æ“ç¡®è®¤ç›¸åŒç»“æœ: {best['number']} (ç½®ä¿¡åº¦: {best['confidence']})")
            return best
        
        # 2. EasyOCRç»“æœä¼˜å…ˆï¼ˆç²¾åº¦æ›´é«˜ï¼‰
        easyocr_candidates = [c for c in all_candidates if c['source'] == 'easyocr']
        if easyocr_candidates:
            best = max(easyocr_candidates, key=lambda x: x['confidence'])
            log_messages.append(f"ğŸ¤– é€‰æ‹©EasyOCRæœ€ä½³ç»“æœ: {best['number']} (ç½®ä¿¡åº¦: {best['confidence']})")
            return best
        
        # 3. å›é€€åˆ°Tesseractæœ€ä½³ç»“æœ
        tesseract_candidates = [c for c in all_candidates if c['source'] == 'tesseract']
        if tesseract_candidates:
            best = max(tesseract_candidates, key=lambda x: x['confidence'])
            log_messages.append(f"âš¡ é€‰æ‹©Tesseractæœ€ä½³ç»“æœ: {best['number']} (ç½®ä¿¡åº¦: {best['confidence']})")
            return best
        
        return None
    
    def _get_easyocr_reader(self):
        """è·å–EasyOCRè¯»å–å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
        if not self._easyocr_initialized:
            try:
                if not lazy_import_ocr():
                    return None
                
                logger.info("ğŸš€ åˆå§‹åŒ– EasyOCR å¼•æ“ï¼ˆé¦–æ¬¡ä½¿ç”¨éœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œè¯·ç¨å€™...ï¼‰")
                # æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡ï¼ŒGPUåŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
                self._easyocr_reader = easyocr.Reader(['ch_sim', 'en'], gpu=True)
                self._easyocr_initialized = True
                logger.info("âœ… EasyOCR å¼•æ“åˆå§‹åŒ–å®Œæˆ")
                
            except Exception as e:
                logger.warning(f"EasyOCR åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨ Tesseract: {e}")
                try:
                    # å°è¯•ä»…CPUæ¨¡å¼
                    self._easyocr_reader = easyocr.Reader(['ch_sim', 'en'], gpu=False)
                    self._easyocr_initialized = True
                    logger.info("âœ… EasyOCR CPUæ¨¡å¼åˆå§‹åŒ–å®Œæˆ")
                except Exception as e2:
                    logger.error(f"EasyOCR CPUæ¨¡å¼ä¹Ÿå¤±è´¥: {e2}")
                    self._easyocr_reader = None
                    self._easyocr_initialized = True
        
        return self._easyocr_reader
    
    def _extract_text_with_easyocr(self, image) -> tuple:
        """ä½¿ç”¨EasyOCRæå–æ–‡æœ¬"""
        try:
            reader = self._get_easyocr_reader()
            if reader is None:
                return None, "EasyOCRåˆå§‹åŒ–å¤±è´¥"
            
            # è½¬æ¢PILå›¾åƒä¸ºnumpyæ•°ç»„
            if hasattr(image, 'convert'):
                # PILå›¾åƒ
                cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
            else:
                # å·²ç»æ˜¯numpyæ•°ç»„
                cv_image = image
            
            # EasyOCRè¯†åˆ«
            logger.info("ğŸ” ä½¿ç”¨ EasyOCR è¿›è¡Œé«˜ç²¾åº¦è¯†åˆ«...")
            results = reader.readtext(cv_image, detail=1, paragraph=False)
            
            # åˆå¹¶æ‰€æœ‰è¯†åˆ«çš„æ–‡æœ¬
            text_parts = []
            confidence_scores = []
            
            for (bbox, text, confidence) in results:
                if confidence > 0.3:  # è¿‡æ»¤ä½ç½®ä¿¡åº¦æ–‡æœ¬
                    text_parts.append(text)
                    confidence_scores.append(confidence)
            
            full_text = ' '.join(text_parts)
            avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0
            
            info = f"è¯†åˆ«åˆ°{len(text_parts)}ä¸ªæ–‡æœ¬å—ï¼Œå¹³å‡ç½®ä¿¡åº¦:{avg_confidence:.2f}"
            logger.info(f"EasyOCRè¯†åˆ«ç»“æœ: {info}")
            
            return full_text, info
            
        except Exception as e:
            logger.error(f"EasyOCRè¯†åˆ«å¤±è´¥: {e}")
            return None, f"EasyOCRè¯†åˆ«å¤±è´¥: {str(e)}"
    
    def _enhance_image_for_ocr(self, image):
        """ä¸“é—¨ä¸ºOCRä¼˜åŒ–çš„å›¾åƒå¢å¼ºå¤„ç†ï¼ˆæ•°å­—è¯†åˆ«ä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            # 1. é¦–å…ˆåˆ›å»ºå¤šä¸ªé¢„å¤„ç†ç‰ˆæœ¬
            enhanced_versions = []
            
            # ç‰ˆæœ¬1ï¼šæ ‡å‡†å¢å¼º
            enhanced = image.copy()
            
            # å¯¹æ¯”åº¦å¢å¼ºï¼ˆé’ˆå¯¹æ•°å­—è¯†åˆ«ä¼˜åŒ–ï¼‰
            enhancer = ImageEnhance.Contrast(enhanced)
            enhanced = enhancer.enhance(1.6)  # æé«˜å¯¹æ¯”åº¦
            
            # æ¸…æ™°åº¦å¢å¼º
            enhancer = ImageEnhance.Sharpness(enhanced)
            enhanced = enhancer.enhance(1.5)  # æé«˜æ¸…æ™°åº¦
            
            # äº®åº¦å¾®è°ƒ
            enhancer = ImageEnhance.Brightness(enhanced)
            enhanced = enhancer.enhance(1.05)
            
            enhanced_versions.append(('æ ‡å‡†å¢å¼º', enhanced))
            
            # ç‰ˆæœ¬2ï¼šé«˜å¯¹æ¯”åº¦äºŒå€¼åŒ–ï¼ˆé€‚åˆæ•°å­—è¯†åˆ«ï¼‰
            if enhanced.mode != 'L':
                gray = enhanced.convert('L')
            else:
                gray = enhanced
            
            import numpy as np
            cv_image = np.array(gray)
            
            # é«˜æ–¯æ¨¡ç³Šå»å™ª
            cv_image = cv2.GaussianBlur(cv_image, (1, 1), 0)
            
            # å¤šç§äºŒå€¼åŒ–æ–¹æ³•
            # æ–¹æ³•1: OTSUè‡ªé€‚åº”é˜ˆå€¼
            _, binary1 = cv2.threshold(cv_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
            enhanced_versions.append(('OTSUäºŒå€¼åŒ–', Image.fromarray(binary1)))
            
            # æ–¹æ³•2: è‡ªé€‚åº”é˜ˆå€¼ï¼ˆé«˜æ–¯ï¼‰
            binary2 = cv2.adaptiveThreshold(cv_image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                          cv2.THRESH_BINARY, 11, 2)
            enhanced_versions.append(('è‡ªé€‚åº”é˜ˆå€¼', Image.fromarray(binary2)))
            
            # æ–¹æ³•3: é’ˆå¯¹æ•°å­—ä¼˜åŒ–çš„å½¢æ€å­¦å¤„ç†
            kernel = np.ones((1, 1), np.uint8)
            binary3 = cv2.morphologyEx(binary1, cv2.MORPH_CLOSE, kernel)
            
            # é¢å¤–çš„æ•°å­—åˆ†ç¦»å¤„ç†
            # è†¨èƒ€æ“ä½œï¼Œæœ‰åŠ©äºåˆ†ç¦»ç²˜è¿çš„æ•°å­—
            kernel_dilate = np.ones((2, 2), np.uint8)
            binary3 = cv2.dilate(binary3, kernel_dilate, iterations=1)
            # ç„¶åè…èš€å›åŸå¤§å°
            binary3 = cv2.erode(binary3, kernel_dilate, iterations=1)
            
            enhanced_versions.append(('å½¢æ€å­¦ä¼˜åŒ–', Image.fromarray(binary3)))
            
            # ç‰ˆæœ¬4: åè‰²å¤„ç†ï¼ˆæœ‰æ—¶å¯¹OCRæœ‰å¸®åŠ©ï¼‰
            inverted = cv2.bitwise_not(binary1)
            enhanced_versions.append(('åè‰²å¤„ç†', Image.fromarray(inverted)))
            
            # é€‰æ‹©æœ€ä½³ç‰ˆæœ¬ï¼ˆè¿™é‡Œè¿”å›æ ‡å‡†å¢å¼ºç‰ˆï¼Œä½†ä¿ç•™å…¶ä»–ç‰ˆæœ¬ç”¨äºè°ƒè¯•ï¼‰
            logger.debug(f"ç”Ÿæˆ{len(enhanced_versions)}ä¸ªå›¾åƒå¤„ç†ç‰ˆæœ¬")
            
            # è¿”å›OTSUäºŒå€¼åŒ–ç‰ˆæœ¬ï¼Œé€šå¸¸å¯¹æ•°å­—è¯†åˆ«æ•ˆæœæœ€å¥½
            return enhanced_versions[1][1]  # OTSUäºŒå€¼åŒ–ç‰ˆæœ¬
            
        except Exception as e:
            logger.warning(f"å›¾åƒå¢å¼ºå¤±è´¥ï¼Œä½¿ç”¨åŸå›¾: {e}")
            return image
    
    def _enhance_for_digit_recognition(self, image):
        """ä¸“é—¨é’ˆå¯¹æ•°å­—è¯†åˆ«çš„å›¾åƒå¢å¼º"""
        try:
            import numpy as np
            
            # è½¬æ¢ä¸ºç°åº¦
            if image.mode != 'L':
                gray = image.convert('L')
            else:
                gray = image
            
            cv_image = np.array(gray)
            
            # 1. é«˜æ–¯æ¨¡ç³Šå»å™ª
            denoised = cv2.GaussianBlur(cv_image, (1, 1), 0)
            
            # 2. ç›´æ–¹å›¾å‡è¡¡åŒ–å¢å¼ºå¯¹æ¯”åº¦
            equalized = cv2.equalizeHist(denoised)
            
            # 3. åŒè¾¹æ»¤æ³¢ä¿æŒè¾¹ç¼˜çš„åŒæ—¶å»å™ª
            bilateral = cv2.bilateralFilter(equalized, 9, 75, 75)
            
            # 4. å¤šçº§é˜ˆå€¼å¤„ç†
            # OTSUè‡ªåŠ¨é˜ˆå€¼
            _, otsu = cv2.threshold(bilateral, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
            
            # 5. å½¢æ€å­¦å¼€è¿ç®—å»é™¤å°å™ªå£°
            kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))
            opening = cv2.morphologyEx(otsu, cv2.MORPH_OPEN, kernel)
            
            # 6. è†¨èƒ€æ“ä½œå¢å¼ºæ•°å­—ç¬”ç”»
            kernel_dilate = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 1))
            final = cv2.dilate(opening, kernel_dilate, iterations=1)
            
            return Image.fromarray(final)
            
        except Exception as e:
            logger.warning(f"æ•°å­—è¯†åˆ«å¢å¼ºå¤±è´¥: {e}")
            return image
    
    def detect_text_orientation(self, image) -> int:
        """å¤šæ–¹æ³•ç»“åˆçš„æ–‡æœ¬æ–¹å‘æ£€æµ‹ï¼Œè¿”å›æ ‡å‡†è§’åº¦ï¼š0, 90, 180, 270"""
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            # æ–¹æ³•1: åŸºäºéœå¤«çº¿å˜æ¢çš„è§’åº¦æ£€æµ‹
            angle1, method1_info = self._detect_angle_by_hough_lines(gray)
            logger.debug(f"éœå¤«çº¿æ£€æµ‹: {angle1}Â° - {method1_info}")
            
            # æ–¹æ³•2: åŸºäºæŠ•å½±çš„è§’åº¦æ£€æµ‹
            angle2, method2_info = self._detect_angle_by_projection(gray)
            logger.debug(f"æŠ•å½±æ³•æ£€æµ‹: {angle2}Â° - {method2_info}")
            
            # æ–¹æ³•3: åŸºäºå½¢æ€å­¦æ“ä½œçš„è§’åº¦æ£€æµ‹
            angle3, method3_info = self._detect_angle_by_morphology(gray)
            logger.debug(f"å½¢æ€å­¦æ£€æµ‹: {angle3}Â° - {method3_info}")
            
            # ç»¼åˆåˆ¤æ–­æœ€å¯èƒ½çš„è§’åº¦
            angles = [angle1, angle2, angle3]
            methods_info = [method1_info, method2_info, method3_info]
            angle_counts = {}
            
            for angle in angles:
                if angle in angle_counts:
                    angle_counts[angle] += 1
                else:
                    angle_counts[angle] = 1
            
            # è®°å½•è¯¦ç»†çš„æ£€æµ‹ä¿¡æ¯
            logger.debug(f"è§’åº¦æ£€æµ‹è¯¦æƒ…: éœå¤«çº¿={angle1}Â°, æŠ•å½±={angle2}Â°, å½¢æ€å­¦={angle3}Â°")
            logger.debug(f"è§’åº¦ç»Ÿè®¡: {angle_counts}")
            
            # è¿”å›æœ€å¸¸è§çš„è§’åº¦ï¼Œå¦‚æœå¹³ç¥¨åˆ™ä¼˜å…ˆè¿”å›é0è§’åº¦
            if angle_counts:
                max_count = max(angle_counts.values())
                best_angles = [angle for angle, count in angle_counts.items() if count == max_count]
                
                # ä¼˜å…ˆè¿”å›é0çš„è§’åº¦
                for angle in [90, 180, 270]:
                    if angle in best_angles:
                        logger.debug(f"æœ€ç»ˆé€‰æ‹©è§’åº¦: {angle}Â° (æŠ•ç¥¨æ•°: {max_count})")
                        return angle
                
                final_angle = best_angles[0]
                logger.debug(f"æœ€ç»ˆé€‰æ‹©è§’åº¦: {final_angle}Â° (æŠ•ç¥¨æ•°: {max_count})")
                return final_angle
            
            return 0
            
        except Exception as e:
            logger.warning(f"æ–‡æœ¬æ–¹å‘æ£€æµ‹å¤±è´¥: {e}")
            return 0
    
    def _detect_angle_by_hough_lines(self, gray) -> tuple:
        """ä½¿ç”¨éœå¤«çº¿å˜æ¢æ£€æµ‹è§’åº¦"""
        try:
            # è‡ªé€‚åº”é˜ˆå€¼äºŒå€¼åŒ–
            binary = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                         cv2.THRESH_BINARY, 15, 10)
            
            # å½¢æ€å­¦æ“ä½œå¢å¼ºçº¿æ¡
            kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
            binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
            
            # è¾¹ç¼˜æ£€æµ‹
            edges = cv2.Canny(binary, 50, 150, apertureSize=3)
            
            # éœå¤«çº¿å˜æ¢
            lines = cv2.HoughLines(edges, 1, np.pi/180, threshold=50)
            
            if lines is not None and len(lines) > 10:
                angles = []
                
                for line in lines:
                    rho, theta = line[0]
                    angle_deg = np.degrees(theta)
                    
                    # å°†è§’åº¦æ ‡å‡†åŒ–åˆ° [0, 180) åŒºé—´
                    angle_deg = angle_deg % 180
                    
                    # å°†è§’åº¦æ˜ å°„åˆ°æ ‡å‡†æ–¹å‘
                    if angle_deg <= 45:
                        angles.append(0)  # æ°´å¹³
                    elif angle_deg <= 135:
                        angles.append(90)  # å‚ç›´
                    else:
                        angles.append(0)  # æ°´å¹³
                
                if angles:
                    # ç»Ÿè®¡æœ€é¢‘ç¹çš„è§’åº¦
                    from collections import Counter
                    angle_counter = Counter(angles)
                    most_common_angle, count = angle_counter.most_common(1)[0]
                    
                    info = f"æ£€æµ‹åˆ°{len(lines)}æ¡çº¿ï¼Œ{count}æ¡{most_common_angle}Â°çº¿"
                    
                    # å¦‚æœå‚ç›´çº¿æ¡å¤šï¼Œå¯èƒ½æ˜¯æ—‹è½¬90åº¦
                    if most_common_angle == 90:
                        return 90, info
                    else:
                        return 0, info
            
            return 0, f"æ£€æµ‹åˆ°{len(lines) if lines is not None else 0}æ¡çº¿ï¼Œä¸è¶³ä»¥åˆ¤æ–­"
            
        except Exception as e:
            logger.warning(f"éœå¤«çº¿å˜æ¢è§’åº¦æ£€æµ‹å¤±è´¥: {e}")
            return 0, f"æ£€æµ‹å¤±è´¥: {str(e)}"
    
    def _detect_angle_by_projection(self, gray) -> tuple:
        """ä½¿ç”¨æŠ•å½±æ³•æ£€æµ‹è§’åº¦"""
        try:
            # äºŒå€¼åŒ–
            _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
            
            # è®¡ç®—æ°´å¹³å’Œå‚ç›´æŠ•å½±
            h_proj = np.sum(binary == 0, axis=1)  # æ°´å¹³æŠ•å½±
            v_proj = np.sum(binary == 0, axis=0)  # å‚ç›´æŠ•å½±
            
            # è®¡ç®—æŠ•å½±çš„æ–¹å·®ï¼Œæ–¹å·®å¤§è¯´æ˜è¯¥æ–¹å‘ä¸Šæ–‡æœ¬åˆ†å¸ƒä¸å‡åŒ€ï¼Œå¯èƒ½æ˜¯æ­£ç¡®æ–¹å‘
            h_variance = np.var(h_proj)
            v_variance = np.var(v_proj)
            
            ratio = v_variance / h_variance if h_variance > 0 else 0
            info = f"æ°´å¹³æ–¹å·®={h_variance:.2f}, å‚ç›´æ–¹å·®={v_variance:.2f}, æ¯”å€¼={ratio:.2f}"
            
            # å¦‚æœå‚ç›´æŠ•å½±æ–¹å·®æ˜æ˜¾å¤§äºæ°´å¹³æŠ•å½±ï¼Œè¯´æ˜å¯èƒ½éœ€è¦æ—‹è½¬90åº¦
            if v_variance > h_variance * 1.5:
                return 90, info
            else:
                return 0, info
                
        except Exception as e:
            logger.warning(f"æŠ•å½±æ³•è§’åº¦æ£€æµ‹å¤±è´¥: {e}")
            return 0, f"æ£€æµ‹å¤±è´¥: {str(e)}"
    
    def _detect_angle_by_morphology(self, gray) -> tuple:
        """ä½¿ç”¨å½¢æ€å­¦æ“ä½œæ£€æµ‹è§’åº¦"""
        try:
            # äºŒå€¼åŒ–
            _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
            
            # æ°´å¹³å’Œå‚ç›´ç»“æ„å…ƒç´ 
            h_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (15, 1))
            v_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 15))
            
            # æ£€æµ‹æ°´å¹³å’Œå‚ç›´çº¿æ¡
            h_lines = cv2.morphologyEx(binary, cv2.MORPH_OPEN, h_kernel)
            v_lines = cv2.morphologyEx(binary, cv2.MORPH_OPEN, v_kernel)
            
            # ç»Ÿè®¡çº¿æ¡æ•°é‡
            h_count = cv2.countNonZero(h_lines)
            v_count = cv2.countNonZero(v_lines)
            
            ratio = v_count / h_count if h_count > 0 else 0
            info = f"æ°´å¹³çº¿={h_count}åƒç´ , å‚ç›´çº¿={v_count}åƒç´ , æ¯”å€¼={ratio:.2f}"
            
            # å¦‚æœå‚ç›´çº¿æ¡æ˜æ˜¾å¤šäºæ°´å¹³çº¿æ¡ï¼Œå¯èƒ½éœ€è¦æ—‹è½¬
            if v_count > h_count * 1.3:
                return 90, info
            else:
                return 0, info
                
        except Exception as e:
            logger.warning(f"å½¢æ€å­¦è§’åº¦æ£€æµ‹å¤±è´¥: {e}")
            return 0, f"æ£€æµ‹å¤±è´¥: {str(e)}"
    
    def rotate_image(self, image, angle):
        """å®‰å…¨åœ°æ—‹è½¬å›¾åƒ"""
        try:
            if angle == 0:
                return image
            
            # è·å–å›¾åƒå°ºå¯¸
            height, width = image.shape[:2]
            center = (width // 2, height // 2)
            
            # åˆ›å»ºæ—‹è½¬çŸ©é˜µ
            rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)
            
            # è®¡ç®—æ–°çš„è¾¹ç•Œæ¡†å°ºå¯¸
            cos_val = np.abs(rotation_matrix[0, 0])
            sin_val = np.abs(rotation_matrix[0, 1])
            new_width = int((height * sin_val) + (width * cos_val))
            new_height = int((height * cos_val) + (width * sin_val))
            
            # è°ƒæ•´æ—‹è½¬çŸ©é˜µçš„å¹³ç§»éƒ¨åˆ†
            rotation_matrix[0, 2] += (new_width / 2) - center[0]
            rotation_matrix[1, 2] += (new_height / 2) - center[1]
            
            # æ‰§è¡Œæ—‹è½¬
            rotated = cv2.warpAffine(image, rotation_matrix, (new_width, new_height), 
                                   flags=cv2.INTER_CUBIC, 
                                   borderMode=cv2.BORDER_CONSTANT, 
                                   borderValue=(255, 255, 255))
            
            return rotated
            
        except Exception as e:
            logger.warning(f"å›¾åƒæ—‹è½¬å¤±è´¥: {e}")
            return image
    
    def extract_with_ocr(self, pdf_path: str) -> Tuple[Optional[str], List[str]]:
        """ä½¿ç”¨OCRä»PDFä¸­æå–æ–‡æœ¬å¹¶æŸ¥æ‰¾é”€è´§å‡ºåº“å•å·ï¼ŒåŒ…å«æ”¹è¿›çš„æ—‹è½¬æ£€æµ‹"""
        log_messages = []
        
        if not lazy_import_ocr():
            return None, ["âŒ OCRåº“å¯¼å…¥å¤±è´¥"]
        
        try:
            doc = fitz.open(pdf_path)
            
            for page_num in range(len(doc)):
                page = doc[page_num]
                log_messages.append(f"ğŸ“„ å¤„ç†ç¬¬{page_num + 1}é¡µ")
                
                # æé«˜åˆ†è¾¨ç‡ä»¥æ”¹å–„OCRæ•ˆæœ
                mat = fitz.Matrix(2.0, 2.0)  # 200 DPI
                pix = page.get_pixmap(matrix=mat)
                img_data = pix.tobytes("png")
                pix = None
                
                # è½¬æ¢ä¸ºPILå›¾åƒ
                pil_image = Image.open(io.BytesIO(img_data))
                
                # é™åˆ¶å›¾åƒå¤§å°ä»¥èŠ‚çœå†…å­˜
                max_size = 1500
                if max(pil_image.size) > max_size:
                    ratio = max_size / max(pil_image.size)
                    new_size = tuple(int(dim * ratio) for dim in pil_image.size)
                    pil_image = pil_image.resize(new_size, Image.Resampling.LANCZOS)
                
                # ç¡®ä¿å›¾åƒæ˜¯RGBæ¨¡å¼
                if pil_image.mode != 'RGB':
                    pil_image = pil_image.convert('RGB')
                
                # è½¬æ¢ä¸ºOpenCVæ ¼å¼
                cv_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
                
                # æ£€æµ‹æ–‡æœ¬æ–¹å‘
                detected_angle = self.detect_text_orientation(cv_image)
                log_messages.append(f"ğŸ” æ™ºèƒ½è§’åº¦æ£€æµ‹ç»“æœ: {detected_angle}Â°")
                
                # è·å–è¯¦ç»†çš„æ£€æµ‹ä¿¡æ¯ï¼ˆä¸´æ—¶å¯ç”¨debugçº§åˆ«ï¼‰
                old_level = logger.level
                logger.setLevel(logging.DEBUG)
                
                # é‡æ–°è¿è¡Œæ£€æµ‹ä»¥è·å–è¯¦ç»†ä¿¡æ¯
                gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)
                angle1, method1_info = self._detect_angle_by_hough_lines(gray)
                angle2, method2_info = self._detect_angle_by_projection(gray)
                angle3, method3_info = self._detect_angle_by_morphology(gray)
                
                log_messages.append(f"   ğŸ“Š éœå¤«çº¿å˜æ¢: {angle1}Â° ({method1_info})")
                log_messages.append(f"   ğŸ“Š æŠ•å½±åˆ†æ: {angle2}Â° ({method2_info})")
                log_messages.append(f"   ğŸ“Š å½¢æ€å­¦åˆ†æ: {angle3}Â° ({method3_info})")
                
                # æ¢å¤åŸæ—¥å¿—çº§åˆ«
                logger.setLevel(old_level)
                
                # åŸºäºæ£€æµ‹ç»“æœä¼˜åŒ–è§’åº¦å°è¯•ç­–ç•¥
                if detected_angle == 0:
                    # å¦‚æœæ£€æµ‹ä¸ºæ­£å¸¸æ–¹å‘ï¼Œä¼˜å…ˆå°è¯•0åº¦ï¼Œç„¶åå°è¯•å…¶ä»–è§’åº¦
                    angles_to_try = [0, 90, 270, 180]
                    log_messages.append("ğŸ“ ä½¿ç”¨æ ‡å‡†è§’åº¦åºåˆ—: 0Â° â†’ 90Â° â†’ 270Â° â†’ 180Â°")
                elif detected_angle == 90:
                    # å¦‚æœæ£€æµ‹ä¸º90åº¦æ—‹è½¬ï¼Œä¼˜å…ˆå°è¯•-90åº¦(270åº¦)æ ¡æ­£
                    angles_to_try = [270, 90, 0, 180]
                    log_messages.append("ğŸ“ æ£€æµ‹åˆ°90Â°æ—‹è½¬ï¼Œä¼˜å…ˆå°è¯•270Â°æ ¡æ­£")
                else:
                    # å…¶ä»–æƒ…å†µæŒ‰æ£€æµ‹è§’åº¦ä¼˜å…ˆ
                    angles_to_try = [detected_angle, 0, 90, 270, 180]
                    log_messages.append(f"ğŸ“ æŒ‰æ£€æµ‹è§’åº¦{detected_angle}Â°ä¼˜å…ˆå°è¯•")
                
                # å»é‡
                angles_to_try = list(dict.fromkeys(angles_to_try))
                
                for angle in angles_to_try:
                    try:
                        # æ—‹è½¬å›¾åƒ
                        if angle != 0:
                            rotated_cv = self.rotate_image(cv_image, angle)
                            rotated_pil = Image.fromarray(cv2.cvtColor(rotated_cv, cv2.COLOR_BGR2RGB))
                        else:
                            rotated_pil = pil_image
                        
                        # å¤šå±‚å›¾åƒå¢å¼ºå¤„ç†
                        enhanced_image = self._enhance_image_for_ocr(rotated_pil)
                        
                        # åŒé‡éªŒè¯ç­–ç•¥ï¼šTesseractåˆè¯†åˆ« + EasyOCRç²¾ç¡®ç¡®è®¤
                        tesseract_results = []
                        easyocr_results = []
                        final_text = ""
                        ocr_method_used = ""
                        
                        # ç¬¬ä¸€æ­¥ï¼šTesseractå¿«é€Ÿåˆæ­¥è¯†åˆ«
                        log_messages.append(f"âš¡ ç¬¬ä¸€æ­¥ï¼šTesseractå¿«é€Ÿæ‰«æ (è§’åº¦: {angle}Â°)...")
                        
                        ocr_configs = [
                            # é…ç½®1ï¼šä¸“é—¨é’ˆå¯¹æ•°å­—å’Œè¿å­—ç¬¦çš„è¯†åˆ«
                            r'--oem 3 --psm 6 -c tessedit_char_whitelist=0123456789-_ï¼š: ',
                            # é…ç½®2ï¼šåŒ…å«å­—æ¯çš„è®¢å•å·
                            r'--oem 3 --psm 6 -c tessedit_char_whitelist=0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz-_ï¼š: ',
                            # é…ç½®3ï¼šåŒ…å«ä¸­æ–‡çš„å®Œæ•´è¯†åˆ«
                            r'--oem 3 --psm 6',
                        ]
                        
                        tesseract_found_candidates = False
                        for config_index, custom_config in enumerate(ocr_configs):
                            try:
                                tesseract_text = pytesseract.image_to_string(enhanced_image, lang='chi_sim+eng', config=custom_config)
                                if tesseract_text and tesseract_text.strip():
                                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ½œåœ¨çš„è®¢å•å·å€™é€‰
                                    potential_orders = self.find_all_order_candidates(tesseract_text)
                                    if potential_orders:
                                        tesseract_results.append({
                                            'text': tesseract_text,
                                            'config': config_index + 1,
                                            'candidates': potential_orders
                                        })
                                        tesseract_found_candidates = True
                                        log_messages.append(f"âœ… Tesseracté…ç½®{config_index + 1}æ‰¾åˆ°{len(potential_orders)}ä¸ªå€™é€‰")
                                        break
                            except Exception as e:
                                log_messages.append(f"âš ï¸ Tesseracté…ç½®{config_index + 1}å¤±è´¥: {str(e)}")
                                continue
                        
                        # ç¬¬äºŒæ­¥ï¼šæ ¹æ®Tesseractç»“æœå†³å®šEasyOCRç­–ç•¥
                        if tesseract_found_candidates:
                            # ç­–ç•¥Aï¼šæœ‰å€™é€‰ç»“æœï¼Œç”¨EasyOCRç²¾ç¡®éªŒè¯
                            log_messages.append(f"ğŸ¯ ç¬¬äºŒæ­¥ï¼šEasyOCRç²¾ç¡®éªŒè¯Tesseractå€™é€‰ç»“æœ...")
                            easyocr_text, easyocr_info = self._extract_text_with_easyocr(enhanced_image)
                            
                            if easyocr_text and easyocr_text.strip():
                                easyocr_candidates = self.find_all_order_candidates(easyocr_text)
                                easyocr_results.append({
                                    'text': easyocr_text,
                                    'info': easyocr_info,
                                    'candidates': easyocr_candidates
                                })
                                log_messages.append(f"âœ… EasyOCRéªŒè¯ï¼š{easyocr_info}ï¼Œæ‰¾åˆ°{len(easyocr_candidates)}ä¸ªå€™é€‰")
                            
                            # æ¯”è¾ƒä¸¤ä¸ªå¼•æ“çš„ç»“æœ
                            best_result = self._compare_ocr_results(tesseract_results, easyocr_results, log_messages)
                            if best_result:
                                final_text = best_result['text']
                                ocr_method_used = best_result['method']
                        else:
                            # ç­–ç•¥Bï¼šTesseractæ²¡æ‰¾åˆ°å€™é€‰ï¼Œç›´æ¥ç”¨EasyOCRå…¨åŠ›è¯†åˆ«
                            log_messages.append(f"ğŸ¤– ç¬¬äºŒæ­¥ï¼šTesseractæ— å€™é€‰ï¼ŒEasyOCRå…¨åŠ›è¯†åˆ«...")
                            easyocr_text, easyocr_info = self._extract_text_with_easyocr(enhanced_image)
                            
                            if easyocr_text and easyocr_text.strip():
                                final_text = easyocr_text
                                ocr_method_used = f"EasyOCRç‹¬ç«‹è¯†åˆ« ({easyocr_info})"
                                log_messages.append(f"âœ… EasyOCRç‹¬ç«‹è¯†åˆ«ï¼š{easyocr_info}")
                            else:
                                log_messages.append(f"âŒ åŒå¼•æ“å‡æœªè¯†åˆ«åˆ°æœ‰æ•ˆæ–‡æœ¬")
                        
                        text = final_text
                        
                        if text.strip():
                            log_messages.append(f"ğŸ“ ä½¿ç”¨{ocr_method_used}è¯†åˆ«æ–‡æœ¬ç‰‡æ®µ: {text[:150]}...")
                            
                            # æŸ¥æ‰¾é”€è´§å‡ºåº“å•å·
                            candidates = self.find_all_order_candidates(text)
                            if candidates:
                                best_candidate = candidates[0]  # å·²æŒ‰ç½®ä¿¡åº¦æ’åº
                                order_number = best_candidate['number']
                                log_messages.append(f"âœ… æ‰¾åˆ°é”€è´§å‡ºåº“å•å·: {order_number} (ä½¿ç”¨{ocr_method_used}ï¼Œç½®ä¿¡åº¦: {best_candidate['confidence']})")
                                doc.close()
                                return order_number, log_messages
                        else:
                            log_messages.append(f"âš ï¸ è§’åº¦{angle}Â°æ‰€æœ‰OCRæ–¹æ³•éƒ½æœªè¯†åˆ«åˆ°æ–‡æœ¬")
                        
                    except Exception as e:
                        log_messages.append(f"âš ï¸ è§’åº¦{angle}Â°å¤„ç†å¤±è´¥: {str(e)}")
                        continue
            
            doc.close()
            log_messages.append("âŒ æ‰€æœ‰é¡µé¢å’Œè§’åº¦éƒ½æœªæ‰¾åˆ°é”€è´§å‡ºåº“å•å·")
            return None, log_messages
            
        except Exception as e:
            error_msg = f"âŒ OCRå¤„ç†å¤±è´¥: {str(e)}"
            log_messages.append(error_msg)
            logger.error(error_msg, exc_info=True)
            return None, log_messages
    
    def clean_filename(self, order_number: str) -> str:
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦"""
        try:
            # ç§»é™¤æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦
            cleaned = re.sub(r'[<>:"/\\|?*]', '_', order_number)
            # ç§»é™¤é¦–å°¾ç©ºæ ¼
            cleaned = cleaned.strip()
            # ç¡®ä¿ä¸ä¸ºç©º
            if not cleaned:
                cleaned = "unknown_order"
            return cleaned
        except Exception as e:
            logger.error(f"æ–‡ä»¶åæ¸…ç†å¤±è´¥: {e}")
            return "unknown_order"

processor = PDFProcessor()

@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    """ä¸»é¡µ"""
    return templates.TemplateResponse("index.html", {"request": request})

def create_backup(file_path: str, original_filename: str) -> str:
    """åˆ›å»ºæ–‡ä»¶å¤‡ä»½"""
    try:
        from datetime import datetime
        import shutil
        
        # åˆ›å»ºæ—¥æœŸæ–‡ä»¶å¤¹
        today = datetime.now().strftime("%Y-%m-%d")
        backup_dir = Path(f"backup/{today}")
        backup_dir.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆå¤‡ä»½æ–‡ä»¶è·¯å¾„
        backup_path = backup_dir / original_filename
        
        # å¦‚æœå¤‡ä»½æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ æ—¶é—´æˆ³
        if backup_path.exists():
            timestamp = datetime.now().strftime("%H%M%S")
            name_part = backup_path.stem
            ext_part = backup_path.suffix
            backup_path = backup_dir / f"{name_part}_{timestamp}{ext_part}"
        
        # å¤åˆ¶æ–‡ä»¶åˆ°å¤‡ä»½ç›®å½•
        shutil.copy2(file_path, backup_path)
        
        logger.info(f"ğŸ“‚ å¤‡ä»½åˆ›å»ºæˆåŠŸ: {backup_path}")
        return str(backup_path)
        
    except Exception as e:
        logger.error(f"å¤‡ä»½åˆ›å»ºå¤±è´¥: {e}")
        raise

@app.post("/upload")
async def upload_files(files: List[UploadFile] = File(...), enableBackup: str = "true"):
    """ä¸Šä¼ æ–‡ä»¶å¹¶æå–é”€è´§å‡ºåº“å•å·è¿›è¡Œé‡å‘½å"""
    global filename_mapping
    
    # å¼€å§‹å¤„ç†å‰åªæ¸…ç†åŸå§‹æ–‡ä»¶åæ ¼å¼çš„æ–‡ä»¶ï¼ˆä¿ç•™ä¹‹å‰çš„æ­£ç¡®ç»“æœï¼‰
    cleaned_count = clean_original_filename_files()
    if cleaned_count > 0:
        logger.info(f"ğŸ§¹ å¤„ç†å¼€å§‹å‰æ¸…ç†äº† {cleaned_count} ä¸ªåŸå§‹æ–‡ä»¶åæ ¼å¼æ–‡ä»¶")
    
    # è®°å½•å¤„ç†å¼€å§‹æ—¶downloadsç›®å½•çŠ¶æ€
    downloads_dir = Path("downloads")
    if downloads_dir.exists():
        existing_files = list(downloads_dir.glob("*.pdf"))
        logger.info(f"ğŸ“Š æ‰¹æ¬¡å¤„ç†å¼€å§‹ - downloadsç›®å½•ç°æœ‰æ–‡ä»¶: {len(existing_files)}ä¸ª")
        for existing_file in existing_files:
            logger.info(f"  å·²å­˜åœ¨: {existing_file.name}")
    else:
        logger.info(f"ğŸ“Š æ‰¹æ¬¡å¤„ç†å¼€å§‹ - downloadsç›®å½•ä¸å­˜åœ¨")
    
    if not files:
        raise HTTPException(status_code=400, detail="æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶")
        
    processor = PDFProcessor()
    results = []
    processed_count = 0
    backup_enabled = enableBackup.lower() == "true"
    
    logger.info(f"å¤„ç†æ¨¡å¼: {'å¯ç”¨å¤‡ä»½' if backup_enabled else 'ç¦ç”¨å¤‡ä»½'}")
    
    for file in files:
        if not file.filename or not file.filename.lower().endswith('.pdf'):
            results.append({
                "filename": file.filename or "unknown",
                "success": False,
                "message": "âŒ ä¸æ˜¯PDFæ–‡ä»¶"
            })
            continue
        
        upload_path = None
        try:
            # å¤„ç†æ–‡ä»¶åï¼Œå»é™¤è·¯å¾„ä¿¡æ¯ï¼ˆæ–‡ä»¶å¤¹ä¸Šä¼ æ—¶çš„è·¯å¾„ï¼‰
            clean_filename = os.path.basename(file.filename) if file.filename else "unknown.pdf"
            
            # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°uploadsæ ¹ç›®å½•
            upload_path = f"uploads/{clean_filename}"
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ æ—¶é—´æˆ³é¿å…å†²çª
            if os.path.exists(upload_path):
                from datetime import datetime
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                name_part = os.path.splitext(clean_filename)[0]
                ext_part = os.path.splitext(clean_filename)[1]
                upload_path = f"uploads/{name_part}_{timestamp}{ext_part}"
            
            with open(upload_path, "wb") as buffer:
                content = await file.read()
                if not content:
                    raise ValueError("æ–‡ä»¶å†…å®¹ä¸ºç©º")
                buffer.write(content)
            
            logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file.filename} -> {clean_filename}")
            
            # å¤„ç†PDFæ–‡ä»¶
            order_number, log = processor.extract_order_number(upload_path)
            
            if order_number:
                # æ¸…ç†æ–‡ä»¶å
                clean_order = processor.clean_filename(order_number)
                new_filename = f"{clean_order}.pdf"
                
                # æ ¹æ®å¤‡ä»½æ¨¡å¼å†³å®šæ˜¯å¦åˆ›å»ºå¤‡ä»½
                backup_path = "æœªå¯ç”¨å¤‡ä»½"
                if backup_enabled:
                    # ä½¿ç”¨åŸå§‹æ–‡ä»¶åè¿›è¡Œå¤‡ä»½ï¼Œä¿æŒæ–‡ä»¶å¤¹ç»“æ„ä¿¡æ¯
                    backup_filename = file.filename if file.filename else clean_filename
                    backup_path = create_backup(upload_path, backup_filename)
                
                # å¤„ç†é‡åæ–‡ä»¶ - ç¡®ä¿downloadsç›®å½•å­˜åœ¨
                os.makedirs("downloads", exist_ok=True)
                counter = 1
                base_name = clean_order
                final_filename = new_filename
                
                # åŠ å¼ºæ–‡ä»¶åå†²çªæ£€æµ‹ - æŸ¥æ‰¾æ•´ä¸ªdownloadsç›®å½•ï¼Œä¸ä»…æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§è¿˜è€ƒè™‘å¤„ç†é˜Ÿåˆ—ä¸­çš„å…¶ä»–æ–‡ä»¶
                existing_downloads = set(os.path.basename(f) for f in glob.glob("downloads/*.pdf"))
                in_process_names = set()  # è·Ÿè¸ªå½“å‰æ‰¹æ¬¡ä¸­å·²ä½¿ç”¨çš„æ–‡ä»¶å
                
                # æ£€æŸ¥resultsä¸­æ˜¯å¦å·²æœ‰ç›¸åŒæ–‡ä»¶åï¼ˆåœ¨å½“å‰æ‰¹æ¬¡ä¸­ï¼‰
                for result in results:
                    if result.get("new_filename"):
                        in_process_names.add(result["new_filename"])
                
                logger.info(f"æ–‡ä»¶åæ£€æŸ¥ - åŸºç¡€åç§°: {base_name}, åˆå§‹æ–‡ä»¶å: {final_filename}")
                logger.info(f"å·²å­˜åœ¨çš„ä¸‹è½½æ–‡ä»¶: {len(existing_downloads)}ä¸ª, å½“å‰æ‰¹æ¬¡ä¸­å·²ä½¿ç”¨åç§°: {len(in_process_names)}ä¸ª")
                
                # ç¡®ä¿æ–‡ä»¶åä¸å†²çª
                while final_filename in existing_downloads or final_filename in in_process_names:
                    counter += 1
                    final_filename = f"{base_name}_{counter}.pdf"
                    logger.info(f"æ–‡ä»¶åå†²çªï¼Œå°è¯•æ–°åç§°: {final_filename}")
                
                # å­˜å‚¨æ–‡ä»¶åæ˜ å°„å…³ç³» - é‡å‘½åååˆ°åŸå§‹æ–‡ä»¶åçš„æ˜ å°„
                original_filename = os.path.basename(upload_path)
                filename_mapping[final_filename] = original_filename
                
                # åˆ›å»ºé‡å‘½åå‰¯æœ¬åˆ°downloadsæ–‡ä»¶å¤¹ - å¢å¼ºæ–‡ä»¶æ“ä½œç¨³å®šæ€§
                download_path = f"downloads/{final_filename}"
                try:
                    import shutil
                    import time
                    
                    # ç¡®ä¿downloadsç›®å½•å­˜åœ¨ä¸”å¯å†™
                    downloads_dir = Path("downloads")
                    downloads_dir.mkdir(exist_ok=True)
                    
                    # å¤šæ¬¡å°è¯•æ–‡ä»¶å¤åˆ¶ï¼Œæé«˜æˆåŠŸç‡
                    copy_success = False
                    max_copy_attempts = 3
                    
                    for attempt in range(max_copy_attempts):
                        try:
                            # å¤åˆ¶æ–‡ä»¶å‰é¢å¤–æ£€æŸ¥
                            if not os.path.exists(upload_path) or os.path.getsize(upload_path) == 0:
                                raise Exception(f"æºæ–‡ä»¶æ— æ•ˆæˆ–å¤§å°ä¸º0: {upload_path}")
                            
                            # å¤åˆ¶å‰æ£€æŸ¥ç›®æ ‡æ˜¯å¦å·²å­˜åœ¨ï¼ˆåŒé‡æ£€æŸ¥ï¼‰
                            if os.path.exists(download_path):
                                logger.warning(f"ç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œå°†è¢«è¦†ç›–: {download_path}")
                                # æ·»åŠ é¢å¤–çš„å®‰å…¨ä¿éšœ - é‡å‘½åå·²å­˜åœ¨çš„æ–‡ä»¶è€Œä¸æ˜¯è¦†ç›–
                                backup_filename = f"{download_path}.bak.{int(time.time())}"
                                os.rename(download_path, backup_filename)
                                logger.info(f"å·²å­˜åœ¨çš„æ–‡ä»¶å·²å¤‡ä»½ä¸º: {backup_filename}")
                            
                            # å¤åˆ¶æ–‡ä»¶
                            shutil.copy2(upload_path, download_path)
                            
                            # éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
                            if os.path.exists(download_path) and os.path.getsize(download_path) > 0:
                                # é¢å¤–éªŒè¯ï¼šæ£€æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦ä¸€è‡´
                                if os.path.getsize(upload_path) == os.path.getsize(download_path):
                                    # æ–‡ä»¶å†…å®¹éªŒè¯
                                    with open(upload_path, "rb") as src_file:
                                        src_data = src_file.read(1024)  # è¯»å–å‰1KBåšå®Œæ•´æ€§éªŒè¯
                                    with open(download_path, "rb") as dst_file:
                                        dst_data = dst_file.read(1024)
                                    
                                    if src_data == dst_data:
                                        copy_success = True
                                        logger.info(f"æ–‡ä»¶å·²æˆåŠŸä¿å­˜åˆ°ä¸‹è½½ç›®å½•: {download_path} (å°è¯•{attempt+1}/{max_copy_attempts})")
                                        
                                        # é¢å¤–æ£€æŸ¥æ–‡ä»¶æ˜¯å¦çœŸå®å­˜åœ¨ï¼ˆé˜²æ­¢NFSç¼“å­˜ç­‰é—®é¢˜ï¼‰
                                        time.sleep(0.1)  # çŸ­æš‚ç­‰å¾…æ–‡ä»¶ç³»ç»ŸåŒæ­¥
                                        if os.path.exists(download_path):
                                            logger.info(f"æ–‡ä»¶ç¡®è®¤å­˜åœ¨: {download_path}, å¤§å°: {os.path.getsize(download_path)}å­—èŠ‚")
                                        else:
                                            raise Exception("æ–‡ä»¶ç³»ç»ŸåŒæ­¥åæ–‡ä»¶ä¸å­˜åœ¨")
                                            
                                        break
                                    else:
                                        raise Exception("æ–‡ä»¶å†…å®¹éªŒè¯å¤±è´¥ï¼Œå¯èƒ½å¤åˆ¶ä¸å®Œæ•´")
                                else:
                                    raise Exception("æ–‡ä»¶å¤§å°ä¸ä¸€è‡´ï¼Œå¯èƒ½å¤åˆ¶ä¸å®Œæ•´")
                            else:
                                raise Exception("æ–‡ä»¶å¤åˆ¶åä¸å­˜åœ¨æˆ–å¤§å°ä¸º0")
                                
                        except Exception as attempt_error:
                            logger.warning(f"æ–‡ä»¶å¤åˆ¶å°è¯•{attempt+1}å¤±è´¥: {attempt_error}")
                            # æ¸…ç†å¯èƒ½çš„ä¸å®Œæ•´æ–‡ä»¶
                            if os.path.exists(download_path):
                                try:
                                    os.remove(download_path)
                                    logger.info(f"å·²æ¸…ç†ä¸å®Œæ•´æ–‡ä»¶: {download_path}")
                                except:
                                    logger.warning(f"æ¸…ç†ä¸å®Œæ•´æ–‡ä»¶å¤±è´¥: {download_path}")
                            
                            if attempt < max_copy_attempts - 1:
                                time.sleep(0.5 * (attempt + 1))  # é€’å¢ç­‰å¾…æ—¶é—´åé‡è¯•
                                logger.info(f"å°†åœ¨{0.5 * (attempt + 1)}ç§’åé‡è¯•")
                            else:
                                raise attempt_error
                    
                    if not copy_success:
                        raise Exception(f"ç»è¿‡{max_copy_attempts}æ¬¡å°è¯•ï¼Œæ–‡ä»¶å¤åˆ¶ä»ç„¶å¤±è´¥")
                        
                    # å®‰å…¨åˆ é™¤uploadsä¸­çš„ä¸´æ—¶æ–‡ä»¶
                    if os.path.exists(upload_path):
                        try:
                            os.remove(upload_path)
                            logger.info(f"ä¸´æ—¶æ–‡ä»¶å·²åˆ é™¤: {upload_path}")
                        except Exception as del_error:
                            logger.warning(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥ï¼ˆä½†ä¸å½±å“ä¸»æµç¨‹ï¼‰: {del_error}")
                        
                except Exception as copy_error:
                    logger.error(f"æ–‡ä»¶æ“ä½œå¤±è´¥: {copy_error}")
                    # å°è¯•æ¸…ç†å¯èƒ½çš„ä¸å®Œæ•´æ–‡ä»¶
                    if os.path.exists(download_path):
                        try:
                            os.remove(download_path)
                        except:
                            pass
                    raise Exception(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {copy_error}")
                
                results.append({
                    "filename": file.filename,
                    "success": True,
                    "message": f"âœ… æ–‡ä»¶å¤„ç†æˆåŠŸ: {file.filename} â†’ {final_filename}",
                    "new_filename": final_filename,
                    "backup_path": backup_path,
                    "order_number": order_number,
                    "log": log,
                    "download_ready": True
                })
                
                logger.info(f"æ–‡ä»¶å¤„ç†æˆåŠŸ: {file.filename} -> {final_filename}")
                processed_count += 1
                
            else:
                results.append({
                    "filename": file.filename,
                    "success": False,
                    "message": "âŒ æœªæ‰¾åˆ°é”€è´§å‡ºåº“å•å·",
                    "log": log
                })
                
                logger.warning(f"æœªæ‰¾åˆ°é”€è´§å‡ºåº“å•å·: {file.filename}")
            
        except Exception as e:
            error_msg = f"å¤„ç†å¤±è´¥: {str(e)}"
            results.append({
                "filename": file.filename or "unknown",
                "success": False,
                "message": f"âŒ {error_msg}",
                "log": f"ç³»ç»Ÿé”™è¯¯: {str(e)}"
            })
            logger.error(f"æ–‡ä»¶å¤„ç†å¤±è´¥ {file.filename}: {error_msg}", exc_info=True)
        
        finally:
            # æ¸…ç†uploadsä¸­çš„ä¸´æ—¶æ–‡ä»¶
            if upload_path and os.path.exists(upload_path) and upload_path.startswith("uploads/"):
                try:
                    os.remove(upload_path)
                except Exception as e:
                    logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            import gc
            gc.collect()
    
    logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {processed_count} ä¸ªæ–‡ä»¶")
    
    # è®°å½•å¤„ç†ç»“æŸæ—¶downloadsç›®å½•çŠ¶æ€
    if downloads_dir.exists():
        final_files = list(downloads_dir.glob("*.pdf"))
        logger.info(f"ğŸ“Š æ‰¹æ¬¡å¤„ç†ç»“æŸ - downloadsç›®å½•ç°æœ‰æ–‡ä»¶: {len(final_files)}ä¸ª")
        for final_file in final_files:
            logger.info(f"  æœ€ç»ˆå­˜åœ¨: {final_file.name}")
    else:
        logger.info(f"ğŸ“Š æ‰¹æ¬¡å¤„ç†ç»“æŸ - downloadsç›®å½•ä¸å­˜åœ¨")
    
    # ç”Ÿæˆå¤„ç†ä¿¡æ¯
    process_info = None
    if processed_count > 0:
        from datetime import datetime
        download_info = f"ğŸ“¥ å¤„ç†å®Œæˆï¼Œ{processed_count} ä¸ªæ–‡ä»¶å¯ä¾›ä¸‹è½½"
        if backup_enabled:
            backup_info = f"å¤‡ä»½ä¿å­˜åœ¨: backup/{datetime.now().strftime('%Y-%m-%d')}/"
            process_info = f"{download_info}ï¼Œ{backup_info}"
        else:
            process_info = download_info
    
    return JSONResponse({
        "results": results,
        "processed_count": processed_count,
        "total_count": len(files),
        "process_info": process_info,
        "download_available": processed_count > 0
    })

@app.get("/backup-info")
async def get_backup_info():
    """è·å–å¤‡ä»½ä¿¡æ¯"""
    try:
        backup_root = Path("backup")
        if not backup_root.exists():
            return JSONResponse({
                "backup_folders": [],
                "total_backups": 0,
                "message": "æš‚æ— å¤‡ä»½æ–‡ä»¶"
            })
        
        backup_folders = []
        total_backups = 0
        
        # è·å–æ‰€æœ‰æ—¥æœŸæ–‡ä»¶å¤¹
        for date_folder in sorted(backup_root.iterdir(), reverse=True):
            if date_folder.is_dir():
                pdf_files = list(date_folder.glob("*.pdf"))
                if pdf_files:
                    backup_folders.append({
                        "date": date_folder.name,
                        "file_count": len(pdf_files),
                        "files": [f.name for f in pdf_files]
                    })
                    total_backups += len(pdf_files)
        
        return JSONResponse({
            "backup_folders": backup_folders,
            "total_backups": total_backups,
            "message": f"å…±æœ‰ {len(backup_folders)} å¤©çš„å¤‡ä»½ï¼Œæ€»è®¡ {total_backups} ä¸ªæ–‡ä»¶"
        })
        
    except Exception as e:
        logger.error(f"è·å–å¤‡ä»½ä¿¡æ¯å¤±è´¥: {e}")
        return JSONResponse({
            "backup_folders": [],
            "total_backups": 0,
            "message": f"è·å–å¤‡ä»½ä¿¡æ¯å¤±è´¥: {str(e)}"
        })

@app.get("/download-list")
async def get_download_list():
    """è·å–å¯ä¸‹è½½æ–‡ä»¶åˆ—è¡¨"""
    downloads_dir = Path("downloads")
    files = []
    
    try:
        if downloads_dir.exists():
            pdf_files = list(downloads_dir.glob("*.pdf"))
            
            # æŸ¥æ‰¾æ‰€æœ‰ä¸‹è½½æ–‡ä»¶ï¼Œä¼˜å…ˆä½¿ç”¨é‡å‘½ååæ–‡ä»¶å
            renamed_files = []
            for file in pdf_files:
                # æ£€æŸ¥æ­¤æ–‡ä»¶æ˜¯å¦æ˜¯é‡å‘½ååçš„æ–‡ä»¶ï¼ˆé€šè¿‡æ£€æŸ¥å‘½åæ¨¡å¼ï¼‰
                if "-" in file.stem and file.stem.count("-") == 1:
                    renamed_files.append(file.name)
                else:
                    # å¯¹äºåŸå§‹æ–‡ä»¶åæ ¼å¼ï¼ŒæŸ¥æ‰¾æ˜¯å¦æœ‰å¯¹åº”çš„é‡å‘½åæ˜ å°„
                    found = False
                    for renamed, original in filename_mapping.items():
                        if file.name == original:
                            renamed_files.append(renamed)
                            found = True
                            break
                    if not found:
                        # å¦‚æœæ²¡æœ‰æ˜ å°„å…³ç³»ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ–‡ä»¶å
                        renamed_files.append(file.name)
            
            files = renamed_files
            logger.info(f"ä¸‹è½½ç›®å½•æ£€æŸ¥: æ‰¾åˆ° {len(files)} ä¸ªPDFæ–‡ä»¶")
            
            # è¯¦ç»†æ—¥å¿—
            for file in files:
                logger.info(f"  - {file}")
        else:
            logger.warning("ä¸‹è½½ç›®å½•ä¸å­˜åœ¨")
    
        return JSONResponse({
            "files": files,
            "count": len(files),
            "message": f"æ‰¾åˆ° {len(files)} ä¸ªå¯ä¸‹è½½æ–‡ä»¶",
            "downloads_dir": str(downloads_dir),
            "dir_exists": downloads_dir.exists()
        })
    except Exception as e:
        logger.error(f"è·å–ä¸‹è½½åˆ—è¡¨å¤±è´¥: {e}")
        return JSONResponse({
            "files": [],
            "count": 0,
            "message": f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}",
            "error": str(e)
        })

@app.get("/download/{filename}")
async def download_single_file(filename: str):
    """ä¸‹è½½å•ä¸ªæ–‡ä»¶"""
    try:
        downloads_dir = Path("downloads")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ–‡ä»¶åæ˜ å°„
        actual_filename = filename
        mapped_original = None
        
        # å°è¯•ä»æ˜ å°„ä¸­æŸ¥æ‰¾å¯¹åº”çš„åŸå§‹æ–‡ä»¶å
        if filename in filename_mapping:
            mapped_original = filename_mapping[filename]
            logger.info(f"æ–‡ä»¶åæ˜ å°„: {filename} -> {mapped_original}")
        
        # å…ˆå°è¯•ç›´æ¥æŸ¥æ‰¾é‡å‘½ååçš„æ–‡ä»¶
        file_path = downloads_dir / filename
        if not file_path.exists() and mapped_original:
            # å¦‚æœé‡å‘½ååçš„æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•æŸ¥æ‰¾åŸå§‹æ–‡ä»¶
            file_path = downloads_dir / mapped_original
            actual_filename = mapped_original
            logger.info(f"ä½¿ç”¨åŸå§‹æ–‡ä»¶åè·¯å¾„: {file_path}")
        
        # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œå°è¯•æŸ¥æ‰¾å¯èƒ½åŒ¹é…çš„æ–‡ä»¶
        if not file_path.exists():
            logger.info(f"å°è¯•æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶ï¼ŒåŸå§‹åç§°: {filename}")
            # å°è¯•æ‰¾åˆ°å«æœ‰ç›¸ä¼¼å‘½åçš„æ–‡ä»¶
            matching_files = []
            
            for potential_file in downloads_dir.glob("*.pdf"):
                # 1. å°è¯•åŸå§‹æ–‡ä»¶åä¸­åŒ…å«è®¢å•å·çš„éƒ¨åˆ†
                if "-" in filename:
                    order_number = filename.split("-")[1].replace(".pdf", "")
                    if order_number in potential_file.name:
                        matching_files.append(potential_file)
                        logger.info(f"æ‰¾åˆ°è®¢å•å·åŒ¹é…æ–‡ä»¶: {potential_file.name} (åŒ…å« {order_number})")
                
                # 2. å°è¯•åŒ¹é…å‰ç¼€éƒ¨åˆ†
                if filename.startswith(potential_file.stem[:5]):
                    matching_files.append(potential_file)
                    logger.info(f"æ‰¾åˆ°å‰ç¼€åŒ¹é…æ–‡ä»¶: {potential_file.name}")
            
            # å¦‚æœæœ‰åŒ¹é…çš„æ–‡ä»¶ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª
            if matching_files:
                file_path = matching_files[0]
                actual_filename = file_path.name
                logger.info(f"ä½¿ç”¨æœ€ä½³åŒ¹é…æ–‡ä»¶: {file_path}")
        
        # è¯¦ç»†æ—¥å¿—è®°å½•
        logger.info(f"è¯·æ±‚ä¸‹è½½æ–‡ä»¶: {filename} -> å®é™…æ–‡ä»¶å: {actual_filename}")
        logger.info(f"å¯»æ‰¾è·¯å¾„: {file_path}")
        
        # å®‰å…¨æ€§æ£€æŸ¥ï¼šç¡®ä¿æ–‡ä»¶åä¸åŒ…å«è·¯å¾„éå†æ”»å‡»
        if ".." in filename or "/" in filename or "\\" in filename:
            logger.warning(f"ä¸‹è½½è¯·æ±‚åŒ…å«æ— æ•ˆçš„æ–‡ä»¶å: {filename}")
            raise HTTPException(status_code=400, detail="æ— æ•ˆçš„æ–‡ä»¶å (åŒ…å«éæ³•å­—ç¬¦)")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not downloads_dir.exists():
            logger.warning(f"ä¸‹è½½ç›®å½•ä¸å­˜åœ¨: {downloads_dir}")
            raise HTTPException(
                status_code=404, 
                detail=f"ä¸‹è½½ç›®å½•ä¸å­˜åœ¨ï¼Œè¯·å…ˆå¤„ç†æ–‡ä»¶æˆ–ä½¿ç”¨ä¿®å¤åŠŸèƒ½"
            )
            
        if not file_path.exists():
            logger.warning(f"è¯·æ±‚çš„æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
            # æŸ¥æ‰¾æ‰€æœ‰å¯ç”¨æ–‡ä»¶
            available_files = list(downloads_dir.glob("*.pdf"))
            available_names = [f.name for f in available_files]
            
            detail_msg = f"æ–‡ä»¶ '{filename}' ä¸å­˜åœ¨"
            if available_files:
                detail_msg += f"ã€‚å¯ç”¨æ–‡ä»¶: {', '.join(available_names[:5])}" 
                if len(available_names) > 5:
                    detail_msg += f" ç­‰å…± {len(available_names)} ä¸ªæ–‡ä»¶"
                    
            raise HTTPException(status_code=404, detail=detail_msg)
            
        if not file_path.is_file():
            logger.warning(f"è·¯å¾„å­˜åœ¨ä½†ä¸æ˜¯æ–‡ä»¶: {file_path}")
            raise HTTPException(status_code=404, detail=f"'{filename}' ä¸æ˜¯ä¸€ä¸ªæ–‡ä»¶")
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        if not file_path.name.lower().endswith('.pdf'):
            logger.warning(f"è¯·æ±‚ä¸‹è½½éPDFæ–‡ä»¶: {file_path.name}")
            raise HTTPException(status_code=400, detail="åªæ”¯æŒä¸‹è½½PDFæ–‡ä»¶")
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = file_path.stat().st_size
        if file_size == 0:
            logger.warning(f"æ–‡ä»¶å¤§å°ä¸º0: {file_path}")
            raise HTTPException(status_code=400, detail="æ–‡ä»¶å¤§å°ä¸º0ï¼Œå¯èƒ½å·²æŸå")
            
        logger.info(f"å•ä¸ªæ–‡ä»¶ä¸‹è½½æˆåŠŸ: {filename} -> {file_path.name}, å¤§å°: {file_size} å­—èŠ‚")
        
        # è¿”å›å®é™…æ‰¾åˆ°çš„æ–‡ä»¶ï¼Œä½†ä¿æŒç”¨æˆ·è¯·æ±‚çš„ä¸‹è½½æ–‡ä»¶å
        return FileResponse(
            file_path,
            filename=filename,  # ä½¿ç”¨ç”¨æˆ·è¯·æ±‚çš„æ–‡ä»¶åä½œä¸ºä¸‹è½½åç§°
            media_type="application/pdf"
        )
    
    except HTTPException:
        # ç»§ç»­æŠ›å‡ºHTTPå¼‚å¸¸
        raise
    except Exception as e:
        # æ•è·å…¶ä»–å¼‚å¸¸å¹¶è®°å½•
        logger.error(f"ä¸‹è½½æ–‡ä»¶æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ä¸‹è½½æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

@app.post("/download-all")
async def download_all():
    """æ‰¹é‡ä¸‹è½½æ‰€æœ‰é‡å‘½åæ–‡ä»¶ï¼ˆZIPæ ¼å¼ï¼‰"""
    downloads_dir = Path("downloads")
    
    if not downloads_dir.exists() or not list(downloads_dir.glob("*.pdf")):
        raise HTTPException(status_code=404, detail="æ²¡æœ‰å¯ä¸‹è½½çš„æ–‡ä»¶")
    
    # åˆ›å»ºZIPæ–‡ä»¶
    from datetime import datetime
    zip_filename = f"renamed_pdfs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
    zip_path = f"temp_{zip_filename}"
    
    try:
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for pdf_file in downloads_dir.glob("*.pdf"):
                zipf.write(pdf_file, pdf_file.name)
        
        logger.info(f"åˆ›å»ºæ‰¹é‡ä¸‹è½½ZIP: {zip_filename}")
        
        def cleanup_zip():
            try:
                os.remove(zip_path)
            except:
                pass
        
        return FileResponse(
            zip_path,
            filename=zip_filename,
            media_type="application/zip",
            background=cleanup_zip
        )
        
    except Exception as e:
        logger.error(f"åˆ›å»ºZIPæ–‡ä»¶å¤±è´¥: {e}")
        if os.path.exists(zip_path):
            os.remove(zip_path)
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºä¸‹è½½æ–‡ä»¶å¤±è´¥: {str(e)}")

@app.post("/download-selected")
async def download_selected(filenames: List[str]):
    """é€‰æ‹©æ€§ä¸‹è½½æŒ‡å®šæ–‡ä»¶ï¼ˆZIPæ ¼å¼ï¼‰"""
    if not filenames:
        raise HTTPException(status_code=400, detail="æ²¡æœ‰é€‰æ‹©æ–‡ä»¶")
    
    downloads_dir = Path("downloads")
    
    # éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    existing_files = []
    for filename in filenames:
        file_path = downloads_dir / filename
        if file_path.exists() and file_path.suffix.lower() == '.pdf':
            existing_files.append(file_path)
    
    if not existing_files:
        raise HTTPException(status_code=404, detail="é€‰æ‹©çš„æ–‡ä»¶éƒ½ä¸å­˜åœ¨")
    
    # åˆ›å»ºZIPæ–‡ä»¶
    from datetime import datetime
    zip_filename = f"selected_pdfs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
    zip_path = f"temp_{zip_filename}"
    
    try:
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for pdf_file in existing_files:
                zipf.write(pdf_file, pdf_file.name)
        
        logger.info(f"åˆ›å»ºé€‰æ‹©æ€§ä¸‹è½½ZIP: {zip_filename}, åŒ…å« {len(existing_files)} ä¸ªæ–‡ä»¶")
        
        def cleanup_zip():
            try:
                os.remove(zip_path)
            except:
                pass
        
        return FileResponse(
            zip_path,
            filename=zip_filename,
            media_type="application/zip",
            background=cleanup_zip
        )
        
    except Exception as e:
        logger.error(f"åˆ›å»ºZIPæ–‡ä»¶å¤±è´¥: {e}")
        if os.path.exists(zip_path):
            os.remove(zip_path)
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºä¸‹è½½æ–‡ä»¶å¤±è´¥: {str(e)}")

@app.get("/debug-downloads")
async def debug_downloads():
    """è°ƒè¯•ä¸‹è½½ç›®å½• - è¯¦ç»†ä¿¡æ¯"""
    downloads_dir = Path("downloads")
    
    try:
        debug_info = {
            "downloads_dir": str(downloads_dir.absolute()),
            "dir_exists": downloads_dir.exists(),
            "is_directory": downloads_dir.is_dir() if downloads_dir.exists() else False,
            "permissions": oct(downloads_dir.stat().st_mode)[-3:] if downloads_dir.exists() else "N/A",
            "all_files": [],
            "pdf_files": [],
            "file_sizes": {},
            "total_files": 0,
            "total_size": 0
        }
        
        if downloads_dir.exists():
            # è·å–æ‰€æœ‰æ–‡ä»¶
            all_files = list(downloads_dir.iterdir())
            debug_info["all_files"] = [f.name for f in all_files if f.is_file()]
            
            # è·å–PDFæ–‡ä»¶
            pdf_files = list(downloads_dir.glob("*.pdf"))
            debug_info["pdf_files"] = [f.name for f in pdf_files]
            
            # æ–‡ä»¶å¤§å°ä¿¡æ¯
            for file in pdf_files:
                size = file.stat().st_size
                debug_info["file_sizes"][file.name] = {
                    "size_bytes": size,
                    "size_mb": round(size / 1024 / 1024, 2),
                    "modified": file.stat().st_mtime
                }
                debug_info["total_size"] += size
            
            debug_info["total_files"] = len(pdf_files)
        
        logger.info(f"è°ƒè¯•ä¸‹è½½ç›®å½•: {debug_info}")
        return JSONResponse(debug_info)
        
    except Exception as e:
        logger.error(f"è°ƒè¯•ä¸‹è½½ç›®å½•å¤±è´¥: {e}")
        return JSONResponse({
            "error": str(e),
            "downloads_dir": str(downloads_dir),
            "dir_exists": False
        })

@app.post("/auto-fix")
async def auto_fix_uploads():
    """è‡ªåŠ¨ä¿®å¤ï¼šå°†uploadsä¸­é—ç•™çš„å¤„ç†å¥½çš„PDFæ–‡ä»¶ç§»åŠ¨åˆ°downloadsç›®å½•ï¼Œå¹¶ä»å¤‡ä»½ç›®å½•æ¢å¤ç¼ºå¤±æ–‡ä»¶"""
    uploads_dir = Path("uploads")
    downloads_dir = Path("downloads")
    backup_dir = Path("backup")
    
    # ç¡®ä¿downloadsç›®å½•å­˜åœ¨
    downloads_dir.mkdir(exist_ok=True)
    
    fixed_count = 0
    backup_files_added = 0
    results = []
    
    # ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥uploadsä¸­çš„æ–‡ä»¶
    if uploads_dir.exists():
        pdf_files = list(uploads_dir.glob("*.pdf"))
        
        for file in pdf_files:
            try:
                # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦æ˜¯å¤„ç†åçš„æ ¼å¼ï¼ˆåŒ…å«-çš„é”€è´§å•å·ï¼‰
                if "-" in file.stem and (file.stem.count("-") == 1):
                    # è¿™æ˜¯ä¸€ä¸ªå¤„ç†åçš„æ–‡ä»¶ï¼Œåº”è¯¥ç§»åŠ¨åˆ°downloads
                    destination = downloads_dir / file.name
                    
                    # å¤„ç†é‡åæƒ…å†µ
                    counter = 1
                    original_stem = file.stem
                    original_suffix = file.suffix
                    while destination.exists():
                        new_name = f"{original_stem}_{counter}{original_suffix}"
                        destination = downloads_dir / new_name
                        counter += 1
                    
                    # ç§»åŠ¨æ–‡ä»¶
                    import shutil
                    shutil.copy2(str(file), str(destination))
                    
                    fixed_count += 1
                    results.append({
                        "original": file.name,
                        "moved_to": destination.name,
                        "success": True
                    })
                    logger.info(f"è‡ªåŠ¨ä¿®å¤: {file.name} -> {destination.name}")
                    
                else:
                    # ä¿ç•™æœªå¤„ç†çš„æ–‡ä»¶
                    results.append({
                        "original": file.name,
                        "moved_to": None,
                        "success": False,
                        "reason": "éå¤„ç†åæ–‡ä»¶ï¼Œä¿ç•™åœ¨uploads"
                    })
                    
            except Exception as e:
                results.append({
                    "original": file.name,
                    "moved_to": None,
                    "success": False,
                    "reason": f"ç§»åŠ¨å¤±è´¥: {str(e)}"
                })
                logger.error(f"è‡ªåŠ¨ä¿®å¤å¤±è´¥ {file.name}: {e}")
    
    # ç¬¬äºŒæ­¥ï¼šä»å¤‡ä»½ç›®å½•å¤åˆ¶æ–‡ä»¶åˆ°downloads
    backup_date_dirs = []
    if backup_dir.exists():
        backup_date_dirs = [d for d in backup_dir.iterdir() if d.is_dir()]
        # æŒ‰æ—¥æœŸå€’åºæ’åˆ—ï¼Œç¡®ä¿ä½¿ç”¨æœ€æ–°çš„å¤‡ä»½
        backup_date_dirs.sort(reverse=True)
    
    for date_dir in backup_date_dirs:
        logger.info(f"æ£€æŸ¥å¤‡ä»½ç›®å½•: {date_dir}")
        backup_files = list(date_dir.glob("*.pdf"))
        
        for backup_file in backup_files:
            try:
                # å°è¯•ä»åŸå§‹æ–‡ä»¶æ¢å¤å¤„ç†åæ–‡ä»¶
                if "-" in backup_file.stem and backup_file.stem.count("-") == 1:
                    # è¿™æ˜¯ä¸€ä¸ªå¤„ç†åçš„æ–‡ä»¶åæ ¼å¼ (å¦‚1403-20250110...)
                    destination = downloads_dir / backup_file.name
                    
                    if not destination.exists():
                        import shutil
                        shutil.copy2(str(backup_file), str(destination))
                        backup_files_added += 1
                        results.append({
                            "original": backup_file.name,
                            "moved_to": backup_file.name,
                            "success": True,
                            "source": "backup-direct-processed"
                        })
                        logger.info(f"ä»å¤‡ä»½æ¢å¤å¤„ç†åæ–‡ä»¶: {backup_file.name}")
            except Exception as e:
                logger.error(f"ä»å¤‡ä»½æ¢å¤å¤±è´¥ {backup_file.name}: {e}")
    
    # ç¬¬ä¸‰æ­¥ï¼šå¦‚æœè¿˜æ˜¯æ²¡æœ‰æ–‡ä»¶ï¼Œä»å¤‡ä»½å¤åˆ¶é‡å‘½åæ ¼å¼çš„PDFæ–‡ä»¶ï¼ˆæ’é™¤åŸå§‹æ–‡ä»¶åæ ¼å¼ï¼‰
    if backup_files_added == 0 and fixed_count == 0 and backup_date_dirs:
        latest_backup = backup_date_dirs[0]
        logger.info(f"å°è¯•ä»æœ€æ–°å¤‡ä»½ç›®å½•å¤åˆ¶é‡å‘½åæ–‡ä»¶: {latest_backup}")
        
        import re
        # åªå¤åˆ¶é‡å‘½åæ ¼å¼çš„æ–‡ä»¶ï¼ˆåŒ…å«-æˆ–_çš„é”€è´§å•å·æ ¼å¼ï¼‰
        renamed_pattern = re.compile(r'^[0-9]{4}[-_][0-9]{8,}.*\.pdf$')
        original_pattern = re.compile(r'^\d{17}_\d{4}.*\.pdf$')  # æ’é™¤åŸå§‹æ–‡ä»¶åæ ¼å¼
        
        for backup_file in latest_backup.glob("*.pdf"):
            try:
                # æ£€æŸ¥æ˜¯å¦æ˜¯é‡å‘½åæ ¼å¼çš„æ–‡ä»¶ï¼Œæ’é™¤åŸå§‹æ–‡ä»¶åæ ¼å¼
                if (renamed_pattern.match(backup_file.name) and 
                    not original_pattern.match(backup_file.name)):
                    
                    destination = downloads_dir / backup_file.name
                    if not destination.exists():
                        import shutil
                        shutil.copy2(str(backup_file), str(destination))
                        backup_files_added += 1
                        results.append({
                            "original": backup_file.name,
                            "moved_to": backup_file.name,
                            "success": True,
                            "source": "backup-renamed-only"
                        })
                        logger.info(f"ä»å¤‡ä»½å¤åˆ¶é‡å‘½åæ–‡ä»¶: {backup_file.name}")
                    else:
                        logger.info(f"è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶: {backup_file.name}")
                else:
                    logger.debug(f"è·³è¿‡åŸå§‹æ–‡ä»¶åæ ¼å¼: {backup_file.name}")
            except Exception as e:
                logger.error(f"ä»å¤‡ä»½å¤åˆ¶å¤±è´¥ {backup_file.name}: {e}")
    
    total_fixed = fixed_count + backup_files_added
    
    # è‡ªåŠ¨ä¿®å¤å®Œæˆåæ¸…ç†ä»»ä½•æ®‹ç•™çš„åŸå§‹æ–‡ä»¶åæ ¼å¼æ–‡ä»¶ï¼ˆå¤‡ä»½æ¢å¤å¯èƒ½äº§ç”Ÿï¼‰
    auto_cleaned_count = clean_original_filename_files()
    if auto_cleaned_count > 0:
        logger.info(f"ğŸ§¹ è‡ªåŠ¨ä¿®å¤åæ¸…ç†äº† {auto_cleaned_count} ä¸ªåŸå§‹æ–‡ä»¶åæ ¼å¼æ–‡ä»¶")
    
    message = f"âœ… è‡ªåŠ¨ä¿®å¤å®Œæˆï¼Œå…±å¤„ç† {total_fixed} ä¸ªæ–‡ä»¶"
    if fixed_count > 0:
        message += f"ï¼ˆä»uploadsä¿®å¤: {fixed_count}ä¸ªï¼‰"
    if backup_files_added > 0:
        message += f"ï¼ˆä»å¤‡ä»½æ¢å¤: {backup_files_added}ä¸ªï¼‰"
    if auto_cleaned_count > 0:
        message += f"ï¼ˆæ¸…ç†åŸå§‹æ–‡ä»¶: {auto_cleaned_count}ä¸ªï¼‰"
    
    logger.info(message)
    
    return JSONResponse({
        "message": message,
        "fixed_count": fixed_count,
        "backup_files_added": backup_files_added,
        "cleaned_count": auto_cleaned_count,
        "total_fixed": total_fixed,
        "details": results
    })

@app.post("/clear")
async def clear_downloads():
    """æ¸…ç†ä¸‹è½½æ–‡ä»¶"""
    global filename_mapping
    
    # ä½¿ç”¨ç»Ÿä¸€çš„æ¸…ç†å‡½æ•°
    cleared_count = clean_all_downloads()
    
    # æ¸…ç©ºæ–‡ä»¶åæ˜ å°„å…³ç³»
    filename_mapping.clear()
    
    logger.info(f"æ‰‹åŠ¨æ¸…ç†äº† {cleared_count} ä¸ªä¸‹è½½æ–‡ä»¶")
    return JSONResponse({"message": f"âœ… å·²æ¸…ç† {cleared_count} ä¸ªä¸‹è½½æ–‡ä»¶"})

@app.post("/selective-backup")
async def selective_backup(files: List[UploadFile] = File(...)):
    """é€‰æ‹©æ€§å¤‡ä»½åŠŸèƒ½"""
    if not files:
        raise HTTPException(status_code=400, detail="æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶")
    
    results = []
    backup_count = 0
    
    for file in files:
        if not file.filename or not file.filename.lower().endswith('.pdf'):
            results.append({
                "filename": file.filename or "unknown",
                "success": False,
                "message": "âŒ ä¸æ˜¯PDFæ–‡ä»¶"
            })
            continue
        
        try:
            # å¤„ç†æ–‡ä»¶åï¼Œå»é™¤è·¯å¾„ä¿¡æ¯
            clean_filename = os.path.basename(file.filename) if file.filename else "unknown.pdf"
            
            # ä¸´æ—¶ä¿å­˜æ–‡ä»¶
            temp_path = f"temp_{clean_filename}"
            with open(temp_path, "wb") as buffer:
                content = await file.read()
                if not content:
                    raise ValueError("æ–‡ä»¶å†…å®¹ä¸ºç©º")
                buffer.write(content)
            
            # åˆ›å»ºå¤‡ä»½ï¼Œä½¿ç”¨åŸå§‹æ–‡ä»¶åä¿æŒæ–‡ä»¶å¤¹ç»“æ„ä¿¡æ¯
            backup_filename = file.filename if file.filename else clean_filename
            backup_path = create_backup(temp_path, backup_filename)
            
            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            os.remove(temp_path)
            
            results.append({
                "filename": file.filename,
                "success": True,
                "message": "âœ… å¤‡ä»½æˆåŠŸ",
                "backup_path": backup_path
            })
            backup_count += 1
            
        except Exception as e:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            clean_filename = os.path.basename(file.filename) if file.filename else "unknown.pdf"
            temp_path = f"temp_{clean_filename}"
            if os.path.exists(temp_path):
                try:
                    os.remove(temp_path)
                except:
                    pass
            
            results.append({
                "filename": file.filename or "unknown",
                "success": False,
                "message": f"âŒ å¤‡ä»½å¤±è´¥: {str(e)}"
            })
    
    logger.info(f"é€‰æ‹©æ€§å¤‡ä»½å®Œæˆï¼ŒæˆåŠŸå¤‡ä»½ {backup_count} ä¸ªæ–‡ä»¶")
    
    return JSONResponse({
        "results": results,
        "backup_count": backup_count,
        "total_count": len(files),
        "message": f"âœ… é€‰æ‹©æ€§å¤‡ä»½å®Œæˆï¼ŒæˆåŠŸå¤‡ä»½ {backup_count}/{len(files)} ä¸ªæ–‡ä»¶"
    })

@app.post("/clear-backup")
async def clear_backup(date: str = None):
    """æ¸…ç†å¤‡ä»½æ–‡ä»¶"""
    try:
        backup_root = Path("backup")
        cleared_count = 0
        
        if not backup_root.exists():
            return JSONResponse({"message": "âš ï¸ å¤‡ä»½ç›®å½•ä¸å­˜åœ¨"})
        
        if date:
            # æ¸…ç†æŒ‡å®šæ—¥æœŸçš„å¤‡ä»½
            date_folder = backup_root / date
            if date_folder.exists():
                for file in date_folder.glob("*.pdf"):
                    try:
                        os.remove(file)
                        cleared_count += 1
                    except Exception as e:
                        logger.warning(f"æ¸…ç†å¤‡ä»½æ–‡ä»¶å¤±è´¥ {file}: {e}")
                
                # å¦‚æœæ–‡ä»¶å¤¹ä¸ºç©ºï¼Œåˆ é™¤æ–‡ä»¶å¤¹
                try:
                    if not any(date_folder.iterdir()):
                        date_folder.rmdir()
                except:
                    pass
                
                logger.info(f"æ¸…ç†äº† {date} çš„ {cleared_count} ä¸ªå¤‡ä»½æ–‡ä»¶")
                return JSONResponse({"message": f"âœ… å·²æ¸…ç† {date} çš„ {cleared_count} ä¸ªå¤‡ä»½æ–‡ä»¶"})
            else:
                return JSONResponse({"message": f"âš ï¸ æ—¥æœŸ {date} çš„å¤‡ä»½ä¸å­˜åœ¨"})
        else:
            # æ¸…ç†æ‰€æœ‰å¤‡ä»½
            for date_folder in backup_root.iterdir():
                if date_folder.is_dir():
                    for file in date_folder.glob("*.pdf"):
                        try:
                            os.remove(file)
                            cleared_count += 1
                        except Exception as e:
                            logger.warning(f"æ¸…ç†å¤‡ä»½æ–‡ä»¶å¤±è´¥ {file}: {e}")
                    
                    # åˆ é™¤ç©ºæ–‡ä»¶å¤¹
                    try:
                        if not any(date_folder.iterdir()):
                            date_folder.rmdir()
                    except:
                        pass
            
            logger.info(f"æ¸…ç†äº†æ‰€æœ‰ {cleared_count} ä¸ªå¤‡ä»½æ–‡ä»¶")
            return JSONResponse({"message": f"âœ… å·²æ¸…ç†æ‰€æœ‰ {cleared_count} ä¸ªå¤‡ä»½æ–‡ä»¶"})
    
    except Exception as e:
        logger.error(f"æ¸…ç†å¤‡ä»½å¤±è´¥: {e}")
        return JSONResponse({"message": f"âŒ æ¸…ç†å¤‡ä»½å¤±è´¥: {str(e)}"})

def find_available_port(start_port: int = 8000, max_attempts: int = 10) -> int:
    """æŸ¥æ‰¾å¯ç”¨ç«¯å£"""
    import socket
    
    for port in range(start_port, start_port + max_attempts):
        try:
            # å°è¯•ç»‘å®šç«¯å£
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                sock.bind(('localhost', port))
                logger.info(f"âœ… æ‰¾åˆ°å¯ç”¨ç«¯å£: {port}")
                return port
        except OSError:
            logger.debug(f"ç«¯å£ {port} å·²è¢«å ç”¨")
            continue
    
    # å¦‚æœæ‰€æœ‰ç«¯å£éƒ½è¢«å ç”¨ï¼Œè¿”å›ä¸€ä¸ªéšæœºç«¯å£
    import random
    random_port = random.randint(8010, 8999)
    logger.warning(f"âš ï¸ å‰{max_attempts}ä¸ªç«¯å£éƒ½è¢«å ç”¨ï¼Œå°è¯•éšæœºç«¯å£: {random_port}")
    return random_port

def start_server():
    """å¯åŠ¨æœåŠ¡å™¨"""
    logger.info("ğŸš€ å¯åŠ¨PDFæ‰¹é‡é‡å‘½åå·¥å…·...")
    
    # æŸ¥æ‰¾å¯ç”¨ç«¯å£
    available_port = find_available_port()
    
    try:
        logger.info(f"ğŸŒ æœåŠ¡å°†åœ¨ä»¥ä¸‹åœ°å€å¯åŠ¨:")
        logger.info(f"   - æœ¬åœ°è®¿é—®: http://localhost:{available_port}")
        logger.info(f"   - ç½‘ç»œè®¿é—®: http://0.0.0.0:{available_port}")
        logger.info("=" * 50)
        
        uvicorn.run(app, host="0.0.0.0", port=available_port)
        
    except Exception as e:
        logger.error(f"âŒ æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
        
        # å°è¯•å¤‡ç”¨ç«¯å£
        logger.info("ğŸ”„ å°è¯•å¤‡ç”¨ç«¯å£...")
        backup_port = find_available_port(9000, 5)
        
        try:
            logger.info(f"ğŸŒ ä½¿ç”¨å¤‡ç”¨ç«¯å£: {backup_port}")
            uvicorn.run(app, host="0.0.0.0", port=backup_port)
        except Exception as backup_e:
            logger.error(f"âŒ å¤‡ç”¨ç«¯å£å¯åŠ¨ä¹Ÿå¤±è´¥: {backup_e}")
            logger.error("ğŸ’¡ è¯·æ‰‹åŠ¨æŒ‡å®šç«¯å£å·æˆ–æ£€æŸ¥ç½‘ç»œé…ç½®")

if __name__ == "__main__":
    start_server()
